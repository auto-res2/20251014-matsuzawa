
Input:
You are an AI research assistant tasked with generating Hydra configuration files for experiment runs.

# Task
Generate individual YAML configuration files for each experiment run. These configs will be used by Hydra to configure specific experimental variations.

# Input Information

## Research Method
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

## Experimental Design
{'experiment_summary': 'Comparative analysis of efficient model architectures across vision and language tasks', 'evaluation_metrics': ['accuracy', 'f1_score', 'inference_time', 'model_size'], 'proposed_method': 'Evaluate MobileNetV2 and DistilBERT variants for efficient deployment', 'comparative_methods': ['MobileNetV2-0.5 (3.5M parameters)', 'DistilBERT-base (66M parameters)'], 'models_to_use': ['MobileNetV2-0.5-3.5M', 'DistilBERT-base-66M'], 'datasets_to_use': ['CIFAR-10', 'alpaca-cleaned'], 'hyperparameters_to_search': None, 'external_resources': None, 'experiment_code': None}

## Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned


# Requirements

## Configuration Structure
Each run configuration should include:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings)
- optuna: Hyperparameter search space definition for Optuna optimization
  - Define search spaces for key hyperparameters using Optuna's suggest methods
  - Example: learning_rate: [1e-5, 1e-3], batch_size: [16, 32, 64]
- Any other experiment-specific settings

## Format
- Generate one YAML configuration per experiment run
- Ensure valid YAML syntax
- Use meaningful parameter values based on the research method and experimental design

## Example Configuration
```yaml
run_id: baseline_bert_imdb
method: baseline
model:
  name: bert-base-uncased
  hidden_size: 768
  num_layers: 12
dataset:
  name: imdb
  max_length: 512
  batch_size: 32
training:
  learning_rate: 2e-5
  epochs: 3
  optimizer: adamw
  warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
```

# Experimental Environment
cpu
RAMï¼š500 MB

# Instructions
1. Generate one YAML configuration for each experiment run listed above
2. Ensure configurations reflect the differences between baseline, proposed, and ablation methods
3. Use appropriate hyperparameters based on the experimental design
4. Include Optuna search space if hyperparameter optimization is beneficial for the experiment
5. For Optuna search spaces, use appropriate distribution types:
   - loguniform: For learning rates, regularization parameters
   - uniform: For dropout rates, weight decay
   - int: For hidden dimensions, number of layers
   - categorical: For discrete choices like batch size, optimizer type

Generate the configurations now:
Output:
{
  "run_configs": [
    {
      "run_id": "comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10",
      "run_config_yaml": "run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\nmethod: comparative-2\nmodel:\n  name: MobileNetV2\n  variant: 0.5\n  parameters: 3.5M\n  pretrained: false\n  width_mult: 0.5\n  dropout: 0.2\ndataset:\n  name: CIFAR-10\n  splits:\n    train: 45000\n    val: 5000\n    test: 10000\n  image_size: 32\n  normalization:\n    mean: [0.4914, 0.4822, 0.4465]\n    std: [0.2023, 0.1994, 0.2010]\n  augmentations:\n    random_crop: true\n    random_flip: horizontal\ntraining:\n  epochs: 50\n  batch_size: 64\n  learning_rate: 0.05\n  optimizer: sgd\n  momentum: 0.9\n  weight_decay: 0.0005\n  lr_scheduler:\n    type: cosine\n    T_max: 50\noptuna:\n  n_trials: 25\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 0.001\n      high: 0.1\n    batch_size:\n      type: categorical\n      choices: [32, 64, 128]\n    momentum:\n      type: uniform\n      low: 0.7\n      high: 0.95\n    weight_decay:\n      type: loguniform\n      low: 1e-5\n      high: 1e-3\n"
    },
    {
      "run_id": "comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned",
      "run_config_yaml": "run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned\nmethod: comparative-2\nmodel:\n  name: MobileNetV2\n  variant: 0.5\n  parameters: 3.5M\n  pretrained: false\n  input_adapter:\n    type: char_cnn\n    embedding_dim: 256\ndataset:\n  name: alpaca-cleaned\n  task: instruction_following\n  max_seq_length: 512\n  tokenizer: char\ntraining:\n  epochs: 3\n  batch_size: 16\n  learning_rate: 0.001\n  optimizer: adam\n  weight_decay: 0.01\n  gradient_accumulation_steps: 2\noptuna:\n  n_trials: 15\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1e-4\n      high: 1e-2\n    batch_size:\n      type: categorical\n      choices: [8, 16, 32]\n    weight_decay:\n      type: loguniform\n      low: 1e-5\n      high: 1e-1\n"
    },
    {
      "run_id": "comparative-2-DistilBERT-base-66M-CIFAR-10",
      "run_config_yaml": "run_id: comparative-2-DistilBERT-base-66M-CIFAR-10\nmethod: comparative-2\nmodel:\n  name: distilbert-base-uncased\n  parameters: 66M\n  pretrained: true\n  modality_adapter:\n    type: linear_patch_embedding\n    patch_size: 4\n    sequence_length: 64\ndataset:\n  name: CIFAR-10\n  splits:\n    train: 45000\n    val: 5000\n    test: 10000\n  image_size: 32\n  normalization:\n    mean: [0.4914, 0.4822, 0.4465]\n    std: [0.2023, 0.1994, 0.2010]\ntraining:\n  epochs: 10\n  batch_size: 32\n  learning_rate: 0.0001\n  optimizer: adamw\n  weight_decay: 0.01\n  lr_scheduler:\n    type: linear\n    warmup_steps: 500\noptuna:\n  n_trials: 20\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1e-5\n      high: 1e-3\n    batch_size:\n      type: categorical\n      choices: [16, 32, 64]\n    patch_size:\n      type: categorical\n      choices: [2, 4, 8]\n    weight_decay:\n      type: loguniform\n      low: 1e-5\n      high: 1e-2\n"
    },
    {
      "run_id": "comparative-2-DistilBERT-base-66M-alpaca-cleaned",
      "run_config_yaml": "run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned\nmethod: comparative-2\nmodel:\n  name: distilbert-base-uncased\n  parameters: 66M\n  pretrained: true\ndataset:\n  name: alpaca-cleaned\n  task: instruction_following\n  max_seq_length: 512\n  tokenizer: sentencepiece\ntraining:\n  epochs: 3\n  batch_size: 16\n  learning_rate: 2e-5\n  optimizer: adamw\n  weight_decay: 0.01\n  lr_scheduler:\n    type: linear\n    warmup_steps: 500\noptuna:\n  n_trials: 25\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1e-5\n      high: 1e-3\n    batch_size:\n      type: categorical\n      choices: [8, 16, 32]\n    weight_decay:\n      type: loguniform\n      low: 1e-5\n      high: 1e-2\n"
    }
  ]
}
