
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches: `uv run python -u -m src.main run={run_id} results_dir={path}`

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations

4. **Model Implementation**:
   - Complete model architectures for baseline, proposed, and ablation methods
   - All models implemented from scratch (no placeholders)
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - No missing files from the structure
   - All functionality contained within specified files

6. **Structured Logging**:
   - train.py prints JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py prints JSON-formatted comparison results to stdout
   - main.py redirects subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log`
   - Logs are forwarded to main process stdout/stderr for GitHub Actions capture

7. **Configuration Files**:
   - config/config.yaml contains list of all experiment run_ids
   - NOTE: config/run/{run_id}.yaml files are provided separately in the "Experiment Runs" section below (not in ExperimentCode)
   - The generated code must properly reference these config files via Hydra
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation and Figures** (if WandB not used):
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/`)
   - Figures include legends, annotations, and proper labels
   - Uses `plt.tight_layout()` before saving
   - Consistent result formatting and comparison logic

9. **WandB Integration** (if WandB is used):
   - Proper WandB initialization with entity/project from config
   - Metrics logged to WandB during training
   - WandB run URL printed to stdout
   - Metadata saved to `.research/iteration{experiment_iteration}/wandb_metadata.json`
   - Figures uploaded as WandB artifacts

10. **Immediate Executability**:
    - Code can be run immediately without modifications
    - All imports and dependencies properly specified in pyproject.toml
    - No missing external resources or undefined variables
    - Proper module structure for `uv run python -m src.main` execution

## Detection of Common Issues
Flag the following problems if found:

- **Incomplete Implementation**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns
- **Truncation**: Code sections that appear shortened or simplified inappropriately
- **Missing Functionality**: Expected features not implemented
- **Configuration Issues**: Missing or incomplete run configs
- **Import Errors**: Missing dependencies or incorrect import statements
- **Not Executable**: Code that cannot be run immediately
- **Inconsistent Structure**: Files not matching required structure
- **Logging Issues**: Missing or incorrect JSON logging format

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

# Experimental Design
- Strategy: Comparative analysis of efficient model architectures across vision and language tasks
- Proposed Method: Evaluate MobileNetV2 and DistilBERT variants for efficient deployment
- Evaluation Metrics: ['accuracy', 'f1_score', 'inference_time', 'model_size']

# Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  width_mult: 0.5
  dropout: 0.2
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
  augmentations:
    random_crop: true
    random_flip: horizontal
training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.05
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0005
  lr_scheduler:
    type: cosine
    T_max: 50
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 0.001
      high: 0.1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    momentum:
      type: uniform
      low: 0.7
      high: 0.95
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  input_adapter:
    type: char_cnn
    embedding_dim: 256
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: char
training:
  epochs: 3
  batch_size: 16
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 0.01
  gradient_accumulation_steps: 2
optuna:
  n_trials: 15
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-1

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-CIFAR-10
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
  modality_adapter:
    type: linear_patch_embedding
    patch_size: 4
    sequence_length: 64
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    patch_size:
      type: categorical
      choices: [2, 4, 8]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: sentencepiece
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "runs_list:\n  - comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\n  - comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned\n  - comparative-2-DistilBERT-base-66M-CIFAR-10\n  - comparative-2-DistilBERT-base-66M-alpaca-cleaned\n\nrun: all  # default run selection; pass run={run_id} from CLI to override\nresults_dir: outputs\n\ndefaults: []\n\n# You can add global WandB defaults here if desired\nwandb:\n  entity: gengaru617\n  project: 251014-test\n  tags: [hydra, comparison]\n  run_name: null\n  ", "evaluate_py": "import json\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom hydra.utils import to_absolute_path\nfrom omegaconf import DictConfig\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport wandb\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    results_dir = Path(to_absolute_path(cfg.results_dir)).expanduser()\n    run_ids: List[str] = cfg.get(\"evaluate_run_ids\", []) or [p.name for p in results_dir.iterdir() if p.is_dir()]\n\n    records = []\n    for rid in run_ids:\n        res_path = results_dir / rid / \"results.json\"\n        if not res_path.exists():\n            print(f\"Warning: missing results for {rid}\")\n            continue\n        data = json.loads(res_path.read_text())\n        records.append({\"run_id\": rid, \"best_val_acc\": data.get(\"best_val_acc\", 0)})\n\n    df = pd.DataFrame.from_records(records)\n    print(json.dumps({\"comparison\": df.to_dict(orient=\"records\")}))\n\n    # ---- Figure -------------------------------------------------------------\n    fig, ax = plt.subplots(figsize=(10, 4))\n    df.sort_values(\"best_val_acc\", inplace=True)\n    ax.barh(df.run_id, df.best_val_acc)\n    ax.set_xlabel(\"Best Validation Accuracy\")\n    fig.tight_layout()\n\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / \"comparison.pdf\"\n    fig.savefig(fig_path)\n    print(f\"Saved comparison figure \u2192 {fig_path}\")\n\n    # Upload as W\u0026B artifact where possible\n    if \"WANDB_API_KEY\" in os.environ:\n        wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n        run = wandb.init(project=\"251014-test\", entity=\"gengaru617\", name=\"evaluation\", reinit=True)\n        artifact = wandb.Artifact(\"comparison-figure\", type=\"figure\")\n        artifact.add_file(str(fig_path))\n        run.log_artifact(artifact)\n        run.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "import json\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom hydra.utils import to_absolute_path\nfrom omegaconf import DictConfig, OmegaConf\n\n# -----------------------------------------------------------------------------\n# Helpers\n# -----------------------------------------------------------------------------\n\ndef _stream_redirect(stream, paths: List[Path]):\n    \"\"\"Continuously read from `stream` and write to each file in `paths` and stdout.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        decoded = line.decode()\n        for p in paths:\n            p.write_text(decoded, append=True) if hasattr(p, \"write_text\") else p.write(decoded)\n        sys.stdout.write(decoded)\n    stream.close()\n\n\ndef _launch(run_id: str, results_dir: Path):\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    stdout_log = run_dir / \"stdout.log\"\n    stderr_log = run_dir / \"stderr.log\"\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\", f\"run={run_id}\", f\"results_dir={str(results_dir)}\"]\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # Redirect outputs using threads\n    threads = [\n        threading.Thread(target=_stream_redirect, args=(proc.stdout, [stdout_log])),\n        threading.Thread(target=_stream_redirect, args=(proc.stderr, [stderr_log])),\n    ]\n    for t in threads:\n        t.start()\n    for t in threads:\n        t.join()\n\n    proc.wait()\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Subprocess for {run_id} failed with return code {proc.returncode}\")\n\n\n# -----------------------------------------------------------------------------\n# Main Hydra entry point\n# -----------------------------------------------------------------------------\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    results_dir = Path(to_absolute_path(cfg.results_dir)).expanduser().absolute()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    if cfg.run == \"all\":\n        run_ids = cfg.runs_list\n    else:\n        run_ids = [cfg.run]\n\n    for run_id in run_ids:\n        print(f\"=== Running {run_id} ===\")\n        _launch(run_id, results_dir)\n\n    # After all runs, launch evaluation\n    subprocess.run([sys.executable, \"-u\", \"-m\", \"src.evaluate\", f\"results_dir={str(results_dir)}\"], check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "import math\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n\nfrom transformers import DistilBertModel, DistilBertConfig\nfrom omegaconf import DictConfig\n\n# -----------------------------------------------------------------------------\n# MobileNetV2 based classifiers\n# -----------------------------------------------------------------------------\n\n\nclass CIFARMobileNetV2Classifier(nn.Module):\n    def __init__(self, num_classes: int, width_mult: float = 1.0, dropout: float = 0.0, pretrained: bool = False):\n        super().__init__()\n        weights = MobileNet_V2_Weights.DEFAULT if pretrained else None\n        self.backbone = mobilenet_v2(weights=weights, width_mult=width_mult)\n        self.backbone.classifier[1] = nn.Linear(self.backbone.last_channel, num_classes)\n        if dropout \u003e 0:\n            self.backbone.classifier.insert(1, nn.Dropout(p=dropout))\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        return self.backbone(x)\n\n\n# -----------------------------------------------------------------------------\n# Char CNN adapter to MobileNetV2 for text inputs\n# -----------------------------------------------------------------------------\n\nclass CharCNNAdapter(nn.Module):\n    \"\"\"Convert a sequence of character indices into a 3x32x32 pseudo-image.\"\"\"\n\n    def __init__(self, vocab_size: int, embedding_dim: int):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.pool = nn.AdaptiveAvgPool1d(3072)  # 3*32*32\n\n    def forward(self, char_ids: torch.Tensor):  # [B, L]\n        emb = self.embedding(char_ids).permute(0, 2, 1)  # B x E x L\n        flat = self.pool(emb)  # B x E x 3072/E (E=embedding_dim) but we force output size\n        flat = flat.view(char_ids.size(0), 3, 32, 32)\n        return flat\n\n\nclass CharMobileNetClassifier(nn.Module):\n    def __init__(self, num_classes: int, vocab_size: int, embedding_dim: int, width_mult: float):\n        super().__init__()\n        self.adapter = CharCNNAdapter(vocab_size, embedding_dim)\n        self.mobilenet = CIFARMobileNetV2Classifier(num_classes=num_classes, width_mult=width_mult, pretrained=False)\n        self.num_classes = num_classes\n\n    def forward(self, char_ids):\n        imgs = self.adapter(char_ids)\n        return self.mobilenet(imgs)\n\n\n# -----------------------------------------------------------------------------\n# DistilBERT for images via patch embedding\n# -----------------------------------------------------------------------------\n\nclass LinearPatchEmbedding(nn.Module):\n    def __init__(self, in_channels: int, patch_size: int, emb_dim: int, img_size: int):\n        super().__init__()\n        self.patch_size = patch_size\n        num_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Linear(in_channels * patch_size * patch_size, emb_dim)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, emb_dim))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n\n    def forward(self, x):  # B x C x H x W\n        B, C, H, W = x.size()\n        p = self.patch_size\n        x = x.unfold(2, p, p).unfold(3, p, p)  # B C h w p p\n        x = x.contiguous().view(B, C, -1, p, p)  # B C N p p\n        x = x.flatten(3)  # B C N p*p\n        x = x.permute(0, 2, 1, 3).flatten(2)  # B N C*p*p\n        tokens = self.proj(x)  # B N D\n        cls = self.cls_token.expand(B, -1, -1)\n        tokens = torch.cat((cls, tokens), dim=1) + self.pos_embed\n        return tokens  # B seq D\n\n\nclass DistilBERTImageClassifier(nn.Module):\n    def __init__(self, num_classes: int, patch_size: int, img_size: int = 32):\n        super().__init__()\n        self.num_classes = num_classes\n        self.embed_dim = 768\n        self.patch_embed = LinearPatchEmbedding(3, patch_size, self.embed_dim, img_size)\n        self.transformer = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.classifier = nn.Linear(self.embed_dim, num_classes)\n\n    def forward(self, x):  # x images\n        embeds = self.patch_embed(x)  # B seq D\n        outputs = self.transformer(inputs_embeds=embeds)\n        cls_tok = outputs.last_hidden_state[:, 0]  # B D\n        logits = self.classifier(cls_tok)\n        return logits\n\n\n# -----------------------------------------------------------------------------\n# DistilBERT text classifier\n# -----------------------------------------------------------------------------\n\nclass DistilBERTTextClassifier(nn.Module):\n    def __init__(self, num_classes: int = 2):\n        super().__init__()\n        self.transformer = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_classes)\n        self.num_classes = num_classes\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]):\n        input_ids, attention_mask = inputs\n        out = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n        cls_tok = out.last_hidden_state[:, 0]\n        return self.classifier(cls_tok)\n\n\n# -----------------------------------------------------------------------------\n# Builder\n# -----------------------------------------------------------------------------\n\ndef build_model(cfg: DictConfig) -\u003e nn.Module:\n    dataset_name = cfg.dataset.name\n    model_name = cfg.model.name.lower()\n\n    if model_name == \"mobilenetv2\":\n        if dataset_name == \"CIFAR-10\":\n            return CIFARMobileNetV2Classifier(num_classes=10,\n                                              width_mult=cfg.model.width_mult,\n                                              dropout=cfg.model.get(\"dropout\", 0.0),\n                                              pretrained=cfg.model.pretrained)\n        else:\n            # char-CNN adapter path\n            return CharMobileNetClassifier(num_classes=2,  # fixed for demo\n                                           vocab_size=len(_CHAR2IDX) + 1,\n                                           embedding_dim=cfg.model.input_adapter.embedding_dim,\n                                           width_mult=cfg.model.variant)\n    elif \"distilbert\" in model_name:\n        if dataset_name == \"CIFAR-10\":\n            return DistilBERTImageClassifier(num_classes=10, patch_size=cfg.model.modality_adapter.patch_size)\n        else:\n            return DistilBERTTextClassifier(num_classes=2)\n    else:\n        raise ValueError(f\"Unsupported model: {model_name}\")\n", "preprocess_py": "from pathlib import Path\nfrom typing import List, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Subset\nfrom torch.utils.data import Dataset as TorchDataset\nimport torchvision.transforms as T\nfrom torchvision.datasets import CIFAR10\n\nfrom omegaconf import DictConfig\n\n# Optional heavy libs are imported lazily to keep memory footprint low\n\n# -----------------------------------------------------------------------------\n# Character-level tokenizer and utilities\n# -----------------------------------------------------------------------------\n\n_CHAR_VOCAB = list(\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:\u0027\\\"/\\\\|_@#$%^\u0026*~`+-=\u003c\u003e()[]{} \\\\t\\\\n\")\n_CHAR2IDX = {c: i + 1 for i, c in enumerate(_CHAR_VOCAB)}  # 0 \u2192 padding\n\n\ndef char_tokenize(text: str, max_len: int) -\u003e torch.Tensor:\n    text = text.lower()[:max_len]\n    indices = [_CHAR2IDX.get(ch, 0) for ch in text]\n    if len(indices) \u003c max_len:\n        indices.extend([0] * (max_len - len(indices)))\n    return torch.tensor(indices, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------\n# Alpaca dataset (huggingface datasets library)\n# -----------------------------------------------------------------------------\n\nclass AlpacaDataset(TorchDataset):\n    \"\"\"Alpaca dataset wrapper supporting char or sentencepiece tokenisation.\"\"\"\n\n    def __init__(self, cfg: DictConfig, split: str):\n        from datasets import load_dataset  # lazy import\n        self.cfg = cfg\n        name = cfg.dataset.name\n        self.raw = load_dataset(\"yahma/alpaca-cleaned\", split=split)\n\n        # Determine tokenizer type\n        self.tokeniser_type = cfg.dataset.tokenizer\n        if self.tokeniser_type == \"char\":\n            self.max_len = cfg.dataset.max_seq_length\n        else:\n            from transformers import DistilBertTokenizerFast\n            self.tokeniser = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n            self.max_len = cfg.dataset.max_seq_length\n\n    def __len__(self):\n        return len(self.raw)\n\n    def __getitem__(self, idx):\n        ex = self.raw[idx]\n        text = (ex[\"instruction\"].strip() + \" \" + ex[\"input\"].strip()).strip()\n        label = 1 if len(ex[\"output\"]) \u003e 100 else 0  # simple heuristic label for demonstration\n\n        if self.tokeniser_type == \"char\":\n            return char_tokenize(text, self.max_len), torch.tensor(label, dtype=torch.long)\n        else:\n            tokens = self.tokeniser(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n            input_ids = tokens[\"input_ids\"].squeeze(0)\n            attention_mask = tokens[\"attention_mask\"].squeeze(0)\n            return (input_ids, attention_mask), torch.tensor(label, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------\n# DataModule\n# -----------------------------------------------------------------------------\n\nclass DataModule:\n    def __init__(self, cfg: DictConfig):\n        self.cfg = cfg\n        self.train_dataset = None\n        self.val_dataset = None\n        self.test_dataset = None\n\n    # ------------------------------------------------------------------\n    # Setup\n    # ------------------------------------------------------------------\n    def setup(self):\n        if self.cfg.dataset.name == \"CIFAR-10\":\n            self._setup_cifar10()\n        elif self.cfg.dataset.name == \"alpaca-cleaned\":\n            self._setup_alpaca()\n        else:\n            raise ValueError(f\"Unsupported dataset: {self.cfg.dataset.name}\")\n\n    # ------------------------------------------------------------------\n    # CIFAR\n    # ------------------------------------------------------------------\n    def _setup_cifar10(self):\n        cfg = self.cfg.dataset\n        mean, std = cfg.normalization.mean, cfg.normalization.std\n        train_aug = [T.RandomCrop(32, padding=4)] if cfg.augmentations.random_crop else []\n        if cfg.augmentations.random_flip == \"horizontal\":\n            train_aug.append(T.RandomHorizontalFlip())\n        train_aug += [T.ToTensor(), T.Normalize(mean, std)]\n        test_aug = [T.ToTensor(), T.Normalize(mean, std)]\n\n        full_train = CIFAR10(root=Path(\"data\"), train=True, download=True, transform=T.Compose(train_aug))\n        full_indices = list(range(len(full_train)))\n        train_count = cfg.splits.train\n        val_count = cfg.splits.val\n        train_indices = full_indices[:train_count]\n        val_indices = full_indices[train_count:train_count + val_count]\n\n        self.train_dataset = Subset(full_train, train_indices)\n        self.val_dataset = Subset(CIFAR10(root=Path(\"data\"), train=True, download=True, transform=T.Compose(test_aug)), val_indices)\n        self.test_dataset = CIFAR10(root=Path(\"data\"), train=False, download=True, transform=T.Compose(test_aug))\n\n    # ------------------------------------------------------------------\n    # Alpaca\n    # ------------------------------------------------------------------\n    def _setup_alpaca(self):\n        self.train_dataset = AlpacaDataset(self.cfg, split=\"train[:90%]\")\n        self.val_dataset = AlpacaDataset(self.cfg, split=\"train[90%:95%]\")\n        self.test_dataset = AlpacaDataset(self.cfg, split=\"train[95%:]\")\n\n    # ------------------------------------------------------------------\n    # Dataloaders\n    # ------------------------------------------------------------------\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.cfg.training.batch_size, shuffle=True, num_workers=2, collate_fn=self._collate)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.cfg.training.batch_size, shuffle=False, num_workers=2, collate_fn=self._collate)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.cfg.training.batch_size, shuffle=False, num_workers=2, collate_fn=self._collate)\n\n    # ------------------------------------------------------------------\n    # Collate\n    # ------------------------------------------------------------------\n    def _collate(self, batch: List):\n        if self.cfg.dataset.name == \"alpaca-cleaned\" and self.cfg.dataset.tokenizer != \"char\":\n            # ( (input_ids, attn_mask), label )\n            input_ids = torch.stack([item[0][0] for item in batch])\n            attn_mask = torch.stack([item[0][1] for item in batch])\n            labels = torch.stack([item[1] for item in batch])\n            return (input_ids, attn_mask), labels\n        else:\n            x = torch.stack([item[0] for item in batch]) if isinstance(batch[0][0], torch.Tensor) else [item[0] for item in batch]\n            y = torch.stack([item[1] for item in batch])\n            return x, y\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=42\", \"wheel\"]\n\n[project]\nname = \"comparison_experiments\"\nversion = \"0.1.0\"\ndescription = \"Lightweight model comparison experiments with Hydra \u0026 W\u0026B\"\nrequires-python = \"\u003e=3.8\"\ndependencies = [\n    \"torch\u003e=2.0.0\",\n    \"torchvision\u003e=0.15.0\",\n    \"hydra-core\u003e=1.3.2\",\n    \"omegaconf\u003e=2.3.0\",\n    \"transformers\u003e=4.33.0\",\n    \"datasets\u003e=2.12.0\",\n    \"wandb\u003e=0.15.0\",\n    \"matplotlib\u003e=3.7.0\",\n    \"pandas\u003e=2.0.0\",\n]\n", "train_py": "import os\nimport sys\nimport json\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport hydra\nfrom hydra.utils import to_absolute_path\nfrom omegaconf import DictConfig, OmegaConf\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom .preprocess import DataModule\nfrom .model import build_model\n\n# ---- W\u0026B -------------------------------------------------------------------\nimport wandb\n\n\n# -----------------------------------------------------------------------------\n# Utility helpers\n# -----------------------------------------------------------------------------\n\ndef _setup_wandb(run_cfg: DictConfig, global_cfg: DictConfig, results_dir: Path) -\u003e wandb.sdk.wandb_run.Run:\n    \"\"\"Initialise Weights \u0026 Biases and persist metadata to disk.\"\"\"\n    wandb_cfg = run_cfg.get(\"wandb\", {})\n    wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))\n    tags = wandb_cfg.get(\"tags\", [])\n    wandb_run = wandb.init(\n        entity=wandb_cfg.get(\"entity\", \"gengaru617\"),\n        project=wandb_cfg.get(\"project\", \"251014-test\"),\n        name=wandb_cfg.get(\"run_name\", run_cfg.run_id),\n        config=OmegaConf.to_container(run_cfg, resolve=True),\n        tags=tags,\n        reinit=True,\n    )\n\n    # Save metadata \u2192 .research/iterationX/wandb_metadata.json\n    iteration = os.environ.get(\"EXPERIMENT_ITERATION\", \"0\")\n    metadata_path = Path(f\".research/iteration{iteration}\")\n    metadata_path.mkdir(parents=True, exist_ok=True)\n    with open(metadata_path / \"wandb_metadata.json\", \"w\", encoding=\"utf8\") as fp:\n        json.dump({\n            \"wandb_entity\": wandb_run.entity,\n            \"wandb_project\": wandb_run.project,\n            \"wandb_run_id\": wandb_run.id,\n        }, fp, indent=2)\n\n    # Also symlink or copy to results_dir for convenience\n    try:\n        (results_dir / \"wandb_metadata.json\").write_text((metadata_path / \"wandb_metadata.json\").read_text())\n    except Exception:\n        pass\n\n    print(f\"[WandB] Run initialised: {wandb_run.url}\")\n    return wandb_run\n\n\n# -----------------------------------------------------------------------------\n# Train loop\n# -----------------------------------------------------------------------------\n\ndef train_one_epoch(model: nn.Module, loader, loss_fn, optimiser, device):\n    model.train()\n    running_loss, running_correct = 0.0, 0\n    n_samples = 0\n    for batch in loader:\n        optimiser.zero_grad()\n        inputs, labels = batch\n        if isinstance(inputs, (tuple, list)):\n            inputs = [x.to(device) for x in inputs]\n        else:\n            inputs = inputs.to(device)\n        labels = labels.to(device)\n        outputs = model(inputs)  # see model.forward signature\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        optimiser.step()\n\n        running_loss += loss.item() * labels.size(0)\n        if outputs.ndim == 2:  # classification\n            preds = outputs.argmax(1)\n            running_correct += (preds == labels).sum().item()\n        n_samples += labels.size(0)\n\n    epoch_loss = running_loss / n_samples\n    epoch_acc = running_correct / n_samples if running_correct else 0.0\n    return epoch_loss, epoch_acc\n\n\ndef evaluate(model: nn.Module, loader, loss_fn, device):\n    model.eval()\n    running_loss, running_correct = 0.0, 0\n    n_samples = 0\n    with torch.no_grad():\n        for batch in loader:\n            inputs, labels = batch\n            if isinstance(inputs, (tuple, list)):\n                inputs = [x.to(device) for x in inputs]\n            else:\n                inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, labels)\n            running_loss += loss.item() * labels.size(0)\n            if outputs.ndim == 2:\n                preds = outputs.argmax(1)\n                running_correct += (preds == labels).sum().item()\n            n_samples += labels.size(0)\n\n    epoch_loss = running_loss / n_samples\n    epoch_acc = running_correct / n_samples if running_correct else 0.0\n    return epoch_loss, epoch_acc\n\n\n# -----------------------------------------------------------------------------\n# Hydra entry point\n# -----------------------------------------------------------------------------\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    # ------------------------------------------------------------------\n    results_dir = Path(to_absolute_path(cfg.results_dir)).expanduser().absolute()\n    run_id: str = cfg.run\n    run_cfg_path = Path(__file__).resolve().parent.parent / \"config\" / \"run\" / f\"{run_id}.yaml\"\n    if not run_cfg_path.exists():\n        raise FileNotFoundError(f\"Run config not found: {run_cfg_path}\")\n\n    run_cfg: DictConfig = OmegaConf.load(run_cfg_path)\n    # Merge top-level overrides (like results_dir) back into run_cfg\n    merged_cfg = OmegaConf.merge(run_cfg, {\"results_dir\": str(results_dir)})\n\n    # ------------------------------------------------------------------\n    # Print experiment description BEFORE any numeric output\n    # ------------------------------------------------------------------\n    print(\"=\" * 80)\n    print(\"Experiment description\")\n    print(\"Run ID          :\", run_id)\n    print(\"Method          :\", merged_cfg.method)\n    print(\"Model           :\", merged_cfg.model)\n    print(\"Dataset         :\", merged_cfg.dataset.name)\n    print(\"Training params :\", merged_cfg.training)\n    print(\"=\" * 80)\n\n    # ------------------------------------------------------------------\n    # Prepare environment\n    # ------------------------------------------------------------------\n    torch.manual_seed(42)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    datamodule = DataModule(merged_cfg)\n    datamodule.setup()\n\n    model = build_model(merged_cfg).to(device)\n\n    # ------------------------------------------------------------------\n    # Optimiser / LR scheduler\n    # ------------------------------------------------------------------\n    train_cfg = merged_cfg.training\n    if train_cfg.optimizer == \"sgd\":\n        optimiser = optim.SGD(model.parameters(), lr=train_cfg.learning_rate, momentum=train_cfg.momentum, weight_decay=train_cfg.weight_decay)\n    elif train_cfg.optimizer in {\"adam\", \"adamw\"}:\n        optimiser_cls = optim.AdamW if train_cfg.optimizer == \"adamw\" else optim.Adam\n        optimiser = optimiser_cls(model.parameters(), lr=train_cfg.learning_rate, weight_decay=train_cfg.weight_decay)\n    else:\n        raise ValueError(f\"Unsupported optimiser: {train_cfg.optimizer}\")\n\n    if \"lr_scheduler\" in train_cfg and train_cfg.lr_scheduler.get(\"type\") == \"cosine\":\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=train_cfg.lr_scheduler.T_max)\n    else:\n        scheduler = None\n\n    # ------------------------------------------------------------------\n    # Criterion\n    # ------------------------------------------------------------------\n    num_classes = getattr(model, \"num_classes\", None)\n    loss_fn = nn.CrossEntropyLoss() if num_classes else nn.MSELoss()\n\n    # ------------------------------------------------------------------\n    # W\u0026B\n    # ------------------------------------------------------------------\n    wandb_run = _setup_wandb(merged_cfg, cfg, results_dir)\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    history = []\n    best_val_acc = 0.0\n    epochs = train_cfg.epochs\n    for epoch in range(1, epochs + 1):\n        start_time = time.time()\n        train_loss, train_acc = train_one_epoch(model, datamodule.train_dataloader(), loss_fn, optimiser, device)\n        val_loss, val_acc = evaluate(model, datamodule.val_dataloader(), loss_fn, device)\n        if scheduler is not None:\n            scheduler.step()\n        epoch_time = time.time() - start_time\n\n        # Logging\n        metrics = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n            \"epoch_time\": epoch_time,\n        }\n        history.append(metrics)\n        wandb_run.log(metrics, step=epoch)\n        print(json.dumps({\"run_id\": run_id, **metrics}))\n\n        # Checkpoint best\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            ckpt_path = results_dir / run_id / \"best_model.pt\"\n            ckpt_path.parent.mkdir(parents=True, exist_ok=True)\n            torch.save(model.state_dict(), ckpt_path)\n            wandb_run.log_artifact(str(ckpt_path), name=f\"{run_id}-best-model\")\n\n    # ------------------------------------------------------------------\n    # Save final metrics\n    # ------------------------------------------------------------------\n    final_results: Dict[str, Any] = {\n        \"run_id\": run_id,\n        \"best_val_acc\": best_val_acc,\n        \"history\": history,\n        \"wandb_url\": wandb_run.url,\n    }\n    out_path = results_dir / run_id / \"results.json\"\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    out_path.write_text(json.dumps(final_results, indent=2))\n    print(json.dumps(final_results))\n\n    wandb_run.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "Code is not ready for execution. Major blocking issues:\n1. Incomplete / incorrect imports:\n   • src/evaluate.py uses os.environ but never imports os.\n2. NameError in model builder:\n   • src/model.py references _CHAR2IDX without importing it from src.preprocess.\n3. Log redirection implementation broken:\n   • _stream_redirect in src/main.py calls Path.write_text(decoded, append=True) – Path.write_text has no “append” argument, causing a TypeError at runtime. No correct file‐append logic is implemented.\n4. WandB artifact logging incorrect:\n   • train.py calls wandb_run.log_artifact(str(path), name=…) but log_artifact expects a wandb.Artifact object.\n5. CharCNNAdapter shape logic very likely invalid (forces reshape to 3×32×32 regardless of embedding_dim) and will raise a runtime error if embedding_dim ≠ 3072.\n6. evaluate.py does not satisfy structured logging requirement fully and lacks run_id field; also figure upload path uses os (missing import).\n\nBecause of these errors the code will raise exceptions and cannot be executed immediately, violating “Immediate Executability”, “Complete Implementation”, and “Structured Logging” criteria."
}
