
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches: `uv run python -u -m src.main run={run_id} results_dir={path}`

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations

4. **Model Implementation**:
   - Complete model architectures for baseline, proposed, and ablation methods
   - All models implemented from scratch (no placeholders)
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - No missing files from the structure
   - All functionality contained within specified files

6. **Structured Logging**:
   - train.py prints JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py prints JSON-formatted comparison results to stdout
   - main.py redirects subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log`
   - Logs are forwarded to main process stdout/stderr for GitHub Actions capture

7. **Configuration Files**:
   - config/config.yaml contains list of all experiment run_ids
   - NOTE: config/run/{run_id}.yaml files are provided separately in the "Experiment Runs" section below (not in ExperimentCode)
   - The generated code must properly reference these config files via Hydra
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation and Figures** (if WandB not used):
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/`)
   - Figures include legends, annotations, and proper labels
   - Uses `plt.tight_layout()` before saving
   - Consistent result formatting and comparison logic

9. **WandB Integration** (if WandB is used):
   - Proper WandB initialization with entity/project from config
   - Metrics logged to WandB during training
   - WandB run URL printed to stdout
   - Metadata saved to `.research/iteration{experiment_iteration}/wandb_metadata.json`
   - Figures uploaded as WandB artifacts

10. **Immediate Executability**:
    - Code can be run immediately without modifications
    - All imports and dependencies properly specified in pyproject.toml
    - No missing external resources or undefined variables
    - Proper module structure for `uv run python -m src.main` execution

## Detection of Common Issues
Flag the following problems if found:

- **Incomplete Implementation**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns
- **Truncation**: Code sections that appear shortened or simplified inappropriately
- **Missing Functionality**: Expected features not implemented
- **Configuration Issues**: Missing or incomplete run configs
- **Import Errors**: Missing dependencies or incorrect import statements
- **Not Executable**: Code that cannot be run immediately
- **Inconsistent Structure**: Files not matching required structure
- **Logging Issues**: Missing or incorrect JSON logging format

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

# Experimental Design
- Strategy: Comparative analysis of efficient model architectures across vision and language tasks
- Proposed Method: Evaluate MobileNetV2 and DistilBERT variants for efficient deployment
- Evaluation Metrics: ['accuracy', 'f1_score', 'inference_time', 'model_size']

# Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  width_mult: 0.5
  dropout: 0.2
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
  augmentations:
    random_crop: true
    random_flip: horizontal
training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.05
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0005
  lr_scheduler:
    type: cosine
    T_max: 50
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 0.001
      high: 0.1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    momentum:
      type: uniform
      low: 0.7
      high: 0.95
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  input_adapter:
    type: char_cnn
    embedding_dim: 256
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: char
training:
  epochs: 3
  batch_size: 16
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 0.01
  gradient_accumulation_steps: 2
optuna:
  n_trials: 15
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-1

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-CIFAR-10
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
  modality_adapter:
    type: linear_patch_embedding
    patch_size: 4
    sequence_length: 64
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    patch_size:
      type: categorical
      choices: [2, 4, 8]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: sentencepiece
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "# config/config.yaml\n# -----------------------------------------------------------------------------\n# Global Hydra configuration file.  All experiment-specific parameters reside\n# in config/run/*.yaml \u2013 here we set global defaults \u0026 WandB credentials.\n# -----------------------------------------------------------------------------\n\ndefaults:\n  - run: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\n\n# Path where all experimental outputs will be saved.\nresults_dir: outputs\n\nwandb:\n  entity: gengaru617\n  project: 251014-test\n  tags: []\n  mode: online  # (online/offline/disabled)\n  resume: allow\n  log_model: true\n\n# End of file\n\n# NOTE: choose experiment with CLI override, e.g.:\n#   uv run python -u -m src.main run=comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned\n# -----------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# End\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# END OF ROOT CONFIG\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\n\n\n\n\n\n\n\n]", "evaluate_py": "\"\"\"src/evaluate.py\nAggregates results from all experimental runs and creates comparison figures under\n{results_dir}/images/.  Comparison metrics are printed to stdout in JSON format\nand also logged to Weights \u0026 Biases as a separate run.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport hydra\nimport matplotlib.pyplot as plt\nimport wandb\nfrom omegaconf import DictConfig\n\n\ndef _gather_results(results_dir: Path) -\u003e List[Dict]:\n    \"\"\"Collect all results.json files below results_dir.\"\"\"\n    results = []\n    for rjson in results_dir.glob(\"*/results.json\"):\n        results.append(json.loads(rjson.read_text()))\n    return results\n\n\ndef _plot_accuracy(results: List[Dict], image_path: Path) -\u003e None:\n    \"\"\"Create and save bar chart of test accuracy.\"\"\"\n    image_path.parent.mkdir(parents=True, exist_ok=True)\n    labels = [r[\"run_id\"] for r in results]\n    acc = [r[\"test_accuracy\"] for r in results]\n    fig, ax = plt.subplots(figsize=(8, 4))\n    bars = ax.bar(labels, acc, color=\"skyblue\")\n    ax.set_ylabel(\"Test Accuracy\")\n    ax.set_title(\"Accuracy comparison across runs\")\n    ax.set_ylim(0, 1)\n    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n    for bar, a in zip(bars, acc):\n        ax.text(bar.get_x() + bar.get_width() / 2, a + 0.01, f\"{a:.2%}\", ha=\"center\", va=\"bottom\")\n    ax.legend([\"Accuracy\"], loc=\"best\")\n    plt.tight_layout()\n    fig.savefig(image_path)\n    plt.close(fig)\n\n\n@hydra.main(config_path=\"../../config\", config_name=\"config\", version_base=\"1.3\")\ndef main(cfg: DictConfig) -\u003e None:  # type: ignore\n    results_dir = Path(cfg.results_dir)\n    results = _gather_results(results_dir)\n    if not results:\n        print(\"{}\")\n        return\n\n    # Compute best run\n    best_run = max(results, key=lambda r: r[\"test_accuracy\"])\n\n    # Save plot\n    images_dir = results_dir / \"images\"\n    plot_path = images_dir / \"comparison_accuracy.png\"\n    _plot_accuracy(results, plot_path)\n\n    # Start W\u0026B evaluation run\n    wandb_run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        name=\"evaluation\",\n        tags=[\"evaluation\"],\n        mode=cfg.wandb.mode,\n    )\n    wandb_run.log({\"comparison_accuracy\": wandb.Image(str(plot_path))})\n    wandb_run.finish()\n\n    summary = {\n        \"num_runs\": len(results),\n        \"best_run\": best_run[\"run_id\"],\n        \"best_accuracy\": best_run[\"test_accuracy\"],\n    }\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "\"\"\"src/main.py\nMain orchestrator.  It launches src.train in a subprocess with tee-like logging\nand, after completion, optionally triggers evaluation.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n# ---------------------------------------------------------------------------\n# Helper: concurrent tee to file \u0026 stdout/stderr\n# ---------------------------------------------------------------------------\n\ndef _stream_reader(src, dsts: List):\n    for line in iter(src.readline, b\"\"):\n        decoded = line.decode()\n        for d in dsts:\n            d.write(decoded)\n            d.flush()\n    src.close()\n\n\ndef tee_subprocess(cmd: List[str], log_dir: Path) -\u003e int:\n    \"\"\"Run command while streaming stdout/stderr to both console and files.\"\"\"\n    log_dir.mkdir(parents=True, exist_ok=True)\n    with open(log_dir / \"stdout.log\", \"w\") as stdout_file, open(log_dir / \"stderr.log\", \"w\") as stderr_file:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        threads = [\n            threading.Thread(target=_stream_reader, args=(proc.stdout, [sys.stdout, stdout_file]), daemon=True),\n            threading.Thread(target=_stream_reader, args=(proc.stderr, [sys.stderr, stderr_file]), daemon=True),\n        ]\n        for t in threads:\n            t.start()\n        proc.wait()\n        for t in threads:\n            t.join()\n        return proc.returncode\n\n\n# ---------------------------------------------------------------------------\n# Hydra entrypoint\n# ---------------------------------------------------------------------------\n\n\n@hydra.main(config_path=\"../../config\", config_name=\"config\", version_base=\"1.3\")\ndef main(cfg: DictConfig) -\u003e None:  # type: ignore\n    run_id = cfg.run.run_id\n    results_dir = Path(cfg.results_dir)\n    run_results_dir = results_dir / run_id\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={run_id}\",\n        f\"results_dir={str(results_dir)}\",\n    ]\n    print(f\"Launching training subprocess: {\u0027 \u0027.join(cmd)}\")\n    code = tee_subprocess(cmd, run_results_dir)\n    if code != 0:\n        raise RuntimeError(f\"Training subprocess for {run_id} failed with exit code {code}\")\n\n    # Trigger evaluation across all runs (optional)\n    eval_cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.evaluate\",\n        f\"results_dir={str(results_dir)}\",\n    ]\n    print(\"Launching evaluation \u2026\")\n    tee_subprocess(eval_cmd, results_dir / \"evaluation\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"src/model.py\nNetwork architectures and model-builder factory.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import Any\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import mobilenet_v2\nfrom transformers import DistilBertConfig, DistilBertModel\n\n\n# ---------------------------------------------------------------------------\n# MobileNetV2 wrapper\n# ---------------------------------------------------------------------------\n\n\nclass MobileNetV2Classifier(nn.Module):\n    def __init__(self, num_classes: int, width_mult: float = 1.0, dropout: float = 0.2, pretrained: bool = False):\n        super().__init__()\n        backbone = mobilenet_v2(pretrained=pretrained, width_mult=width_mult)\n        in_feat = backbone.classifier[1].in_features\n        backbone.classifier = nn.Identity()\n        self.backbone = backbone\n        self.classifier = nn.Sequential(nn.Dropout(dropout), nn.Linear(in_feat, num_classes))\n\n    def forward(self, x):\n        x = self.backbone(x)\n        return self.classifier(x)\n\n\n# ---------------------------------------------------------------------------\n# DistilBERT \u2013 image modality (simple patch embedding)\n# ---------------------------------------------------------------------------\n\n\nclass PatchEmbedding(nn.Module):\n    \"\"\"Convert image to sequence of patch embeddings.\"\"\"\n\n    def __init__(self, in_ch: int, embed_dim: int, patch_size: int, img_size: int):\n        super().__init__()\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):  # (B, C, H, W)\n        x = self.proj(x)  # (B, D, H\u0027, W\u0027)\n        x = x.flatten(2).transpose(1, 2)  # (B, N, D)\n        return x\n\n\nclass DistilBertImageClassifier(nn.Module):\n    def __init__(self, num_classes: int, patch_size: int = 4, img_size: int = 32, pretrained: bool = True):\n        super().__init__()\n        self.embed_dim = 768\n        self.patch = PatchEmbedding(3, self.embed_dim, patch_size, img_size)\n        config = DistilBertConfig()\n        self.bert = DistilBertModel(config) if not pretrained else DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.cls = nn.Linear(self.embed_dim, num_classes)\n        # Add CLS token param\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n        # Position embeddings\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch.n_patches + 1, self.embed_dim))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n\n    def forward(self, x):\n        b = x.size(0)\n        x = self.patch(x)\n        cls_tokens = self.cls_token.expand(b, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed[:, : x.size(1), :]\n        # DistilBERT expects (batch, seq_len, dim)\n        outputs = self.bert(inputs_embeds=x)\n        pooled = outputs.last_hidden_state[:, 0]  # CLS output\n        return self.cls(pooled)\n\n\n# ---------------------------------------------------------------------------\n# Simple Char-CNN for text classification (Alpaca)\n# ---------------------------------------------------------------------------\n\n\nclass CharCNNEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int = 128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.conv = nn.Conv1d(embed_dim, 256, kernel_size=7, padding=3)\n        self.pool = nn.AdaptiveMaxPool1d(1)\n\n    def forward(self, x):  # (B, L)\n        x = self.embed(x).transpose(1, 2)  # (B, D, L)\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = self.pool(x).squeeze(-1)\n        return x\n\n\nclass CharCNNClassifier(nn.Module):\n    def __init__(self, num_classes: int, vocab_size: int):\n        super().__init__()\n        self.backbone = CharCNNEmbedding(vocab_size)\n        self.head = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        return self.head(x)\n\n\n# ---------------------------------------------------------------------------\n# Model factory\n# ---------------------------------------------------------------------------\n\ndef build_model(model_cfg: Any, num_classes: int):\n    name = model_cfg.name.lower()\n    if name == \"mobilenetv2\":\n        return MobileNetV2Classifier(\n            num_classes=num_classes,\n            width_mult=model_cfg.get(\"width_mult\", 1.0),\n            dropout=model_cfg.get(\"dropout\", 0.2),\n            pretrained=model_cfg.get(\"pretrained\", False),\n        )\n    elif \"distilbert\" in name and model_cfg.get(\"modality_adapter\", None):\n        # Image modality adapter\n        adapter = model_cfg.modality_adapter\n        return DistilBertImageClassifier(\n            num_classes=num_classes,\n            patch_size=adapter.patch_size,\n            img_size=adapter.get(\"img_size\", 32),\n            pretrained=model_cfg.get(\"pretrained\", True),\n        )\n    elif \"distilbert\" in name:\n        raise NotImplementedError(\"Text DistilBERT classifier not implemented for brevity.\")\n    elif model_cfg.get(\"input_adapter\", {}).get(\"type\") == \"char_cnn\":\n        vocab_size = 101  # printable + PAD\n        return CharCNNClassifier(num_classes=num_classes, vocab_size=vocab_size)\n    else:\n        raise ValueError(f\"Unsupported model config: {model_cfg}\")\n", "preprocess_py": "\"\"\"src/preprocess.py\nFull data-loading \u0026 preprocessing utilities for all supported datasets.\n\"\"\"\nfrom __future__ import annotations\n\nimport string\nfrom pathlib import Path\nfrom typing import Tuple, List\n\nimport torch\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\n\n\n# ---------------------------------------------------------------------------\n# Vision \u2013 CIFAR-10\n# ---------------------------------------------------------------------------\n\ndef _cifar_transforms(cfg, train: bool):\n    mean, std = cfg.normalization.mean, cfg.normalization.std\n    t: List = []\n    if train and cfg.augmentations.random_crop:\n        t.append(transforms.RandomCrop(cfg.image_size, padding=4))\n    if train and cfg.augmentations.random_flip == \"horizontal\":\n        t.append(transforms.RandomHorizontalFlip())\n    t.extend(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std),\n        ]\n    )\n    return transforms.Compose(t)\n\n\ndef _get_cifar_dataloaders(dataset_cfg, train_cfg):  # noqa: C901\n    root = Path(\".data/cifar10\")\n    root.mkdir(parents=True, exist_ok=True)\n    full_train = CIFAR10(root=root, train=True, download=True, transform=_cifar_transforms(dataset_cfg, True))\n    test_set = CIFAR10(root=root, train=False, download=True, transform=_cifar_transforms(dataset_cfg, False))\n    # Split training into train/val\n    val_size = dataset_cfg.splits.val\n    train_size = dataset_cfg.splits.train\n    assert train_size + val_size == len(full_train)\n    train_set, val_set = random_split(full_train, [train_size, val_size])\n    # Re-apply transform for val set without augmentation\n    val_set.dataset.transform = _cifar_transforms(dataset_cfg, False)\n\n    def loader(ds, shuffle):\n        return DataLoader(\n            ds,\n            batch_size=train_cfg.batch_size,\n            shuffle=shuffle,\n            num_workers=2,\n            pin_memory=True,\n        )\n\n    return (\n        loader(train_set, True),\n        loader(val_set, False),\n        loader(test_set, False),\n        10,  # num_classes\n    )\n\n\n# ---------------------------------------------------------------------------\n# NLP \u2013 Alpaca Cleaned\n# ---------------------------------------------------------------------------\n\n_CHAR_VOCAB = {c: i + 1 for i, c in enumerate(string.printable)}  # 0 reserved for PAD\n_PAD_IDX = 0\n\n\nclass _AlpacaCharDataset(Dataset):\n    def __init__(self, split, max_len: int):\n        self.ds = load_dataset(\"yahma/alpaca-cleaned\", split=split)\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.ds)\n\n    def _encode(self, text: str):\n        ids = [_CHAR_VOCAB.get(c, 0) for c in text.lower()[: self.max_len]]\n        if len(ids) \u003c self.max_len:\n            ids.extend([_PAD_IDX] * (self.max_len - len(ids)))\n        return torch.tensor(ids, dtype=torch.long)\n\n    def __getitem__(self, idx):\n        rec = self.ds[idx]\n        text = (rec[\"instruction\"] + \" \" + rec.get(\"input\", \"\")).strip()\n        label = 0  # Alpaca has no class label; placeholder for demonstration\n        return self._encode(text), torch.tensor(label)\n\n\ndef _get_alpaca_dataloaders(dataset_cfg, train_cfg):\n    max_len = dataset_cfg.max_seq_length\n    full = _AlpacaCharDataset(\"train\", max_len)\n    train_size = int(0.9 * len(full))\n    val_size = len(full) - train_size\n    train_set, val_set = random_split(full, [train_size, val_size])\n\n    def loader(ds, shuffle):\n        return DataLoader(\n            ds,\n            batch_size=train_cfg.batch_size,\n            shuffle=shuffle,\n            num_workers=2,\n            pin_memory=True,\n        )\n\n    # No official test split \u2013 reuse val as test\n    return loader(train_set, True), loader(val_set, False), loader(val_set, False), 1\n\n\n# ---------------------------------------------------------------------------\n# Public API\n# ---------------------------------------------------------------------------\n\ndef get_dataloaders(cfg) -\u003e Tuple:\n    \"\"\"Return (train_loader, val_loader, test_loader, num_classes).\"\"\"\n    if cfg.dataset.name.lower() == \"cifar-10\":\n        return _get_cifar_dataloaders(cfg.dataset, cfg.training)\n    elif cfg.dataset.name.lower() == \"alpaca-cleaned\":\n        return _get_alpaca_dataloaders(cfg.dataset, cfg.training)\n    else:\n        raise ValueError(f\"Unsupported dataset {cfg.dataset.name}\")\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"lightweight_models_experiments\"\nversion = \"0.1.0\"\ndependencies = [\n    \"torch\u003e=1.13.0\",\n    \"torchvision\u003e=0.14.0\",\n    \"transformers\u003e=4.33.0\",\n    \"datasets\u003e=2.13.0\",\n    \"hydra-core\u003e=1.3.2\",\n    \"omegaconf\u003e=2.3.0\",\n    \"wandb\u003e=0.15.8\",\n    \"matplotlib\u003e=3.8.0\",\n    \"tqdm\u003e=4.66.1\",\n    \"scikit-learn\u003e=1.3.2\",\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n", "train_py": "\"\"\"src/train.py\nTraining script executed as a subprocess by src.main.  \nIt trains a single experimental run specified by the Hydra run config and\nsaves fully-structured JSON metrics as well as model checkpoints.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport shutil\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, Tuple, List\n\nimport hydra\nimport torch\nimport torch.nn.functional as F\nimport wandb\nfrom omegaconf import DictConfig, OmegaConf\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports must be absolute \u2013 we assume src is on PYTHONPATH\nfrom src import model as model_lib  # noqa: E402\nfrom src import preprocess as prep  # noqa: E402\n\n\n# ---------------------------------------------------------------------------\n# Utility helpers\n# ---------------------------------------------------------------------------\n\ndef _set_seed(seed: int = 42) -\u003e None:\n    \"\"\"Deterministic training for reproducibility.\"\"\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    import random, numpy as np  # noqa: E402\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n# ---------------------------------------------------------------------------\n# Core training / evaluation functions\n# ---------------------------------------------------------------------------\n\ndef _train_one_epoch(\n    epoch: int,\n    model: torch.nn.Module,\n    loader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n) -\u003e Tuple[float, float]:\n    \"\"\"Train for exactly one epoch and return (loss, accuracy).\"\"\"\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    loop = tqdm(loader, desc=f\"[Train] Epoch {epoch}\", leave=False)\n    for batch in loop:\n        inputs, labels = batch\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        out = model(inputs)\n        loss = F.cross_entropy(out, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        preds = out.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += inputs.size(0)\n        loop.set_postfix({\"loss\": loss.item()})\n    return running_loss / total, correct / total\n\n\ndef _eval(\n    epoch: int,\n    split: str,\n    model: torch.nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n) -\u003e Tuple[float, float]:\n    \"\"\"Evaluate on validation/test split.\"\"\"\n    model.eval()\n    loss, correct, total = 0.0, 0, 0\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            out = model(inputs)\n            l = F.cross_entropy(out, labels)\n            loss += l.item() * inputs.size(0)\n            preds = out.argmax(dim=1)\n            correct += (preds == labels).sum().item()\n            total += inputs.size(0)\n    return loss / total, correct / total\n\n\n# ---------------------------------------------------------------------------\n# Hydra main entrypoint\n# ---------------------------------------------------------------------------\n\n\n@hydra.main(config_path=\"../../config\", config_name=\"config\", version_base=\"1.3\")\ndef main(cfg: DictConfig) -\u003e None:  # noqa: C901\n    \"\"\"Single-run training orchestrator.\"\"\"\n    run_cfg = cfg.run  # run-specific parameters\n    results_dir = Path(cfg.results_dir) / run_cfg.run_id\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ---------------------------------------------------------------------\n    # 1.  Reproducibility \u0026 device\n    # ---------------------------------------------------------------------\n    _set_seed(42)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ---------------------------------------------------------------------\n    # 2.  Initialise Weights \u0026 Biases\n    # ---------------------------------------------------------------------\n    wandb_cfg = {\n        \"entity\": cfg.wandb.entity,\n        \"project\": cfg.wandb.project,\n        \"name\": run_cfg.run_id,\n        \"tags\": cfg.wandb.tags + [run_cfg.method],\n        \"config\": OmegaConf.to_container(run_cfg, resolve=True),\n        \"mode\": cfg.wandb.mode,\n        \"resume\": cfg.wandb.resume,\n        \"save_code\": True,\n    }\n    wandb_run = wandb.init(**wandb_cfg)\n\n    # Save metadata immediately (required)\n    iteration = os.getenv(\"EXPERIMENT_ITERATION\", \"0\")\n    metadata_path = Path(\".research\") / f\"iteration{iteration}\" / \"wandb_metadata.json\"\n    metadata_path.parent.mkdir(parents=True, exist_ok=True)\n    metadata = {\n        \"wandb_entity\": cfg.wandb.entity,\n        \"wandb_project\": cfg.wandb.project,\n        \"wandb_run_id\": wandb_run.id,\n    }\n    metadata_path.write_text(json.dumps(metadata, indent=2))\n\n    # ---------------------------------------------------------------------\n    # 3.  Data\n    # ---------------------------------------------------------------------\n    train_loader, val_loader, test_loader, num_classes = prep.get_dataloaders(run_cfg)\n\n    # ---------------------------------------------------------------------\n    # 4.  Model\n    # ---------------------------------------------------------------------\n    model = model_lib.build_model(run_cfg.model, num_classes=num_classes)\n    model.to(device)\n\n    # ---------------------------------------------------------------------\n    # 5.  Optimiser \u0026 scheduler\n    # ---------------------------------------------------------------------\n    optim_name = run_cfg.training.optimizer.lower()\n    lr = run_cfg.training.learning_rate\n    wd = run_cfg.training.get(\"weight_decay\", 0.0)\n    if optim_name == \"sgd\":\n        optimizer = torch.optim.SGD(\n            model.parameters(),\n            lr=lr,\n            momentum=run_cfg.training.get(\"momentum\", 0.9),\n            weight_decay=wd,\n        )\n    elif optim_name in {\"adam\", \"adamw\"}:\n        Adam = torch.optim.AdamW if optim_name == \"adamw\" else torch.optim.Adam\n        optimizer = Adam(model.parameters(), lr=lr, weight_decay=wd)\n    else:\n        raise ValueError(f\"Unsupported optimiser: {optim_name}\")\n\n    sched_cfg = run_cfg.training.get(\"lr_scheduler\")\n    if sched_cfg is None:\n        scheduler = None\n    else:\n        if sched_cfg.type == \"cosine\":\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=sched_cfg.T_max)\n        elif sched_cfg.type == \"linear\":\n            total_steps = len(train_loader) * run_cfg.training.epochs\n            scheduler = torch.optim.lr_scheduler.LinearLR(\n                optimizer,\n                start_factor=0.0,\n                total_iters=sched_cfg.warmup_steps,\n            )\n        else:\n            raise ValueError(f\"Unsupported scheduler {sched_cfg.type}\")\n\n    # ---------------------------------------------------------------------\n    # 6.  Training loop\n    # ---------------------------------------------------------------------\n    history: List[Dict[str, float]] = []\n    best_val_acc, best_state = 0.0, None\n\n    for epoch in range(1, run_cfg.training.epochs + 1):\n        train_loss, train_acc = _train_one_epoch(epoch, model, train_loader, optimizer, device)\n        val_loss, val_acc = _eval(epoch, \"val\", model, val_loader, device)\n        if scheduler is not None:\n            scheduler.step()\n\n        # Logging\n        metrics = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n            \"lr\": optimizer.param_groups[0][\"lr\"],\n        }\n        wandb.log(metrics, step=epoch)\n        history.append(metrics)\n\n        # keep best checkpoint\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            best_state = {\n                \"model_state\": model.state_dict(),\n                \"epoch\": epoch,\n                \"val_acc\": val_acc,\n            }\n\n    # ---------------------------------------------------------------------\n    # 7.  Test evaluation\n    # ---------------------------------------------------------------------\n    model.load_state_dict(best_state[\"model_state\"])\n    test_loss, test_acc = _eval(0, \"test\", model, test_loader, device)\n    wandb.summary.update({\"best_val_acc\": best_val_acc, \"test_acc\": test_acc})\n\n    # ---------------------------------------------------------------------\n    # 8.  Save checkpoint \u0026 metrics\n    # ---------------------------------------------------------------------\n    ckpt_path = results_dir / \"best_model.pt\"\n    torch.save(best_state, ckpt_path)\n    if cfg.wandb.log_model:\n        wandb.save(str(ckpt_path))\n\n    results = {\n        \"run_id\": run_cfg.run_id,\n        \"best_val_acc\": best_val_acc,\n        \"test_accuracy\": test_acc,\n        \"test_loss\": test_loss,\n        \"history\": history,\n    }\n    (results_dir / \"results.json\").write_text(json.dumps(results, indent=2))\n    print(json.dumps(results))\n\n    # Finish W\u0026B\n    print(f\"WandB URL: {wandb_run.url}\")\n    wandb.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "Code does not satisfy multiple mandatory criteria:\n1. Incomplete model implementation – text DistilBERT branch in src/model.py raises NotImplementedError, which breaks run comparative-2-DistilBERT-base-66M-alpaca-cleaned.\n2. Data pipeline for Alpaca creates single-class labels, leading to invalid cross-entropy training and meaningless results.\n3. config/config.yaml lists only one run_id; it must enumerate all four provided runs and the associated files config/run/*.yaml must exist inside the repo. At present they are missing, so Hydra will fail at startup.\n4. Evaluation figure saved as .png, not PDF as required.\n5. Minor: CrossEntropy with num_classes=1 is invalid and will crash if reached.\nBecause of these issues, the code is not immediately executable or publication-ready."
}
