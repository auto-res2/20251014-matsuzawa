
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches: `uv run python -u -m src.main run={run_id} results_dir={path}`

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations

4. **Model Implementation**:
   - Complete model architectures for baseline, proposed, and ablation methods
   - All models implemented from scratch (no placeholders)
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - No missing files from the structure
   - All functionality contained within specified files

6. **Structured Logging**:
   - train.py prints JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py prints JSON-formatted comparison results to stdout
   - main.py redirects subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log`
   - Logs are forwarded to main process stdout/stderr for GitHub Actions capture

7. **Configuration Files**:
   - config/config.yaml contains list of all experiment run_ids
   - NOTE: config/run/{run_id}.yaml files are provided separately in the "Experiment Runs" section below (not in ExperimentCode)
   - The generated code must properly reference these config files via Hydra
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation and Figures** (if WandB not used):
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/`)
   - Figures include legends, annotations, and proper labels
   - Uses `plt.tight_layout()` before saving
   - Consistent result formatting and comparison logic

9. **WandB Integration** (if WandB is used):
   - Proper WandB initialization with entity/project from config
   - Metrics logged to WandB during training
   - WandB run URL printed to stdout
   - Metadata saved to `.research/iteration{experiment_iteration}/wandb_metadata.json`
   - Figures uploaded as WandB artifacts

10. **Immediate Executability**:
    - Code can be run immediately without modifications
    - All imports and dependencies properly specified in pyproject.toml
    - No missing external resources or undefined variables
    - Proper module structure for `uv run python -m src.main` execution

## Detection of Common Issues
Flag the following problems if found:

- **Incomplete Implementation**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns
- **Truncation**: Code sections that appear shortened or simplified inappropriately
- **Missing Functionality**: Expected features not implemented
- **Configuration Issues**: Missing or incomplete run configs
- **Import Errors**: Missing dependencies or incorrect import statements
- **Not Executable**: Code that cannot be run immediately
- **Inconsistent Structure**: Files not matching required structure
- **Logging Issues**: Missing or incorrect JSON logging format

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

# Experimental Design
- Strategy: Comparative analysis of efficient model architectures across vision and language tasks
- Proposed Method: Evaluate MobileNetV2 and DistilBERT variants for efficient deployment
- Evaluation Metrics: ['accuracy', 'f1_score', 'inference_time', 'model_size']

# Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  width_mult: 0.5
  dropout: 0.2
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
  augmentations:
    random_crop: true
    random_flip: horizontal
training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.05
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0005
  lr_scheduler:
    type: cosine
    T_max: 50
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 0.001
      high: 0.1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    momentum:
      type: uniform
      low: 0.7
      high: 0.95
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  input_adapter:
    type: char_cnn
    embedding_dim: 256
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: char
training:
  epochs: 3
  batch_size: 16
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 0.01
  gradient_accumulation_steps: 2
optuna:
  n_trials: 15
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-1

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-CIFAR-10
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
  modality_adapter:
    type: linear_patch_embedding
    patch_size: 4
    sequence_length: 64
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    patch_size:
      type: categorical
      choices: [2, 4, 8]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: sentencepiece
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "# config/config.yaml\n# Root Hydra configuration file. Lists available runs so they can be queried\n# programmatically and sets a default.\n\ndefaults:\n  - run: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\n\n# List of all run identifiers (for convenience)\navailable_runs:\n  - comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\n  - comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned\n  - comparative-2-DistilBERT-base-66M-CIFAR-10\n  - comparative-2-DistilBERT-base-66M-alpaca-cleaned\n\nevaluate: true\n\nresults_dir: runs_output\n", "evaluate_py": "\"\"\"src/evaluate.py\nAggregates result.json files across experiment runs and generates comparison\nfigures. Results are both saved locally and uploaded to WandB as artefacts.\n\"\"\"\nfrom __future__ import annotations\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport hydra\nfrom omegaconf import DictConfig\n\nimport matplotlib\nmatplotlib.use(\"Agg\")  # Headless backend\nimport matplotlib.pyplot as plt\n\nimport wandb\n\n\ndef collect_results(results_dir: Path) -\u003e List[Dict]:\n    records = []\n    for res_file in results_dir.rglob(\"results.json\"):\n        with res_file.open() as f:\n            data = json.load(f)\n            final = data[\"final\"]\n            records.append({\n                \"run_id\": final[\"run_id\"],\n                \"test_acc\": final[\"test_acc\"],\n                \"test_loss\": final[\"test_loss\"],\n            })\n    return records\n\n\ndef plot_accuracy(records: List[Dict], save_path: Path):\n    runs = [r[\"run_id\"] for r in records]\n    accs = [r[\"test_acc\"] for r in records]\n    plt.figure(figsize=(10, 6))\n    plt.bar(runs, accs)\n    plt.ylabel(\"Test Accuracy\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    plt.savefig(save_path)\n    plt.close()\n\n\n@hydra.main(config_path=\"..\\/config\", config_name=\"config\", version_base=\"1.3\")\ndef _main(cfg: DictConfig):\n    results_dir = Path(cfg.results_dir)\n    records = collect_results(results_dir)\n\n    # Output comparison as JSON\n    summary = {r[\"run_id\"]: {\"accuracy\": r[\"test_acc\"], \"loss\": r[\"test_loss\"]} for r in records}\n    print(json.dumps(summary, indent=2))\n\n    # Plot\n    fig_path = results_dir / \"comparison_accuracy.png\"\n    plot_accuracy(records, fig_path)\n\n    # Upload to WandB if WANDB_API_KEY available\n    if os.environ.get(\"WANDB_API_KEY\"):\n        wandb_run = wandb.init(\n            project=\"251014-test\",\n            entity=\"gengaru617\",\n            name=\"evaluation\",\n            mode=\"online\",\n        )\n        wandb.log({\"comparison_accuracy\": wandb.Image(str(fig_path))})\n        wandb.save(str(fig_path))\n        wandb.finish()\n\n\nif __name__ == \"__main__\":\n    _main()\n", "main_py": "\"\"\"src/main.py\nMain orchestrator: launches training run (train.py) as subprocess, captures\nstdout/stderr to tee-style logs, and optionally calls evaluator.\n\"\"\"\nfrom __future__ import annotations\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\n\n\ndef tee_subprocess(cmd: List[str], log_stdout: Path, log_stderr: Path):\n    \"\"\"Execute cmd, tee stdout/stderr to both terminal and log files.\"\"\"\n    with log_stdout.open(\"w\") as out_f, log_stderr.open(\"w\") as err_f:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)\n        # Stream line by line\n        assert process.stdout is not None and process.stderr is not None\n        for line in process.stdout:\n            sys.stdout.write(line)\n            out_f.write(line)\n        for line in process.stderr:\n            sys.stderr.write(line)\n            err_f.write(line)\n        process.wait()\n        if process.returncode != 0:\n            raise RuntimeError(f\"Subprocess {\u0027 \u0027.join(cmd)} exited with code {process.returncode}\")\n\n\n@hydra.main(config_path=\"..\\/config\", config_name=\"config\", version_base=\"1.3\")\ndef _main(cfg: DictConfig):\n    # Build command to run training\n    run_id = cfg.run\n    results_root = Path(cfg.results_dir).expanduser().resolve()\n    run_dir = results_root / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={run_id}\",\n        f\"results_dir={str(run_dir)}\",\n    ]\n\n    tee_subprocess(cmd, run_dir / \"stdout.log\", run_dir / \"stderr.log\")\n\n    # After completion, optionally run evaluation across all results\n    if cfg.get(\"evaluate\", True):\n        eval_cmd = [\n            sys.executable,\n            \"-u\",\n            \"-m\",\n            \"src.evaluate\",\n            f\"results_dir={str(results_root)}\",\n        ]\n        tee_subprocess(eval_cmd, results_root / \"evaluation_stdout.log\", results_root / \"evaluation_stderr.log\")\n\n\nif __name__ == \"__main__\":\n    _main()\n", "model_py": "\"\"\"src/model.py\nContains all model architectures required for experiments: MobileNetV2 (vision),\nDistilBERT (vision \u0026 text). A helper `get_model(cfg, num_classes)` constructs\nthe right model instance for a given Hydra config.\n\"\"\"\nfrom __future__ import annotations\nimport math\n\nimport torch\nfrom torch import nn\nfrom torchvision.models import mobilenet_v2\n\nfrom transformers import DistilBertModel, DistilBertConfig\n\n\n# ------------------------------ MobileNetV2 ------------------------------ #\n\nclass MobileNetV2Classifier(nn.Module):\n    def __init__(self, num_classes: int, width_mult: float = 1.0, pretrained: bool = False, dropout: float = 0.2):\n        super().__init__()\n        self.mnet = mobilenet_v2(weights=\"IMAGENET1K_V1\" if pretrained else None, width_mult=width_mult)\n        in_features = self.mnet.classifier[1].in_features\n        self.mnet.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(in_features, num_classes),\n        )\n\n    def forward(self, x):\n        return self.mnet(x)\n\n\n# --------------------------- DistilBERT \u2011 Vision -------------------------- #\n\nclass PatchEmbedding(nn.Module):\n    \"\"\"Split image into patches \u0026 project to hidden dim.\"\"\"\n\n    def __init__(self, in_channels: int, hidden_size: int, patch_size: int):\n        super().__init__()\n        self.patch_size = patch_size\n        self.proj = nn.Conv2d(in_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):  # N C H W\n        x = self.proj(x)  # N hidden H\u0027 W\u0027\n        x = x.flatten(2).transpose(1, 2)  # N (H\u0027W\u0027) hidden\n        return x\n\n\nclass DistilBERTImageClassifier(nn.Module):\n    def __init__(self, num_classes: int, patch_size: int = 4, sequence_length: int = 64, pretrained: bool = True):\n        super().__init__()\n        config = DistilBertConfig()\n        self.backbone = DistilBertModel.from_pretrained(\"distilbert-base-uncased\") if pretrained else DistilBertModel(config)\n        self.hidden_size = self.backbone.config.dim\n        self.patch_embed = PatchEmbedding(3, self.hidden_size, patch_size)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.hidden_size))\n        self.pos_embed = nn.Parameter(torch.zeros(1, sequence_length + 1, self.hidden_size))\n        self.layernorm = nn.LayerNorm(self.hidden_size)\n        self.classifier = nn.Linear(self.hidden_size, num_classes)\n\n    def forward(self, x):\n        bs = x.size(0)\n        x = self.patch_embed(x)  # bs seq hidden\n        cls_tokens = self.cls_token.expand(bs, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed[:, : x.size(1), :]\n        x = self.layernorm(x)\n        out = self.backbone(inputs_embeds=x, attention_mask=torch.ones(x.size()[:-1], device=x.device)).last_hidden_state\n        cls_out = out[:, 0]\n        return self.classifier(cls_out)\n\n\n# --------------------------- DistilBERT \u2011 Text --------------------------- #\n\nclass DistilBERTTextClassifier(nn.Module):\n    def __init__(self, num_classes: int, vocab_size: int | None = None, pretrained: bool = True):\n        super().__init__()\n        if pretrained:\n            self.backbone = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        else:\n            cfg = DistilBertConfig(vocab_size=vocab_size or 32000)\n            self.backbone = DistilBertModel(cfg)\n        if vocab_size is not None and vocab_size != self.backbone.config.vocab_size:\n            self.backbone.resize_token_embeddings(vocab_size)\n        self.hidden_size = self.backbone.config.dim\n        self.classifier = nn.Linear(self.hidden_size, num_classes)\n\n    def forward(self, input_ids):\n        attn_mask = (input_ids != 0).long()\n        out = self.backbone(input_ids=input_ids, attention_mask=attn_mask).last_hidden_state\n        cls_out = out[:, 0]\n        return self.classifier(cls_out)\n\n\n# ------------------------------ factory --------------------------------- #\n\ndef get_model(cfg, num_classes: int):\n    name = cfg.model.name.lower()\n    if \"mobilenetv2\" in name:\n        return MobileNetV2Classifier(\n            num_classes=num_classes,\n            width_mult=cfg.model.width_mult,\n            pretrained=cfg.model.pretrained,\n            dropout=cfg.model.dropout,\n        )\n    elif \"distilbert\" in name:\n        # Distinguish modality by dataset name\n        if \"cifar\" in cfg.dataset.name.lower():\n            return DistilBERTImageClassifier(\n                num_classes=num_classes,\n                patch_size=cfg.model.modality_adapter.patch_size,\n                sequence_length=cfg.model.modality_adapter.sequence_length,\n                pretrained=cfg.model.pretrained,\n            )\n        else:  # Text\n            vocab_size = None\n            if cfg.dataset.tokenizer == \"sentencepiece\":\n                # For SentencePiece we trained a new vocab of size 8000 (see preprocess)\n                vocab_size = 8000 + 3  # pad + bos + eos\n            return DistilBERTTextClassifier(\n                num_classes=num_classes,\n                vocab_size=vocab_size,\n                pretrained=cfg.model.pretrained,\n            )\n    else:\n        raise ValueError(f\"Unknown model name {cfg.model.name}\")\n", "preprocess_py": "\"\"\"src/preprocess.py\nComplete preprocessing pipeline for image (CIFAR-10) and text (alpaca-cleaned)\ndatasets. Provides get_datasets(cfg) that returns train/val/test datasets and\nnum_classes.\n\"\"\"\nfrom __future__ import annotations\nimport json\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nimport torch\nfrom torch.utils.data import Dataset, random_split\nfrom torchvision import transforms as T\nfrom torchvision.datasets import CIFAR10\n\n# ------------------------------ Tokenisers ------------------------------ #\n\nclass CharTokenizer:\n    \"\"\"Character-level tokenizer suitable for quick prototyping.\"\"\"\n\n    def __init__(self):\n        # Reserve 0 for padding\n        self.stoi = {ch: i + 1 for i, ch in enumerate(list(\"abcdefghijklmnopqrstuvwxyz0123456789 .,;!?\"))}\n        self.itos = {i: s for s, i in self.stoi.items()}\n        self.pad_id = 0\n\n    def encode(self, text: str, max_length: int):\n        ids = [self.stoi.get(ch.lower(), 1) for ch in text][:max_length]\n        padding = [self.pad_id] * (max_length - len(ids))\n        return ids + padding\n\n\nclass SentencePieceTokenizer:\n    \"\"\"SentencePiece tokenizer; trains on the fly if model file not found.\"\"\"\n\n    def __init__(self, model_path: Path, vocab_size: int = 8000):\n        import sentencepiece as spm  # Lazy import\n\n        self.processor = spm.SentencePieceProcessor()\n        if model_path.exists():\n            self.processor.Load(str(model_path))\n        else:\n            # Train quickly on a tiny synthetic corpus to ensure availability\n            fake_corpus = str(model_path.with_suffix(\".txt\"))\n            with open(fake_corpus, \"w\", encoding=\"utf-8\") as f:\n                for _ in range(1000):\n                    f.write(\"hello world .\\n\")\n            spm.SentencePieceTrainer.Train(\n                input=fake_corpus,\n                model_prefix=str(model_path.with_suffix(\"\")),\n                vocab_size=vocab_size,\n                model_type=\"unigram\",\n                character_coverage=1.0,\n                bos_id=1,\n                eos_id=2,\n            )\n            self.processor.Load(str(model_path))\n        self.pad_id = 0\n\n    def encode(self, text: str, max_length: int):\n        ids = self.processor.EncodeAsIds(text)[:max_length]\n        padding = [self.pad_id] * (max_length - len(ids))\n        return ids + padding\n\n\n# ------------------------------ Datasets ------------------------------ #\n\nclass AlpacaDataset(Dataset):\n    \"\"\"Simple instruction-following dataset for classification.\n    For demonstration the label is the length bucket of the output text.\n    \"\"\"\n\n    def __init__(self, records: List[dict], tokenizer, max_seq_length: int = 512):\n        self.records = records\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, idx):\n        item = self.records[idx]\n        text = (item.get(\"instruction\", \"\") + \" \" + item.get(\"input\", \"\")).strip()\n        label = 0 if len(item.get(\"output\", \"\")) \u003c 50 else 1  # Binary label based on output length\n        input_ids = torch.tensor(self.tokenizer.encode(text, self.max_seq_length))\n        return input_ids, torch.tensor(label, dtype=torch.long)\n\n\n# ------------------------------ Public API ------------------------------ #\n\ndef get_cifar10(cfg) -\u003e Tuple[Dataset, Dataset, Dataset, int]:\n    tfms_train = [\n        T.RandomCrop(cfg.dataset.image_size, padding=4) if cfg.dataset.augmentations.random_crop else None,\n        T.RandomHorizontalFlip() if cfg.dataset.augmentations.random_flip == \"horizontal\" else None,\n        T.ToTensor(),\n        T.Normalize(mean=cfg.dataset.normalization.mean, std=cfg.dataset.normalization.std),\n    ]\n    tfms_train = T.Compose([t for t in tfms_train if t is not None])\n    tfms_test = T.Compose([\n        T.ToTensor(),\n        T.Normalize(mean=cfg.dataset.normalization.mean, std=cfg.dataset.normalization.std),\n    ])\n    root = Path(\"data/cifar10\")\n    train_full = CIFAR10(root=root.as_posix(), train=True, transform=tfms_train, download=True)\n    test_ds = CIFAR10(root=root.as_posix(), train=False, transform=tfms_test, download=True)\n    # Split train into train/val according to cfg\n    val_size = cfg.dataset.splits.val\n    train_size = cfg.dataset.splits.train\n    assert train_size + val_size == len(train_full)\n    train_ds, val_ds = random_split(train_full, [train_size, val_size])\n    return train_ds, val_ds, test_ds, 10\n\n\ndef load_alpaca_json(path: Path) -\u003e List[dict]:\n    if not path.exists():\n        # Create a tiny fake dataset (ensures code runs in CI)\n        fake = []\n        for _ in range(200):\n            fake.append({\n                \"instruction\": \"Say hello\",\n                \"input\": \"world\",\n                \"output\": \"hello world\" if random.random() \u003c 0.5 else \"greetings earth\",\n            })\n        return fake\n    with path.open() as f:\n        return json.load(f)\n\n\ndef get_alpaca(cfg) -\u003e Tuple[Dataset, Dataset, Dataset, int]:\n    tokenizer_type = cfg.dataset.tokenizer\n    if tokenizer_type == \"char\":\n        tokenizer = CharTokenizer()\n    elif tokenizer_type == \"sentencepiece\":\n        model_path = Path(\"data/spm_alpaca.model\")\n        tokenizer = SentencePieceTokenizer(model_path)\n    else:\n        raise ValueError(f\"Unsupported tokenizer {tokenizer_type}\")\n\n    data_path = Path(cfg.dataset.get(\"path\", \"data/alpaca-cleaned.json\"))\n    records = load_alpaca_json(data_path)\n\n    dataset = AlpacaDataset(records, tokenizer, cfg.dataset.max_seq_length)\n    n_total = len(dataset)\n    train_end = int(0.9 * n_total)\n    val_end = int(0.95 * n_total)\n    train_ds, val_ds, test_ds = torch.utils.data.random_split(\n        dataset, [train_end, val_end - train_end, n_total - val_end]\n    )\n    num_classes = 2\n    return train_ds, val_ds, test_ds, num_classes\n\n\ndef get_datasets(cfg):\n    name = cfg.dataset.name.lower()\n    if name == \"cifar-10\" or name == \"cifar10\":\n        return get_cifar10(cfg)\n    elif name == \"alpaca-cleaned\":\n        return get_alpaca(cfg)\n    else:\n        raise ValueError(f\"Unsupported dataset: {name}\")\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=42\", \"wheel\"]\n\n[project]\nname = \"comparative_experiments\"\nversion = \"0.1.0\"\ndescription = \"Lightweight model comparison experiments\"\nauthors = [{name = \"AI Researcher\"}]\nrequires-python = \"\u003e=3.8\"\ndependencies = [\n    \"torch\u003e=1.13.0\",\n    \"torchvision\u003e=0.14.0\",\n    \"transformers\u003e=4.25.0\",\n    \"sentencepiece\u003e=0.1.97\",\n    \"hydra-core\u003e=1.3.0\",\n    \"optuna\u003e=3.4.0\",\n    \"wandb\u003e=0.15.0\",\n    \"matplotlib\u003e=3.7.0\",\n    \"scikit-learn\u003e=1.2.0\",\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n", "train_py": "\"\"\"src/train.py\nTraining script executed by Hydra. Handles a single experiment run specified\nby run_id. Implements full training loop, Optuna optimisation (if enabled),\nWandB tracking, JSON logging, and result serialisation.\n\"\"\"\nfrom __future__ import annotations\nimport json\nimport os\nimport shutil\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Dict, Tuple, Union\n\nimport hydra\nfrom hydra.utils import instantiate, call\nfrom omegaconf import DictConfig, OmegaConf\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.optim import SGD, Adam, AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\n\nimport optuna\n\nimport wandb\n\n# Local imports MUST be relative to the package root when using -m\nfrom .preprocess import get_datasets\nfrom .model import get_model\n\n# -----------------------------------------------------------------------------\n# Helper utilities\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed: int = 42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    import random, numpy as np  # pylint: disable=import-error\n    random.seed(seed)\n    np.random.seed(seed)\n\n\ndef prepare_optimizer(model: nn.Module, cfg: DictConfig):\n    params = [p for p in model.parameters() if p.requires_grad]\n    opt_name = cfg.training.optimizer.lower()\n    if opt_name == \"sgd\":\n        opt = SGD(\n            params,\n            lr=cfg.training.learning_rate,\n            momentum=cfg.training.momentum,\n            weight_decay=cfg.training.weight_decay,\n        )\n    elif opt_name == \"adam\":\n        opt = Adam(params, lr=cfg.training.learning_rate, weight_decay=cfg.training.weight_decay)\n    elif opt_name == \"adamw\":\n        opt = AdamW(params, lr=cfg.training.learning_rate, weight_decay=cfg.training.weight_decay)\n    else:\n        raise ValueError(f\"Unsupported optimizer: {opt_name}\")\n    return opt\n\n\ndef prepare_scheduler(optimizer, cfg: DictConfig):\n    sched_cfg = cfg.training.get(\"lr_scheduler\", None)\n    if sched_cfg is None:\n        return None\n    sched_type = sched_cfg.type.lower()\n    if sched_type == \"cosine\":\n        return CosineAnnealingLR(optimizer, T_max=sched_cfg.T_max)\n    elif sched_type == \"linear\":\n        total_steps = cfg.training.epochs * cfg.training.steps_per_epoch\n        warmup_steps = int(sched_cfg.warmup_steps)\n\n        def lr_lambda(current_step):\n            if current_step \u003c warmup_steps:\n                return float(current_step) / float(max(1, warmup_steps))\n            return max(\n                0.0,\n                float(total_steps - current_step) / float(max(1, total_steps - warmup_steps)),\n            )\n\n        return LambdaLR(optimizer, lr_lambda)\n    else:\n        raise ValueError(f\"Unsupported scheduler type: {sched_type}\")\n\n\ndef save_wandb_metadata(save_dir: Path, run_id: str):\n    iteration = os.environ.get(\"EXPERIMENT_ITERATION\", \"0\")\n    meta_path = Path(f\".research/iteration{iteration}\")\n    meta_path.mkdir(parents=True, exist_ok=True)\n    with (meta_path / \"wandb_metadata.json\").open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(\n            {\n                \"wandb_entity\": \"gengaru617\",\n                \"wandb_project\": \"251014-test\",\n                \"wandb_run_id\": run_id,\n            },\n            f,\n            indent=2,\n        )\n\n\ndef log_to_stdout(obj: Dict[str, Any]):\n    print(json.dumps(obj, ensure_ascii=False))\n    sys.stdout.flush()\n\n\n# -----------------------------------------------------------------------------\n# Core train / validate loop\n# -----------------------------------------------------------------------------\n\ndef train_one_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    running_loss, running_correct, total = 0.0, 0, 0\n    for batch in loader:\n        inputs, labels = batch[0].to(device), batch[1].to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        preds = outputs.argmax(dim=1)\n        running_correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    acc = running_correct / total\n    return running_loss / total, acc\n\n\ndef validate(model, loader, criterion, device):\n    model.eval()\n    running_loss, running_correct, total = 0.0, 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            inputs, labels = batch[0].to(device), batch[1].to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n            preds = outputs.argmax(dim=1)\n            running_correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    acc = running_correct / total\n    return running_loss / total, acc\n\n\n# -----------------------------------------------------------------------------\n# Optuna objective (if enabled)\n# -----------------------------------------------------------------------------\n\ndef build_objective(cfg: DictConfig, data, device):\n    train_loader, val_loader, num_classes = data\n\n    def objective(trial: optuna.trial.Trial):\n        # Sample hyper-params\n        for hp_name, hp_cfg in cfg.optuna.search_space.items():\n            hp_type = hp_cfg.type\n            if hp_type == \"loguniform\":\n                cfg.training[hp_name] = trial.suggest_float(hp_name, hp_cfg.low, hp_cfg.high, log=True)\n            elif hp_type == \"uniform\":\n                cfg.training[hp_name] = trial.suggest_float(hp_name, hp_cfg.low, hp_cfg.high)\n            elif hp_type == \"categorical\":\n                cfg.training[hp_name] = trial.suggest_categorical(hp_name, hp_cfg.choices)\n            else:\n                raise ValueError(f\"Unsupported hp type: {hp_type}\")\n\n        model = get_model(cfg, num_classes=num_classes).to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = prepare_optimizer(model, cfg)\n        scheduler = prepare_scheduler(optimizer, cfg)\n\n        for epoch in range(cfg.training.epochs):\n            train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n            val_loss, val_acc = validate(model, val_loader, criterion, device)\n            if scheduler is not None:\n                scheduler.step()\n            trial.report(val_acc, epoch)\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n        return val_acc\n\n    return objective\n\n\n# -----------------------------------------------------------------------------\n# Main entry\n# -----------------------------------------------------------------------------\n\n@hydra.main(config_path=\"..\\/config\", config_name=\"config\", version_base=\"1.3\")\ndef _main(cfg: DictConfig):\n    # Ensure experiment description is printed *before* anything else\n    print(\"=\" * 80)\n    print(\"Experiment Description:\")\n    print(OmegaConf.to_yaml(cfg, resolve=True))\n    print(\"=\" * 80)\n    sys.stdout.flush()\n\n    results_dir = Path(cfg.results_dir).expanduser().resolve()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # Initialise WandB\n    # ------------------------------------------------------------------\n    wandb_mode = \"online\" if os.environ.get(\"WANDB_API_KEY\") else \"offline\"\n    wandb_run = wandb.init(\n        project=\"251014-test\",\n        entity=\"gengaru617\",\n        name=cfg.run_id,\n        tags=[cfg.method, cfg.dataset.name, cfg.model.name],\n        config=OmegaConf.to_container(cfg, resolve=True),\n        mode=wandb_mode,\n    )\n    print(f\"WandB run URL: {wandb_run.url if wandb_run else \u0027offline\u0027}\")\n    save_wandb_metadata(results_dir, wandb_run.id if wandb_run else \"offline\")\n\n    # ------------------------------------------------------------------\n    # Reproducibility\n    # ------------------------------------------------------------------\n    set_seed(42)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # Data preparation\n    # ------------------------------------------------------------------\n    train_ds, val_ds, test_ds, num_classes = get_datasets(cfg)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg.training.batch_size,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True,\n    )\n    test_loader = DataLoader(\n        test_ds,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True,\n    )\n\n    cfg.training.steps_per_epoch = len(train_loader)\n\n    # ------------------------------------------------------------------\n    # Model / Optimiser / Scheduler\n    # ------------------------------------------------------------------\n    model = get_model(cfg, num_classes=num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = prepare_optimizer(model, cfg)\n    scheduler = prepare_scheduler(optimizer, cfg)\n\n    # ------------------------------------------------------------------\n    # Optuna optimisation (optional)\n    # ------------------------------------------------------------------\n    if cfg.get(\"optuna\", None):\n        study = optuna.create_study(direction=\"maximize\")\n        objective = build_objective(cfg, (train_loader, val_loader, num_classes), device)\n        study.optimize(objective, n_trials=cfg.optuna.n_trials, timeout=None, show_progress_bar=True)\n        best_params = study.best_trial.params\n        log_to_stdout({\"run_id\": cfg.run_id, \"best_hparams\": best_params})\n        # Update training config with best params\n        for k, v in best_params.items():\n            cfg.training[k] = v\n        # Re-create optimiser / sched after hp search\n        optimizer = prepare_optimizer(model, cfg)\n        scheduler = prepare_scheduler(optimizer, cfg)\n\n    # ------------------------------------------------------------------\n    # Training Loop\n    # ------------------------------------------------------------------\n    metrics_per_epoch = []\n    for epoch in range(cfg.training.epochs):\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n        if scheduler:\n            scheduler.step()\n        epoch_metrics = {\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n        }\n        metrics_per_epoch.append(epoch_metrics)\n        # Log to stdout \u0026 wandb\n        log_to_stdout({\"run_id\": cfg.run_id, **epoch_metrics})\n        if wandb_run:\n            wandb.log(epoch_metrics, step=epoch + 1)\n\n    # ------------------------------------------------------------------\n    # Final evaluation on test split\n    # ------------------------------------------------------------------\n    test_loss, test_acc = validate(model, test_loader, criterion, device)\n    final_metrics = {\n        \"run_id\": cfg.run_id,\n        \"test_loss\": test_loss,\n        \"test_acc\": test_acc,\n    }\n    log_to_stdout(final_metrics)\n    if wandb_run:\n        wandb.summary.update(final_metrics)\n\n    # ------------------------------------------------------------------\n    # Save artefacts\n    # ------------------------------------------------------------------\n    model_path = results_dir / \"model.pt\"\n    torch.save(model.state_dict(), model_path)\n    with (results_dir / \"results.json\").open(\"w\", encoding=\"utf-8\") as f:\n        json.dump({\"per_epoch\": metrics_per_epoch, \"final\": final_metrics}, f, indent=2)\n\n    if wandb_run:\n        wandb.save(str(model_path))\n        wandb.finish()\n\n    # Ensure Hydra changes directory back so caller can find results_dir\n    shutil.copytree(os.getcwd(), results_dir / \"hydra\", dirs_exist_ok=True)\n\n\nif __name__ == \"__main__\":\n    _main()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "The code is mostly functional but several core-validation requirements are violated:\n\n1. WandB integration (Req. 9) – project/entity are hard-coded inside train.py / evaluate.py instead of being read from the Hydra config.  Configuration keys are missing, so runs cannot be redirected to another project without code edits.\n\n2. Evaluation figures (Req. 8) – figures must be stored in {results_dir}/images/ and include legends/annotations.  evaluate.py saves comparison_accuracy.png directly in results_dir and does not add a legend.\n\n3. tee_subprocess implementation in main.py reads stdout completely before stderr, which can dead-lock if stderr fills the OS pipe (production-readiness issue under Req. 1 & 6).\n\n4. Config structure – hydra config/config.yaml lacks the wandb section expected by Req. 7; results_dir default exists but run list is declared under available_runs rather than Hydra group enumeration, making programmatic iteration harder.\n\nBecause of these issues the package is not yet immediately executable and publication-ready."
}
