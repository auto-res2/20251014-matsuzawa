
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches: `uv run python -u -m src.main run={run_id} results_dir={path}`

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations

4. **Model Implementation**:
   - Complete model architectures for baseline, proposed, and ablation methods
   - All models implemented from scratch (no placeholders)
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - No missing files from the structure
   - All functionality contained within specified files

6. **Structured Logging**:
   - train.py prints JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py prints JSON-formatted comparison results to stdout
   - main.py redirects subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log`
   - Logs are forwarded to main process stdout/stderr for GitHub Actions capture

7. **Configuration Files**:
   - config/config.yaml contains list of all experiment run_ids
   - NOTE: config/run/{run_id}.yaml files are provided separately in the "Experiment Runs" section below (not in ExperimentCode)
   - The generated code must properly reference these config files via Hydra
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation and Figures** (if WandB not used):
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/`)
   - Figures include legends, annotations, and proper labels
   - Uses `plt.tight_layout()` before saving
   - Consistent result formatting and comparison logic

9. **WandB Integration** (if WandB is used):
   - Proper WandB initialization with entity/project from config
   - Metrics logged to WandB during training
   - WandB run URL printed to stdout
   - Metadata saved to `.research/iteration{experiment_iteration}/wandb_metadata.json`
   - Figures uploaded as WandB artifacts

10. **Immediate Executability**:
    - Code can be run immediately without modifications
    - All imports and dependencies properly specified in pyproject.toml
    - No missing external resources or undefined variables
    - Proper module structure for `uv run python -m src.main` execution

## Detection of Common Issues
Flag the following problems if found:

- **Incomplete Implementation**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns
- **Truncation**: Code sections that appear shortened or simplified inappropriately
- **Missing Functionality**: Expected features not implemented
- **Configuration Issues**: Missing or incomplete run configs
- **Import Errors**: Missing dependencies or incorrect import statements
- **Not Executable**: Code that cannot be run immediately
- **Inconsistent Structure**: Files not matching required structure
- **Logging Issues**: Missing or incorrect JSON logging format

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

# Experimental Design
- Strategy: Comparative analysis of efficient model architectures across vision and language tasks
- Proposed Method: Evaluate MobileNetV2 and DistilBERT variants for efficient deployment
- Evaluation Metrics: ['accuracy', 'f1_score', 'inference_time', 'model_size']

# Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  width_mult: 0.5
  dropout: 0.2
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
  augmentations:
    random_crop: true
    random_flip: horizontal
training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.05
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0005
  lr_scheduler:
    type: cosine
    T_max: 50
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 0.001
      high: 0.1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    momentum:
      type: uniform
      low: 0.7
      high: 0.95
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  input_adapter:
    type: char_cnn
    embedding_dim: 256
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: char
training:
  epochs: 3
  batch_size: 16
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 0.01
  gradient_accumulation_steps: 2
optuna:
  n_trials: 15
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-1

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-CIFAR-10
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
  modality_adapter:
    type: linear_patch_embedding
    patch_size: 4
    sequence_length: 64
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    patch_size:
      type: categorical
      choices: [2, 4, 8]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: sentencepiece
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - _self_\n\n# Default run ID (can be overridden from CLI)\nrun: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\n\n# Default results directory\nresults_dir: ./results\n\nevaluate: false", "evaluate_py": "import json\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport hydra\nfrom omegaconf import DictConfig\n\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nimport wandb\n\n# -----------------------------------------------------------------------------\n# Helper\n# -----------------------------------------------------------------------------\n\ndef load_all_results(results_root: Path) -\u003e List[Dict]:\n    records = []\n    for res_file in results_root.rglob(\"results.json\"):\n        with open(res_file, \"r\", encoding=\"utf-8\") as fp:\n            rec = json.load(fp)\n            records.append(rec)\n    return records\n\n\n# -----------------------------------------------------------------------------\n# Main entry\n# -----------------------------------------------------------------------------\n@hydra.main(config_path=\"../../config\", config_name=\"config\", version_base=\"1.3\")\ndef _main(cfg: DictConfig):\n    results_root = Path(cfg.results_dir).expanduser().resolve()\n    all_results = load_all_results(results_root)\n    if not all_results:\n        print(\"No results found \u2013 skipping evaluation.\")\n        return\n\n    # Aggregate metrics\n    names, val_accs, test_accs = [], [], []\n    for rec in all_results:\n        names.append(rec[\"run_id\"])\n        val_accs.append(rec[\"best_val_accuracy\"])\n        test_accs.append(rec[\"test_accuracy\"])\n\n    # Bar chart of test accuracy\n    plt.figure(figsize=(max(6, len(names) * 1.2), 4))\n    plt.bar(names, test_accs, color=\"skyblue\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Test Accuracy\")\n    plt.tight_layout()\n    fig_path = results_root / \"comparison_test_accuracy.png\"\n    plt.savefig(fig_path)\n\n    # Structured results\n    summary = {n: {\"val_accuracy\": v, \"test_accuracy\": t} for n, v, t in zip(names, val_accs, test_accs)}\n    print(json.dumps(summary))\n\n    # Log to WandB (optional)\n    if \"WANDB_API_KEY\" in os.environ:\n        wandb.init(project=\"comparison-summary\", entity=\"gengaru617\", name=\"summary\", reinit=True)\n        wandb.log({\"comparison_chart\": wandb.Image(str(fig_path))})\n        wandb.finish()\n\n\nif __name__ == \"__main__\":\n    _main()\n", "main_py": "import json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\n\n\n# -----------------------------------------------------------------------------\n# Subprocess helper with tee-like logging\n# -----------------------------------------------------------------------------\n\ndef run_with_tee(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    stdout_f = stdout_path.open(\"w\", encoding=\"utf-8\")\n    stderr_f = stderr_path.open(\"w\", encoding=\"utf-8\")\n\n    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n    try:\n        while True:\n            out_line = process.stdout.readline()\n            err_line = process.stderr.readline()\n            if out_line:\n                sys.stdout.write(out_line)\n                stdout_f.write(out_line)\n                stdout_f.flush()\n            if err_line:\n                sys.stderr.write(err_line)\n                stderr_f.write(err_line)\n                stderr_f.flush()\n            if out_line == \"\" and err_line == \"\" and process.poll() is not None:\n                break\n    finally:\n        stdout_f.close()\n        stderr_f.close()\n\n    if process.returncode != 0:\n        raise subprocess.CalledProcessError(process.returncode, cmd)\n\n\n# -----------------------------------------------------------------------------\n# Hydra main \u2013 orchestrates one run (GitHub Actions supplies run=\u003cid\u003e)\n# -----------------------------------------------------------------------------\n@hydra.main(config_path=\"../../config\", config_name=\"config\", version_base=\"1.3\")\ndef _main(cfg: DictConfig):\n    run_id = cfg.run\n    results_dir = Path(cfg.results_dir).expanduser().resolve()\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Launch train subprocess (using the same interpreter)\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\", f\"run={run_id}\", f\"results_dir={results_dir}\"]\n    stdout_path = run_dir / \"stdout.log\"\n    stderr_path = run_dir / \"stderr.log\"\n    print(f\"Launching training subprocess: {\u0027 \u0027.join(cmd)}\")\n    run_with_tee(cmd, stdout_path, stderr_path)\n\n    # Optionally perform evaluation across all runs in results_dir\n    if getattr(cfg, \"evaluate\", False):\n        cmd_eval = [sys.executable, \"-u\", \"-m\", \"src.evaluate\", f\"results_dir={results_dir}\"]\n        run_with_tee(cmd_eval, results_dir / \"evaluate_stdout.log\", results_dir / \"evaluate_stderr.log\")\n\n\nif __name__ == \"__main__\":\n    _main()\n", "model_py": "\"\"\"Model definition module \u2013 contains all architectures used in the experiments.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import mobilenet_v2\n\n\n# -----------------------------------------------------------------------------\n# Vision: MobileNetV2 thin wrapper\n# -----------------------------------------------------------------------------\nclass MobileNetV2Classifier(nn.Module):\n    def __init__(self, num_classes: int, width_mult: float = 1.0, dropout: float = 0.2, pretrained: bool = False):\n        super().__init__()\n        self.backbone = mobilenet_v2(pretrained=pretrained, width_mult=width_mult)\n        in_feat = self.backbone.classifier[1].in_features\n        self.backbone.classifier = nn.Sequential(\n            nn.Dropout(p=dropout), nn.Linear(in_feat, num_classes)\n        )\n\n    def forward(self, pixel_values):  # pixel_values: (B,3,H,W)\n        return self.backbone(pixel_values)\n\n\n# -----------------------------------------------------------------------------\n# NLP: Character-level CNN\n# -----------------------------------------------------------------------------\nclass CharCNNTextClassifier(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=1)\n        self.conv = nn.Sequential(\n            nn.Conv1d(embed_dim, 128, kernel_size=5, padding=2),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2),\n            nn.Conv1d(128, 256, kernel_size=5, padding=2),\n            nn.ReLU(),\n            nn.AdaptiveMaxPool1d(1),\n        )\n        self.fc = nn.Linear(256, num_classes)\n\n    def forward(self, input_ids):  # (B, L)\n        x = self.embedding(input_ids).transpose(1, 2)  # (B,E,L) -\u003e (B,E,L)\n        x = self.conv(x).squeeze(-1)  # (B,C,1) -\u003e (B,C)\n        return self.fc(x)\n\n\n# -----------------------------------------------------------------------------\n# NLP: DistilBERT text classifier\n# -----------------------------------------------------------------------------\nclass DistilBertTextClassifier(nn.Module):\n    def __init__(self, num_classes: int, pretrained_model_name: str = \"distilbert-base-uncased\"):\n        super().__init__()\n        from transformers import DistilBertModel  # lazy import\n\n        self.distilbert = DistilBertModel.from_pretrained(pretrained_model_name)\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2), nn.Linear(self.distilbert.config.hidden_size, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask=None):\n        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled = outputs.last_hidden_state[:, 0]  # CLS token\n        return self.classifier(pooled)\n\n\n# -----------------------------------------------------------------------------\n# Vision: Simple patch embed + Transformer encoder (DistilBERT-style)\n# -----------------------------------------------------------------------------\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size: int = 32, patch_size: int = 4, in_chans: int = 3, embed_dim: int = 768):\n        super().__init__()\n        self.num_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.num_patches, embed_dim))\n\n    def forward(self, x):\n        B = x.size(0)\n        x = self.proj(x).flatten(2).transpose(1, 2)  # (B, N, D)\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        return x\n\n\nclass DistilBertVisionClassifier(nn.Module):\n    def __init__(self, num_classes: int, img_size: int = 32, patch_size: int = 4, n_layers: int = 6):\n        super().__init__()\n        embed_dim = 768\n        self.embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n        encoder_layer = nn.TransformerEncoderLayer(embed_dim, nhead=12, dim_feedforward=2048, dropout=0.1)\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, pixel_values):  # (B,3,H,W)\n        x = self.embed(pixel_values)\n        x = self.encoder(x)\n        x = self.norm(x[:, 0])  # CLS token output\n        return self.head(x)\n\n\n# -----------------------------------------------------------------------------\n# Factory\n# -----------------------------------------------------------------------------\n\ndef build_model(cfg, num_classes: int, extra_info: Dict):\n    name = cfg.model.name.lower()\n    if name == \"mobilenetv2\":\n        return MobileNetV2Classifier(\n            num_classes=num_classes,\n            width_mult=cfg.model.width_mult,\n            dropout=cfg.model.dropout,\n            pretrained=cfg.model.pretrained,\n        )\n    elif name.startswith(\"distilbert\") and cfg.dataset.name.upper() == \"CIFAR-10\":\n        patch_size = cfg.model.modality_adapter.patch_size\n        return DistilBertVisionClassifier(num_classes=num_classes, patch_size=patch_size, img_size=cfg.dataset.image_size)\n    elif name.startswith(\"distilbert\"):\n        return DistilBertTextClassifier(num_classes=num_classes, pretrained_model_name=cfg.model.name)\n    else:  # Char CNN\n        return CharCNNTextClassifier(\n            vocab_size=extra_info[\"vocab_size\"],\n            embed_dim=cfg.model.input_adapter.embedding_dim,\n            num_classes=num_classes,\n        )\n", "preprocess_py": "\"\"\"Data loading \u0026 preprocessing utilities.\"\"\"\nfrom __future__ import annotations\n\nimport string\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision import transforms, datasets\n\n# Optional heavy dependencies are imported lazily to minimise memory foot-print\n\n\n# -----------------------------------------------------------------------------\n# Vision datasets \u2013 presently only CIFAR-10 (but easily extendable)\n# -----------------------------------------------------------------------------\n\ndef _build_cifar10_loaders(cfg) -\u003e Tuple[DataLoader, DataLoader, DataLoader, Dict]:\n    tfs_train = []\n    if cfg.dataset.augmentations.random_crop:\n        tfs_train.append(transforms.RandomCrop(cfg.dataset.image_size, padding=4))\n    if cfg.dataset.augmentations.random_flip == \"horizontal\":\n        tfs_train.append(transforms.RandomHorizontalFlip())\n    tfs_train.extend([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=cfg.dataset.normalization.mean, std=cfg.dataset.normalization.std),\n    ])\n    transform_train = transforms.Compose(tfs_train)\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=cfg.dataset.normalization.mean, std=cfg.dataset.normalization.std),\n    ])\n\n    data_dir = Path(\"./data\").resolve()\n    full_train = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform_train)\n    test = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform_test)\n\n    train_size = cfg.dataset.splits.train\n    val_size = cfg.dataset.splits.val\n    assert train_size + val_size == len(full_train)\n    train, val = random_split(full_train, [train_size, val_size])\n\n    train_loader = DataLoader(train, batch_size=cfg.training.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val, batch_size=cfg.training.batch_size, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test, batch_size=cfg.training.batch_size, shuffle=False, num_workers=2)\n\n    extra = {\"num_classes\": 10}\n    return train_loader, val_loader, test_loader, extra\n\n\n# -----------------------------------------------------------------------------\n# NLP \u2013 Alpaca cleaned instruction-following dataset\n# -----------------------------------------------------------------------------\nclass AlpacaCharDataset(Dataset):\n    \"\"\"Character-level binary classification dataset: label 1 if output length \u003e 100.\"\"\"\n\n    def __init__(self, split: str, max_seq_length: int):\n        from datasets import load_dataset  # lazy import\n\n        raw = load_dataset(\"yahma/alpaca-cleaned\", split=split)\n        self.samples: List[Tuple[str, int]] = []\n        for rec in raw:\n            text = f\"Instruction: {rec[\u0027instruction\u0027]} Input: {rec[\u0027input\u0027]} Output: {rec[\u0027output\u0027]}\"\n            label = 1 if len(rec[\"output\"]) \u003e 100 else 0\n            self.samples.append((text, label))\n        self.vocab = _build_char_vocab()\n        self.max_len = max_seq_length\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        text, label = self.samples[idx]\n        token_ids = [_char2id(c, self.vocab) for c in text[: self.max_len]]\n        pad_len = self.max_len - len(token_ids)\n        token_ids.extend([self.vocab[\"\u003cpad\u003e\"]] * pad_len)\n        return {\n            \"input_ids\": torch.tensor(token_ids, dtype=torch.long),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\nclass AlpacaBertDataset(Dataset):\n    \"\"\"SentencePiece / WordPiece tokenised variant for DistilBERT classification.\"\"\"\n\n    def __init__(self, split: str, tokenizer_name: str, max_seq_length: int):\n        from datasets import load_dataset  # lazy import\n        from transformers import AutoTokenizer\n\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        raw = load_dataset(\"yahma/alpaca-cleaned\", split=split)\n        self.encodings = self.tokenizer(\n            [\n                f\"Instruction: {r[\u0027instruction\u0027]} Input: {r[\u0027input\u0027]}\" for r in raw\n            ],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=max_seq_length,\n        )\n        self.labels = [1 if len(r[\"output\"]) \u003e 100 else 0 for r in raw]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.encodings[\"input_ids\"][idx], dtype=torch.long),\n            \"attention_mask\": torch.tensor(self.encodings[\"attention_mask\"][idx], dtype=torch.long),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n\n\n# -----------------------------------------------------------------------------\n# Helpers\n# -----------------------------------------------------------------------------\n\ndef _build_char_vocab() -\u003e Dict[str, int]:\n    base_chars = list(string.printable)\n    vocab = {ch: i + 2 for i, ch in enumerate(base_chars)}\n    vocab[\"\u003cunk\u003e\"] = 0\n    vocab[\"\u003cpad\u003e\"] = 1\n    return vocab\n\n\ndef _char2id(ch: str, vocab: Dict[str, int]) -\u003e int:\n    return vocab.get(ch, vocab[\"\u003cunk\u003e\"])\n\n\n# -----------------------------------------------------------------------------\n# Master factory\n# -----------------------------------------------------------------------------\n\ndef build_dataloaders(cfg) -\u003e Tuple[DataLoader, DataLoader, DataLoader, Dict]:\n    \"\"\"Return train / val / test loaders and extra info (num_classes etc.).\"\"\"\n    if cfg.dataset.name.upper() == \"CIFAR-10\":\n        return _build_cifar10_loaders(cfg)\n\n    elif cfg.dataset.name.startswith(\"alpaca\"):\n        max_len = cfg.dataset.max_seq_length\n        split_all = [\"train[:90%]\", \"train[90%:]\", \"test\"]\n        if cfg.model.name.lower().startswith(\"distilbert\"):\n            dataset_cls = AlpacaBertDataset\n            kwargs = {\"tokenizer_name\": \"distilbert-base-uncased\"}\n        else:\n            dataset_cls = AlpacaCharDataset\n            kwargs = {}\n\n        train_ds = dataset_cls(split_all[0], max_seq_length=max_len, **kwargs)\n        val_ds = dataset_cls(split_all[1], max_seq_length=max_len, **kwargs)\n        test_ds = dataset_cls(split_all[2], max_seq_length=max_len, **kwargs)\n\n        loader_kwargs = {\n            \"batch_size\": cfg.training.batch_size,\n            \"num_workers\": 2,\n            \"shuffle\": True,\n            \"drop_last\": False,\n        }\n        train_loader = DataLoader(train_ds, **loader_kwargs)\n        loader_kwargs[\"shuffle\"] = False\n        val_loader = DataLoader(val_ds, **loader_kwargs)\n        test_loader = DataLoader(test_ds, **loader_kwargs)\n        extra = {\"num_classes\": 2}\n        if isinstance(train_ds, AlpacaCharDataset):\n            extra[\"vocab_size\"] = len(train_ds.vocab)\n        return train_loader, val_loader, test_loader, extra\n\n    else:\n        raise ValueError(f\"Unsupported dataset: {cfg.dataset.name}\")\n", "pyproject_toml": "[project]\nname = \"research-exp\"\nversion = \"0.1.0\"\ndescription = \"Reproducible research experiments with Hydra, Optuna \u0026 WandB\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"*\"\ntorchvision = \"*\"\nhydra-core = \"\u003e=1.3\"\nomegaconf = \"*\"\noptuna = \"*\"\nwandb = \"*\"\ntransformers = \"*\"\ndatasets = \"*\"\nsentencepiece = \"*\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n", "train_py": "import json\nimport os\nimport sys\nimport subprocess\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict\n\nimport hydra\nfrom hydra.utils import instantiate\nfrom omegaconf import DictConfig, OmegaConf\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nfrom src.preprocess import build_dataloaders\nfrom src.model import build_model\n\nimport wandb\n\n# -----------------------------------------------------------------------------\n# Helper utilities\n# -----------------------------------------------------------------------------\n\ndef describe_experiment(cfg: DictConfig) -\u003e str:\n    \"\"\"Return a human-readable description of the experiment.\"\"\"\n    lines = [\n        f\"Run ID                : {cfg.run_id}\",\n        f\"Method                : {cfg.method}\",\n        f\"Model                 : {cfg.model.name}\",\n        f\"Dataset               : {cfg.dataset.name}\",\n        f\"Training epochs       : {cfg.training.epochs}\",\n        f\"Batch size            : {cfg.training.batch_size}\",\n        f\"Learning rate         : {cfg.training.learning_rate}\",\n        f\"Optimizer             : {cfg.training.optimizer}\",\n    ]\n    return \"\\n\".join(lines)\n\n\ndef get_results_path(cfg: DictConfig) -\u003e Path:\n    root = Path(cfg.results_dir).expanduser().resolve()\n    root.mkdir(parents=True, exist_ok=True)\n    run_dir = root / cfg.run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    return run_dir\n\n\n# -----------------------------------------------------------------------------\n# Training / Evaluation helpers\n# -----------------------------------------------------------------------------\n\ndef train_one_epoch(model: nn.Module, loader: DataLoader, criterion, optimizer, device) -\u003e Dict[str, float]:\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    for batch in loader:\n        labels = batch.pop(\"labels\").to(device)\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        outputs = model(**inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * labels.size(0)\n        preds = outputs.argmax(dim=-1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    return {\n        \"loss\": running_loss / total,\n        \"accuracy\": correct / total,\n    }\n\n\ndef evaluate(model: nn.Module, loader: DataLoader, criterion, device) -\u003e Dict[str, float]:\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            labels = batch.pop(\"labels\").to(device)\n            inputs = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * labels.size(0)\n            preds = outputs.argmax(dim=-1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    return {\n        \"loss\": running_loss / total,\n        \"accuracy\": correct / total,\n    }\n\n\n# -----------------------------------------------------------------------------\n# Main training entry point\n# -----------------------------------------------------------------------------\n@hydra.main(config_path=\"../../config\", config_name=\"config\", version_base=\"1.3\")\ndef _main(cfg: DictConfig):\n    # ------------------------------------------------------------------\n    # Resolve run-specific config and results dir\n    # ------------------------------------------------------------------\n    run_cfg_path = Path(__file__).parent.parent / \"..\" / \"config\" / \"run\" / f\"{cfg.run}.yaml\"\n    run_cfg = OmegaConf.load(run_cfg_path)\n    run_cfg.results_dir = cfg.results_dir  # propagate\n    OmegaConf.set_struct(run_cfg, False)  # allow mutation\n\n    # Create results directory early so that subprocess redirection in parent works.\n    run_dir = get_results_path(run_cfg)\n\n    # ------------------------------------------------------------------\n    # Stdout experiment description (required before numeric results)\n    # ------------------------------------------------------------------\n    print(\"=\" * 80)\n    print(\"EXPERIMENT DESCRIPTION\")\n    print(\"=\" * 80)\n    print(describe_experiment(run_cfg))\n    print(\"=\" * 80)\n\n    # ------------------------------------------------------------------\n    # Prepare data\n    # ------------------------------------------------------------------\n    train_loader, val_loader, test_loader, extra_info = build_dataloaders(run_cfg)\n\n    # ------------------------------------------------------------------\n    # Build model\n    # ------------------------------------------------------------------\n    num_classes = extra_info[\"num_classes\"]\n    model = build_model(run_cfg, num_classes, extra_info)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # ------------------------------------------------------------------\n    # Optimizer \u0026 LR scheduler\n    # ------------------------------------------------------------------\n    optimizer_cfg = run_cfg.training.optimizer.lower()\n    if optimizer_cfg == \"sgd\":\n        optimizer = torch.optim.SGD(\n            model.parameters(),\n            lr=run_cfg.training.learning_rate,\n            momentum=getattr(run_cfg.training, \"momentum\", 0),\n            weight_decay=run_cfg.training.weight_decay,\n        )\n    elif optimizer_cfg in {\"adam\", \"adamw\"}:\n        optim_cls = torch.optim.AdamW if optimizer_cfg == \"adamw\" else torch.optim.Adam\n        optimizer = optim_cls(\n            model.parameters(),\n            lr=run_cfg.training.learning_rate,\n            weight_decay=run_cfg.training.weight_decay,\n        )\n    else:\n        raise ValueError(f\"Unsupported optimizer: {optimizer_cfg}\")\n\n    if \"lr_scheduler\" in run_cfg.training and run_cfg.training.lr_scheduler is not None:\n        sched_cfg = run_cfg.training.lr_scheduler\n        if sched_cfg.type == \"cosine\":\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n                optimizer, T_max=sched_cfg.T_max\n            )\n        elif sched_cfg.type == \"linear\":\n            scheduler = torch.optim.lr_scheduler.LinearLR(\n                optimizer, start_factor=1.0, end_factor=0.0, total_iters=sched_cfg.warmup_steps\n            )\n        else:\n            scheduler = None\n    else:\n        scheduler = None\n\n    # ------------------------------------------------------------------\n    # Loss function\n    # ------------------------------------------------------------------\n    criterion = nn.CrossEntropyLoss()\n\n    # ------------------------------------------------------------------\n    # W\u0026B initialisation\n    # ------------------------------------------------------------------\n    wandb_run = wandb.init(\n        project=run_cfg.get(\"wandb\", {}).get(\"project\", \"default-project\"),\n        entity=run_cfg.get(\"wandb\", {}).get(\"entity\", None),\n        name=run_cfg.get(\"wandb\", {}).get(\"run_name\", run_cfg.run_id),\n        tags=run_cfg.get(\"wandb\", {}).get(\"tags\", []),\n        config=OmegaConf.to_container(run_cfg, resolve=True),\n        reinit=True,\n    )\n\n    # Save metadata immediately\n    iteration_idx = os.getenv(\"EXPERIMENT_ITERATION\", \"0\")\n    metadata_path = Path(\".research\") / f\"iteration{iteration_idx}\" / \"wandb_metadata.json\"\n    metadata_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(metadata_path, \"w\", encoding=\"utf-8\") as fp:\n        json.dump(\n            {\n                \"wandb_entity\": wandb_run.entity,\n                \"wandb_project\": wandb_run.project,\n                \"wandb_run_id\": wandb_run.id,\n            },\n            fp,\n            indent=2,\n        )\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    best_val_acc = 0.0\n    history = {\"train\": [], \"val\": []}\n    for epoch in range(1, run_cfg.training.epochs + 1):\n        train_stats = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        val_stats = evaluate(model, val_loader, criterion, device)\n        if scheduler is not None:\n            scheduler.step()\n\n        history[\"train\"].append(train_stats)\n        history[\"val\"].append(val_stats)\n\n        # Log to wandb\n        wandb.log({\n            \"epoch\": epoch,\n            \"train_loss\": train_stats[\"loss\"],\n            \"train_accuracy\": train_stats[\"accuracy\"],\n            \"val_loss\": val_stats[\"loss\"],\n            \"val_accuracy\": val_stats[\"accuracy\"],\n            \"lr\": optimizer.param_groups[0][\"lr\"],\n        })\n\n        # Save best\n        if val_stats[\"accuracy\"] \u003e best_val_acc:\n            best_val_acc = val_stats[\"accuracy\"]\n            best_ckpt_path = run_dir / \"best_model.pth\"\n            torch.save(model.state_dict(), best_ckpt_path)\n            wandb.save(str(best_ckpt_path))\n\n        print(\n            f\"Epoch [{epoch}/{run_cfg.training.epochs}]  \"\n            f\"TLoss: {train_stats[\u0027loss\u0027]:.4f} TAcc: {train_stats[\u0027accuracy\u0027]:.3f}  \"\n            f\"VLoss: {val_stats[\u0027loss\u0027]:.4f} VAcc: {val_stats[\u0027accuracy\u0027]:.3f}\"\n        )\n\n    # ------------------------------------------------------------------\n    # Final test evaluation with best model\n    # ------------------------------------------------------------------\n    if \"best_ckpt_path\" in locals():\n        model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n    test_stats = evaluate(model, test_loader, criterion, device)\n    wandb.log({\"test_loss\": test_stats[\"loss\"], \"test_accuracy\": test_stats[\"accuracy\"]})\n\n    # ------------------------------------------------------------------\n    # Persist final results\n    # ------------------------------------------------------------------\n    results = {\n        \"run_id\": run_cfg.run_id,\n        \"best_val_accuracy\": best_val_acc,\n        \"test_accuracy\": test_stats[\"accuracy\"],\n        \"history\": history,\n    }\n    with open(run_dir / \"results.json\", \"w\", encoding=\"utf-8\") as fp:\n        json.dump(results, fp, indent=2)\n\n    # Required structured output\n    print(json.dumps(results))\n\n    # Print WandB URL for convenience\n    print(f\"WandB URL: {wandb_run.url}\")\n\n    wandb.finish()\n\n\nif __name__ == \"__main__\":\n    _main()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "The project is NOT ready for immediate execution:\n\n1. Hydra config path is incorrect in all @hydra.main decorators – \"../../config\" goes two levels up from src/, pointing outside the repo. It should be \"../config\" (one level) or left empty. Hydra will fail to locate config files.\n2. Required master config (config/config.yaml) does not list the four run_ids; only a default run field is present, violating requirement 7.\n3. No config/run/*.yaml files are included in the repository even though train.py tries to load them (Path …/config/run/{run_id}.yaml). Code will crash with FileNotFoundError.\n4. evaluate.py uses os.environ but does not import os → NameError during execution.\n5. WandB settings referenced in code but no wandb section defined in configs, causing key-access errors (run_cfg.get(\"wandb\" …) works, but OmegaConf struct is disabled earlier; still okay, but missing recommended config).\n6. Minor: run_with_tee does not merge stderr into log on process termination; not blocking, but requirements allow.\n\nDue to the critical missing configs, wrong Hydra paths and missing import, the code cannot run successfully without fixes."
}
