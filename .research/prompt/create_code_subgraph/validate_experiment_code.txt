
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches: `uv run python -u -m src.main run={run_id} results_dir={path}`

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations

4. **Model Implementation**:
   - Complete model architectures for baseline, proposed, and ablation methods
   - All models implemented from scratch (no placeholders)
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - No missing files from the structure
   - All functionality contained within specified files

6. **Structured Logging**:
   - train.py prints JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py prints JSON-formatted comparison results to stdout
   - main.py redirects subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log`
   - Logs are forwarded to main process stdout/stderr for GitHub Actions capture

7. **Configuration Files**:
   - config/config.yaml contains list of all experiment run_ids
   - Each config/run/{run_id}.yaml has complete configuration with actual parameters
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation and Figures** (if WandB not used):
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/`)
   - Figures include legends, annotations, and proper labels
   - Uses `plt.tight_layout()` before saving
   - Consistent result formatting and comparison logic

9. **WandB Integration** (if WandB is used):
   - Proper WandB initialization with entity/project from config
   - Metrics logged to WandB during training
   - WandB run URL printed to stdout
   - Metadata saved to `.research/iteration{experiment_iteration}/wandb_metadata.json`
   - Figures uploaded as WandB artifacts

10. **Immediate Executability**:
    - Code can be run immediately without modifications
    - All imports and dependencies properly specified in pyproject.toml
    - No missing external resources or undefined variables
    - Proper module structure for `uv run python -m src.main` execution

## Detection of Common Issues
Flag the following problems if found:

- **Incomplete Implementation**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns
- **Truncation**: Code sections that appear shortened or simplified inappropriately
- **Missing Functionality**: Expected features not implemented
- **Configuration Issues**: Missing or incomplete run configs
- **Import Errors**: Missing dependencies or incorrect import statements
- **Not Executable**: Code that cannot be run immediately
- **Inconsistent Structure**: Files not matching required structure
- **Logging Issues**: Missing or incorrect JSON logging format

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

# Experimental Design
- Strategy: Comparative analysis of efficient model architectures across vision and language tasks
- Proposed Method: Evaluate MobileNetV2 and DistilBERT variants for efficient deployment
- Evaluation Metrics: ['accuracy', 'f1_score', 'inference_time', 'model_size']

# Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  num_parameters: 3.5M
  pretrained: false
dataset:
  name: CIFAR-10
  type: image_classification
  input_size: 32
  num_channels: 3
  num_classes: 10
  normalization: true
  augmentation:
    - random_crop
    - random_horizontal_flip
  splits:
    train: 0.8
    val: 0.1
    test: 0.1
training:
  epochs: 100
  batch_size: 64
  learning_rate: 0.01
  optimizer: sgd
  momentum: 0.9
  weight_decay: 1e-4
  lr_scheduler: cosine
  num_workers: 4
  seed: 42
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    momentum:
      type: uniform
      low: 0.7
      high: 0.99
    weight_decay:
      type: loguniform
      low: 1e-6
      high: 1e-3
system:
  device: cpu
  max_ram_mb: 500
  ```
  

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  num_parameters: 3.5M
  pretrained: false
  modality_adapter: token2pixel
dataset:
  name: alpaca-cleaned
  type: text_classification
  max_length: 128
  padding: max_length
  truncation: true
  lowercase: true
  splits:
    train: 0.9
    val: 0.05
    test: 0.05
training:
  epochs: 20
  batch_size: 32
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 1e-5
  lr_scheduler: linear
  warmup_steps: 500
  seed: 42
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-2
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    weight_decay:
      type: loguniform
      low: 1e-6
      high: 1e-3
system:
  device: cpu
  max_ram_mb: 500
  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-CIFAR-10
method: comparative-2
model:
  name: distilbert-base-uncased
  hidden_size: 768
  num_layers: 6
  num_parameters: 66M
  pretrained: false
  modality_adapter: patch_tokenizer
dataset:
  name: CIFAR-10
  type: image_classification
  input_size: 32
  num_channels: 3
  num_classes: 10
  normalization: true
  splits:
    train: 0.8
    val: 0.1
    test: 0.1
training:
  epochs: 10
  batch_size: 32
  learning_rate: 5e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler: linear
  warmup_steps: 1000
  seed: 42
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-6
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    weight_decay:
      type: loguniform
      low: 1e-6
      high: 1e-2
    dropout:
      type: uniform
      low: 0.0
      high: 0.5
system:
  device: cpu
  max_ram_mb: 500
  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned
method: comparative-2
model:
  name: distilbert-base-uncased
  hidden_size: 768
  num_layers: 6
  num_parameters: 66M
  pretrained: false
dataset:
  name: alpaca-cleaned
  type: text_classification
  max_length: 128
  padding: max_length
  truncation: true
  lowercase: true
  splits:
    train: 0.9
    val: 0.05
    test: 0.05
training:
  epochs: 3
  batch_size: 16
  learning_rate: 3e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler: linear
  warmup_steps: 500
  seed: 42
optuna:
  n_trials: 30
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-6
      high: 1e-4
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-6
      high: 1e-2
    dropout:
      type: uniform
      low: 0.0
      high: 0.5
system:
  device: cpu
  max_ram_mb: 500
  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - run: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\n\n# The selected run can be overridden from CLI: run=\u003crun_id\u003e\n# results_dir can be overridden too\nresults_dir: ./results\n\nwandb:\n  entity: gengaru617\n  project: 251014-test\n\n# Dummy placeholder so Hydra knows the group exists\nrun: {}\n", "evaluate_py": "import json\nimport os\nfrom pathlib import Path\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport wandb\n\n\ndef collect_results(results_dir: Path) -\u003e pd.DataFrame:\n    rows: List[dict] = []\n    for run_dir in results_dir.iterdir():\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file, \"r\") as fp:\n                rows.append(json.load(fp))\n    return pd.DataFrame(rows)\n\n\ndef plot_accuracy(df: pd.DataFrame, save_path: Path):\n    plt.figure(figsize=(8, 4))\n    plt.bar(df[\"run_id\"], df[\"test_acc\"])\n    plt.ylabel(\"Test Accuracy\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150)\n    plt.close()\n\n\ndef main(results_dir: str):\n    results_path = Path(results_dir)\n    df = collect_results(results_path)\n    best_row = df.loc[df[\"test_acc\"].idxmax()]\n\n    fig_path = results_path / \"comparison.png\"\n    plot_accuracy(df, fig_path)\n\n    # upload to wandb as artifact (creates new run in job type evaluate)\n    wandb.login()\n    run = wandb.init(project=\"251014-test\", entity=\"gengaru617\", job_type=\"evaluation\")\n    artifact = wandb.Artifact(\"comparison_fig\", type=\"figure\")\n    artifact.add_file(str(fig_path))\n    run.log({\"comparison\": wandb.Image(str(fig_path))})\n    run.log_artifact(artifact)\n    run.finish()\n\n    summary = {\n        \"best_run_id\": best_row[\"run_id\"],\n        \"best_test_accuracy\": float(best_row[\"test_acc\"]),\n    }\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str)\n    args = parser.parse_args()\n    main(args.results_dir)\n", "main_py": "import os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\ndef run_subprocess(cmd: List[str], log_dir: Path):\n    stdout_file = log_dir / \"stdout.log\"\n    stderr_file = log_dir / \"stderr.log\"\n    with open(stdout_file, \"w\") as out_fp, open(stderr_file, \"w\") as err_fp:\n        process = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1\n        )\n        # tee-like streaming\n        while True:\n            stdout_line = process.stdout.readline()\n            stderr_line = process.stderr.readline()\n            if stdout_line:\n                print(stdout_line, end=\"\")\n                out_fp.write(stdout_line)\n                out_fp.flush()\n            if stderr_line:\n                print(stderr_line, end=\"\", file=sys.stderr)\n                err_fp.write(stderr_line)\n                err_fp.flush()\n            if not stdout_line and not stderr_line and process.poll() is not None:\n                break\n        process.wait()\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(process.returncode, cmd)\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    abs_results_dir = Path(hydra.utils.to_absolute_path(cfg.results_dir))\n    abs_results_dir.mkdir(parents=True, exist_ok=True)\n\n    run_cfg = cfg.run  # selected run id from override\n    if isinstance(run_cfg, DictConfig):\n        run_id = run_cfg.run_id\n    else:\n        run_id = str(cfg.run)\n    log_dir = abs_results_dir / run_id\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={run_id}\",\n        f\"results_dir={abs_results_dir}\",\n    ]\n    # propagate experiment iteration env var if exists\n    env = os.environ.copy()\n    if \"EXPERIMENT_ITERATION\" in env:\n        pass\n    run_subprocess(cmd, log_dir)\n\n    # trigger evaluation across all results currently present\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", str(abs_results_dir)]\n    subprocess.check_call(eval_cmd)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "from math import sqrt\nfrom typing import Any\n\nimport torch\nimport torch.nn as nn\nfrom omegaconf import DictConfig\n\n\n# -----------------------------------------------------------------------------\n# MobileNetV2 (width multiplier configurable)\n# -----------------------------------------------------------------------------\n\n\ndef _make_divisible(v, divisor=8, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    if new_v \u003c 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n        padding = (kernel_size - 1) // 2\n        super().__init__(\n            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n            nn.BatchNorm2d(out_planes),\n            nn.ReLU6(inplace=True),\n        )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super().__init__()\n        self.stride = stride\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        if expand_ratio != 1:\n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n        layers.extend(\n            [\n                ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ]\n        )\n        self.conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=10, width_mult: float = 1.0):\n        super().__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        input_channel = _make_divisible(input_channel * width_mult)\n        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult))\n        features = [ConvBNReLU(3, input_channel, stride=1)]  # CIFAR10 32x32 so stride 1 is fine\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = _make_divisible(c * width_mult)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n                input_channel = output_channel\n        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n        self.features = nn.Sequential(*features)\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, num_classes),\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.mean([2, 3])  # global average pooling\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n\n# -----------------------------------------------------------------------------\n# TinyDistilBERT (self-contained implementation, 6 layers)\n# -----------------------------------------------------------------------------\n\n\nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, hidden_size: int, n_heads: int, dropout: float):\n        super().__init__()\n        assert hidden_size % n_heads == 0\n        self.n_heads = n_heads\n        self.d_k = hidden_size // n_heads\n        self.qkv = nn.Linear(hidden_size, hidden_size * 3)\n        self.o = nn.Linear(hidden_size, hidden_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        B, T, C = x.size()\n        qkv = self.qkv(x).chunk(3, dim=-1)  # (B, T, C)\n        q, k, v = [t.view(B, T, self.n_heads, self.d_k).transpose(1, 2) for t in qkv]\n        attn_scores = (q @ k.transpose(-2, -1)) / sqrt(self.d_k)\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        attn = attn_scores.softmax(dim=-1)\n        attn = self.dropout(attn)\n        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, C)\n        return self.o(out)\n\n\nclass TransformerLayer(nn.Module):\n    def __init__(self, hidden_size: int, n_heads: int, dropout: float):\n        super().__init__()\n        self.attn = MultiHeadSelfAttention(hidden_size, n_heads, dropout)\n        self.ln1 = nn.LayerNorm(hidden_size)\n        self.ff = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size * 4),\n            nn.GELU(),\n            nn.Linear(hidden_size * 4, hidden_size),\n            nn.Dropout(dropout),\n        )\n        self.ln2 = nn.LayerNorm(hidden_size)\n\n    def forward(self, x, mask=None):\n        x = x + self.attn(self.ln1(x), mask)\n        x = x + self.ff(self.ln2(x))\n        return x\n\n\nclass TinyDistilBERT(nn.Module):\n    def __init__(self, cfg: DictConfig):\n        super().__init__()\n        self.hidden_size = cfg.model.hidden_size\n        self.n_layers = cfg.model.num_layers\n        self.n_heads = 12  # fixed for 768 dim\n        self.dropout = cfg.optuna.search_space.get(\"dropout\", {}).get(\"low\", 0.1) if \"optuna\" in cfg else 0.1\n\n        self.token_emb = nn.Embedding(30522, self.hidden_size, padding_idx=0)\n        self.pos_emb = nn.Parameter(torch.zeros(1, 512, self.hidden_size))\n        self.dropout_layer = nn.Dropout(self.dropout)\n        self.transformer = nn.ModuleList(\n            [TransformerLayer(self.hidden_size, self.n_heads, self.dropout) for _ in range(self.n_layers)]\n        )\n        self.classifier = nn.Linear(self.hidden_size, cfg.dataset.num_classes if \"num_classes\" in cfg.dataset else 2)\n\n    def forward(self, input_ids):\n        # input_ids shape: (B, T)\n        B, T = input_ids.size()\n        pos_emb = self.pos_emb[:, :T, :]\n        x = self.token_emb(input_ids) + pos_emb\n        x = self.dropout_layer(x)\n        for layer in self.transformer:\n            x = layer(x)\n        cls = x[:, 0, :]\n        return self.classifier(cls)\n\n\n# -----------------------------------------------------------------------------\n# Patch tokenizer for images to feed transformer\n# -----------------------------------------------------------------------------\n\n\nclass PatchTokenizer(nn.Module):\n    def __init__(self, cfg: DictConfig):\n        super().__init__()\n        img_size = cfg.dataset.input_size\n        patch_size = 4\n        self.n_patches = (img_size // patch_size) ** 2\n        self.patch_dim = 3 * patch_size * patch_size\n        hidden_size = cfg.model.hidden_size\n        self.proj = nn.Linear(self.patch_dim, hidden_size)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_size))\n        self.pos_emb = nn.Parameter(torch.zeros(1, self.n_patches + 1, hidden_size))\n\n    def forward(self, x):\n        # x: (B, C, H, W)\n        patches = x.unfold(2, 4, 4).unfold(3, 4, 4)  # (B, C, n_h, n_w, p_h, p_w)\n        patches = patches.contiguous().view(x.size(0), 3, -1, 4, 4)\n        patches = patches.permute(0, 2, 1, 3, 4).contiguous().view(x.size(0), -1, self.patch_dim)\n        tokens = self.proj(patches)\n        cls = self.cls_token.expand(x.size(0), -1, -1)\n        tokens = torch.cat([cls, tokens], dim=1)\n        tokens = tokens + self.pos_emb[:, : tokens.size(1), :]\n        return tokens\n\n\nclass VisionDistilBERT(nn.Module):\n    def __init__(self, cfg: DictConfig):\n        super().__init__()\n        self.tokenizer = PatchTokenizer(cfg)\n        self.transformer = TinyDistilBERT(cfg)\n\n    def forward(self, x):\n        tokens = self.tokenizer(x)\n        return self.transformer(tokens)\n\n\n# -----------------------------------------------------------------------------\n# Model factory\n# -----------------------------------------------------------------------------\n\n\ndef get_model(cfg: DictConfig) -\u003e nn.Module:\n    if cfg.model.name.lower().startswith(\"mobilenetv2\"):\n        width = float(cfg.model.variant)\n        return MobileNetV2(num_classes=cfg.dataset.num_classes, width_mult=width)\n    elif cfg.model.name.lower().startswith(\"distilbert\") or cfg.model.name.lower() == \"distilbert-base-uncased\":\n        if cfg.dataset.type == \"text_classification\":\n            return TinyDistilBERT(cfg)\n        elif cfg.dataset.type == \"image_classification\":\n            return VisionDistilBERT(cfg)\n        else:\n            raise ValueError(\"Unsupported dataset type for DistilBERT\")\n    else:\n        raise ValueError(f\"Unknown model name {cfg.model.name}\")\n", "preprocess_py": "from pathlib import Path\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom omegaconf import DictConfig\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\n\ntry:\n    from datasets import load_dataset  # type: ignore\n    from transformers import AutoTokenizer  # type: ignore\nexcept ImportError:\n    load_dataset = None  # type: ignore\n    AutoTokenizer = None  # type: ignore\n\n\nclass TextDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item, torch.tensor(self.labels[idx])\n\n    def __len__(self):\n        return len(self.labels)\n\n\ndef _get_cifar10(cfg: DictConfig):\n    augment_list = []\n    if \"random_crop\" in cfg.dataset.augmentation:\n        augment_list.append(transforms.RandomCrop(cfg.dataset.input_size, padding=4))\n    if \"random_horizontal_flip\" in cfg.dataset.augmentation:\n        augment_list.append(transforms.RandomHorizontalFlip())\n\n    transform_train = transforms.Compose(\n        augment_list + [transforms.ToTensor()]\n    )\n    transform_test = transforms.Compose([transforms.ToTensor()])\n\n    root = Path(torch.hub.get_dir()) / \"data\"\n    train_dataset_full = datasets.CIFAR10(root=str(root), train=True, download=True, transform=transform_train)\n    test_dataset = datasets.CIFAR10(root=str(root), train=False, download=True, transform=transform_test)\n\n    train_len = int(cfg.dataset.splits.train * len(train_dataset_full))\n    val_len = int(cfg.dataset.splits.val * len(train_dataset_full))\n    _ = len(train_dataset_full) - train_len - val_len\n    train_dataset, val_dataset, _ = random_split(\n        train_dataset_full, [train_len, val_len, len(train_dataset_full) - train_len - val_len]\n    )\n    return train_dataset, val_dataset, test_dataset\n\n\ndef _get_alpaca(cfg: DictConfig):\n    if load_dataset is None or AutoTokenizer is None:\n        raise ImportError(\"Please install \u0027datasets\u0027 and \u0027transformers\u0027 to use text datasets\")\n\n    ds = load_dataset(\"yahma/alpaca-cleaned\")\n    train_ds = ds[\"train\"]\n\n    # simple label extraction: classify on instruction length parity (example)\n    # This is a placeholder label creation for demonstration purposes.\n    train_ds = train_ds.train_test_split(test_size=1 - cfg.dataset.splits.train)\n    val_test_split = train_ds[\"test\"].train_test_split(test_size=cfg.dataset.splits.test / (cfg.dataset.splits.val + cfg.dataset.splits.test))\n    split_ds = {\n        \"train\": train_ds[\"train\"],\n        \"val\": val_test_split[\"train\"],\n        \"test\": val_test_split[\"test\"],\n    }\n\n    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n    tokenised = {}\n    for split, subset in split_ds.items():\n        enc = tokenizer(\n            subset[\"instruction\"],\n            truncation=cfg.dataset.truncation,\n            padding=cfg.dataset.padding,\n            max_length=cfg.dataset.max_length,\n        )\n        labels = (np.array(subset[\"output\"].str.len()) % 2).tolist()  # dummy labels\n        tokenised[split] = TextDataset(enc, labels)\n    return tokenised[\"train\"], tokenised[\"val\"], tokenised[\"test\"]\n\n\ndef create_dataloaders(cfg: DictConfig) -\u003e Tuple[DataLoader, DataLoader, DataLoader]:\n    if cfg.dataset.type == \"image_classification\":\n        train_ds, val_ds, test_ds = _get_cifar10(cfg)\n        collate_fn = None\n    elif cfg.dataset.type == \"text_classification\":\n        train_ds, val_ds, test_ds = _get_alpaca(cfg)\n\n        def collate_fn(batch):\n            x, y = list(zip(*batch))\n            x = {key: torch.stack([b[key] for b in x]) for key in x[0].keys()}\n            y = torch.stack(y)\n            return x, y\n    else:\n        raise ValueError(f\"Unsupported dataset type {cfg.dataset.type}\")\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg.training.batch_size,\n        shuffle=True,\n        num_workers=cfg.training.num_workers,\n        collate_fn=collate_fn,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n        num_workers=cfg.training.num_workers,\n        collate_fn=collate_fn,\n    )\n    test_loader = DataLoader(\n        test_ds,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n        num_workers=cfg.training.num_workers,\n        collate_fn=collate_fn,\n    )\n    return train_loader, val_loader, test_loader\n", "pyproject_toml": "[project]\nname = \"comparative2\"\nversion = \"0.1.0\"\nrequires-python = \"\u003e=3.8\"\ndependencies = [\n  \"torch\u003e=2.0.0\",\n  \"torchvision\",\n  \"hydra-core\u003e=1.3.0\",\n  \"omegaconf\",\n  \"optuna\",\n  \"wandb\",\n  \"datasets\",\n  \"transformers\",\n  \"matplotlib\",\n  \"pandas\"\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n", "train_py": "import json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport hydra\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport wandb\nfrom omegaconf import DictConfig, OmegaConf\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\n\nfrom .model import get_model\nfrom .preprocess import create_dataloaders\n\n\n# -----------------------------------------------------------------------------\n# Utility functions\n# -----------------------------------------------------------------------------\n\ndef _set_seed(seed: int) -\u003e None:\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _describe_experiment(cfg: DictConfig) -\u003e str:\n    return (\n        f\"Running experiment {cfg.run_id} (method={cfg.method})\\n\"\n        f\"Model : {cfg.model.name} / variant={cfg.model.get(\u0027variant\u0027, \u0027n/a\u0027)}\\n\"\n        f\"Dataset: {cfg.dataset.name} (type={cfg.dataset.type})\\n\"\n        f\"Training epochs={cfg.training.epochs}, batch_size={cfg.training.batch_size}, lr={cfg.training.learning_rate}\\n\"\n    )\n\n\n# -----------------------------------------------------------------------------\n# Training / evaluation helpers\n# -----------------------------------------------------------------------------\n\ndef train_epoch(model: nn.Module, loader: DataLoader, optimizer: optim.Optimizer, device: torch.device) -\u003e Tuple[float, float]:\n    model.train()\n    correct = 0\n    total = 0\n    running_loss = 0.0\n    for batch in loader:\n        inputs, targets = batch\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = F.cross_entropy(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * targets.size(0)\n        preds = outputs.argmax(dim=1)\n        correct += (preds == targets).sum().item()\n        total += targets.size(0)\n    return running_loss / total, correct / total\n\n\ndef eval_epoch(model: nn.Module, loader: DataLoader, device: torch.device) -\u003e Tuple[float, float]:\n    model.eval()\n    correct = 0\n    total = 0\n    running_loss = 0.0\n    with torch.no_grad():\n        for batch in loader:\n            inputs, targets = batch\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            outputs = model(inputs)\n            loss = F.cross_entropy(outputs, targets)\n            running_loss += loss.item() * targets.size(0)\n            preds = outputs.argmax(dim=1)\n            correct += (preds == targets).sum().item()\n            total += targets.size(0)\n    return running_loss / total, correct / total\n\n\n# -----------------------------------------------------------------------------\n# Main training entry\n# -----------------------------------------------------------------------------\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -\u003e None:\n    # Hydra changes the working directory; store original absolute result path\n    results_dir = Path(hydra.utils.to_absolute_path(cfg.results_dir))\n    run_dir = results_dir / cfg.run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # --- print human-readable description BEFORE any numerical outputs --------\n    description = _describe_experiment(cfg)\n    print(description, flush=True)\n\n    # ---------------------------- seeding -------------------------------------\n    _set_seed(cfg.training.seed)\n\n    # -------------------------- device ----------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and cfg.system.device == \"cuda\" else \"cpu\")\n\n    # ------------------------ wandb init --------------------------------------\n    wandb.login()\n    wandb_run = wandb.init(\n        project=cfg.wandb.project,\n        entity=cfg.wandb.entity,\n        name=cfg.wandb.run_name if \"run_name\" in cfg.wandb else cfg.run_id,\n        tags=list(cfg.wandb.get(\"tags\", [])),\n        config=OmegaConf.to_container(cfg, resolve=True),\n    )\n\n    # Save metadata right after initialisation\n    iteration = os.getenv(\"EXPERIMENT_ITERATION\", \"0\")\n    metadata_path = Path(f\".research/iteration{iteration}\")\n    metadata_path.mkdir(parents=True, exist_ok=True)\n    with open(metadata_path / \"wandb_metadata.json\", \"w\") as fp:\n        json.dump(\n            {\n                \"wandb_entity\": cfg.wandb.entity,\n                \"wandb_project\": cfg.wandb.project,\n                \"wandb_run_id\": wandb_run.id,\n            },\n            fp,\n            indent=2,\n        )\n    print(f\"WandB URL: {wandb_run.url}\", flush=True)\n\n    # ------------------------- data pipeline ----------------------------------\n    train_loader, val_loader, test_loader = create_dataloaders(cfg)\n\n    # ------------------------- model ------------------------------------------\n    model = get_model(cfg).to(device)\n\n    # ------------------------- optimiser \u0026 sched ------------------------------\n    if cfg.training.optimizer.lower() == \"sgd\":\n        optimizer = optim.SGD(\n            model.parameters(),\n            lr=cfg.training.learning_rate,\n            momentum=cfg.training.momentum,\n            weight_decay=cfg.training.weight_decay,\n        )\n    elif cfg.training.optimizer.lower() in {\"adam\", \"adamw\"}:\n        adam_cls = optim.AdamW if cfg.training.optimizer.lower() == \"adamw\" else optim.Adam\n        optimizer = adam_cls(\n            model.parameters(), lr=cfg.training.learning_rate, weight_decay=cfg.training.weight_decay\n        )\n    else:\n        raise ValueError(f\"Unsupported optimizer {cfg.training.optimizer}\")\n\n    if cfg.training.lr_scheduler == \"cosine\":\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.training.epochs)\n    elif cfg.training.lr_scheduler == \"linear\":\n        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: 1 - step / (cfg.training.epochs))\n    else:\n        scheduler = None\n\n    # --------------------------- training loop --------------------------------\n    history = {\n        \"epoch\": [],\n        \"train_loss\": [],\n        \"train_acc\": [],\n        \"val_loss\": [],\n        \"val_acc\": [],\n    }\n\n    best_val_acc = 0.0\n    start_time = time.time()\n    for epoch in range(1, cfg.training.epochs + 1):\n        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n        val_loss, val_acc = eval_epoch(model, val_loader, device)\n        if scheduler:\n            scheduler.step()\n\n        # logging\n        wandb.log(\n            {\n                \"epoch\": epoch,\n                \"train/loss\": train_loss,\n                \"train/acc\": train_acc,\n                \"val/loss\": val_loss,\n                \"val/acc\": val_acc,\n            }\n        )\n\n        history[\"epoch\"].append(epoch)\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), run_dir / \"best.pt\")\n            wandb.save(str(run_dir / \"best.pt\"))\n\n    # -------------------------------- test ------------------------------------\n    test_loss, test_acc = eval_epoch(model, test_loader, device)\n    wandb.summary[\"test/loss\"] = test_loss\n    wandb.summary[\"test/acc\"] = test_acc\n\n    runtime_sec = time.time() - start_time\n\n    # ------------------------ save metrics ------------------------------------\n    results = {\n        \"run_id\": cfg.run_id,\n        \"best_val_acc\": best_val_acc,\n        \"test_acc\": test_acc,\n        \"test_loss\": test_loss,\n        \"train_history\": history,\n        \"runtime_sec\": runtime_sec,\n    }\n    with open(run_dir / \"results.json\", \"w\") as fp:\n        json.dump(results, fp, indent=2)\n\n    # print JSON to stdout as required\n    print(json.dumps(results))\n\n    wandb.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "The project is NOT ready for execution. Critical blocking issues:\n\n1. Missing configuration files\n   • config/run/*.yaml files referenced by Hydra are absent – code cannot even start.\n\n2. Dataset preprocessing placeholders / errors\n   • alpaca-cleaned pipeline creates dummy labels (“instruction length parity”) which is explicitly marked as a placeholder → violates ‘no placeholder’ rule.\n   • Uses pandas-style .str accessor on a list returned by HuggingFace datasets -> will raise AttributeError.\n\n3. Training loop incompatible with text modality\n   • train_epoch / eval_epoch call inputs.to(device); for text datasets inputs is a dict → AttributeError at first batch.\n\n4. Vision DistilBERT implementation wrong\n   • PatchTokenizer returns float embeddings but TinyDistilBERT expects integer token ids; shape/type mismatch causes runtime failure for CIFAR-10/DistilBERT runs.\n\n5. Normalisation flag in CIFAR-10 config ignored; required preprocessing not implemented.\n\n6. Hydra integration only partial\n   • config/config.yaml lists defaults yet run group directory missing, so CLI specified in requirements will fail.\n\nBecause of these issues the code is incomplete, not executable, and not publication-ready."
}
