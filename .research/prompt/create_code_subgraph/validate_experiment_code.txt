
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches: `uv run python -u -m src.main run={run_id} results_dir={path}`

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations

4. **Model Implementation**:
   - Complete model architectures for baseline, proposed, and ablation methods
   - All models implemented from scratch (no placeholders)
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - No missing files from the structure
   - All functionality contained within specified files

6. **Structured Logging**:
   - train.py prints JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py prints JSON-formatted comparison results to stdout
   - main.py redirects subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log`
   - Logs are forwarded to main process stdout/stderr for GitHub Actions capture

7. **Configuration Files**:
   - config/config.yaml contains list of all experiment run_ids
   - NOTE: config/run/{run_id}.yaml files are provided separately in the "Experiment Runs" section below (not in ExperimentCode)
   - The generated code must properly reference these config files via Hydra
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation and Figures** (if WandB not used):
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/`)
   - Figures include legends, annotations, and proper labels
   - Uses `plt.tight_layout()` before saving
   - Consistent result formatting and comparison logic

9. **WandB Integration** (if WandB is used):
   - Proper WandB initialization with entity/project from config
   - Metrics logged to WandB during training
   - WandB run URL printed to stdout
   - Metadata saved to `.research/iteration{experiment_iteration}/wandb_metadata.json`
   - Figures uploaded as WandB artifacts

10. **Immediate Executability**:
    - Code can be run immediately without modifications
    - All imports and dependencies properly specified in pyproject.toml
    - No missing external resources or undefined variables
    - Proper module structure for `uv run python -m src.main` execution

## Detection of Common Issues
Flag the following problems if found:

- **Incomplete Implementation**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns
- **Truncation**: Code sections that appear shortened or simplified inappropriately
- **Missing Functionality**: Expected features not implemented
- **Configuration Issues**: Missing or incomplete run configs
- **Import Errors**: Missing dependencies or incorrect import statements
- **Not Executable**: Code that cannot be run immediately
- **Inconsistent Structure**: Files not matching required structure
- **Logging Issues**: Missing or incorrect JSON logging format

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

# Experimental Design
- Strategy: Comparative analysis of efficient model architectures across vision and language tasks
- Proposed Method: Evaluate MobileNetV2 and DistilBERT variants for efficient deployment
- Evaluation Metrics: ['accuracy', 'f1_score', 'inference_time', 'model_size']

# Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  width_mult: 0.5
  dropout: 0.2
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
  augmentations:
    random_crop: true
    random_flip: horizontal
training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.05
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0005
  lr_scheduler:
    type: cosine
    T_max: 50
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 0.001
      high: 0.1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    momentum:
      type: uniform
      low: 0.7
      high: 0.95
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  input_adapter:
    type: char_cnn
    embedding_dim: 256
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: char
training:
  epochs: 3
  batch_size: 16
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 0.01
  gradient_accumulation_steps: 2
optuna:
  n_trials: 15
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-1

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-CIFAR-10
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
  modality_adapter:
    type: linear_patch_embedding
    patch_size: 4
    sequence_length: 64
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    patch_size:
      type: categorical
      choices: [2, 4, 8]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: sentencepiece
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "# ------------------------------------------------------------------------------------\n# Top-level Hydra configuration file.\n# The `run` entry will be overridden from the command-line to select which experiment\n# variation to execute (maps to config/run/*.yaml files).\n# ------------------------------------------------------------------------------------\n\ndefaults:\n  - run: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10  # default run, override via CLI\n  - _self_\n\n# Directory that GitHub Actions passes; where artefacts \u0026 logs are stored\nresults_dir: ./outputs\n\n# Whether main.py should execute evaluate.py after the train subprocess finishes\nrun_evaluation: true\n\n# The following keys are placeholders that get populated by the included run-specific file\nrun:\n  run_id: PLACEHOLDER_TO_BE_OVERRIDDEN\n  method: ???\n  model: {}\n  dataset: {}\n  training: {}\n\n# Allow sweeps / Optuna etc. (unused here)\noptuna: {}\n\n# End of config.yaml\n", "evaluate_py": "\"\"\"\nEvaluation / comparison script.\nAggregates individual result.json files produced by train.py, generates a bar chart\nand logs a W\u0026B artifact so results can be inspected remotely.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict\n\nimport hydra\nfrom omegaconf import DictConfig\nimport matplotlib\n\nmatplotlib.use(\"Agg\")  # Use non-interactive backend for CI\nimport matplotlib.pyplot as plt  # noqa: E402  (after Agg set)\nimport wandb  # noqa: E402\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef evaluate(cfg: DictConfig) -\u003e None:\n    results_dir = Path(cfg.results_dir)\n    assert results_dir.exists(), f\"results_dir {results_dir} not found\"\n\n    comparison: Dict[str, Dict] = {}\n    run_ids = []\n    test_accs = []\n\n    for sub in results_dir.iterdir():\n        if not sub.is_dir():\n            continue\n        result_file = sub / \"results.json\"\n        if not result_file.exists():\n            continue\n        with result_file.open(\"r\", encoding=\"utf-8\") as fp:\n            res = json.load(fp)\n        run_id = res[\"run_id\"]\n        comparison[run_id] = res\n        run_ids.append(run_id)\n        test_accs.append(res.get(\"test_accuracy\", 0.0))\n\n    # ------------------ Plotting ------------------------------------------------------\n    fig, ax = plt.subplots(figsize=(10, 4))\n    ax.bar(run_ids, test_accs)\n    ax.set_ylabel(\"Test Accuracy\")\n    ax.set_title(\"Comparison of Test Accuracy across Runs\")\n    ax.set_xticklabels(run_ids, rotation=45, ha=\"right\")\n    plt.tight_layout()\n\n    fig_path = results_dir / \"comparison.png\"\n    fig.savefig(fig_path, dpi=150)\n\n    # --------------- Log to W\u0026B -------------------------------------------------------\n    wandb_run = wandb.init(\n        project=\"251014-test\",\n        entity=\"gengaru617\",\n        name=\"evaluation\",\n        reinit=True,\n        config={\"evaluated_runs\": run_ids},\n    )\n    artifact = wandb.Artifact(name=\"comparison-figure\", type=\"figure\")\n    artifact.add_file(str(fig_path))\n    wandb_run.log_artifact(artifact)\n    wandb_run.finish()\n\n    # --------------- Structured logging output ---------------------------------------\n    print(json.dumps({\"run_id\": \"evaluation\", \"comparison\": comparison}))\n\n\nif __name__ == \"__main__\":\n    evaluate()", "main_py": "\"\"\"\nMain orchestrator script.\nReceives run_id \u0026 results_dir via Hydra command line override, spawns train.py as\nsubprocess, handles log redirection and optionally triggers evaluate.py.\n\"\"\"\nfrom __future__ import annotations\n\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import IO\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\ndef _tee_stream(stream: IO[bytes], log_path: Path, is_stdout: bool):\n    \"\"\"Read bytes from *stream*, write to both console and *log_path*.\"\"\"\n\n    log_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(log_path, \"ab\") as fp:\n        for line in iter(stream.readline, b\"\"):\n            fp.write(line)\n            fp.flush()\n            text = line.decode(errors=\"replace\")\n            if is_stdout:\n                sys.stdout.write(text)\n                sys.stdout.flush()\n            else:\n                sys.stderr.write(text)\n                sys.stderr.flush()\n\n\ndef _launch_train(run_id: str, results_dir: Path):\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={run_id}\",\n        f\"results_dir={str(results_dir)}\",\n    ]\n\n    process = subprocess.Popen(\n        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1\n    )\n\n    stdout_path = results_dir / run_id / \"stdout.log\"\n    stderr_path = results_dir / run_id / \"stderr.log\"\n\n    t_out = threading.Thread(\n        target=_tee_stream, args=(process.stdout, stdout_path, True), daemon=True\n    )\n    t_err = threading.Thread(\n        target=_tee_stream, args=(process.stderr, stderr_path, False), daemon=True\n    )\n    t_out.start()\n    t_err.start()\n\n    process.wait()\n    t_out.join()\n    t_err.join()\n\n    if process.returncode != 0:\n        raise RuntimeError(f\"Training subprocess for {run_id} exited with code {process.returncode}\")\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    run_id = cfg.get(\"run\") if isinstance(cfg.get(\"run\"), str) else cfg.run.run_id\n    results_dir = Path(cfg.results_dir).absolute()\n\n    _launch_train(run_id, results_dir)\n\n    if cfg.get(\"run_evaluation\", True):\n        # Trigger evaluation across all available results\n        eval_cmd = [\n            sys.executable,\n            \"-u\",\n            \"-m\",\n            \"src.evaluate\",\n            f\"results_dir={str(results_dir)}\",\n        ]\n        subprocess.check_call(eval_cmd)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "\"\"\"\nModel definitions and builders.\nIncludes MobileNetV2, DistilBERT adapters and a simple CharCNN-based classifier.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import Any, Dict\n\nimport torch\nfrom torch import nn\nimport torchvision.models as tvm\nfrom transformers import DistilBertConfig, DistilBertModel\n\nfrom .preprocess import _CHAR2IDX  # Re-use global vocabulary\n\n\n# ----------------------------------------------------------------------------------\n# Character-level CNN encoder (for text)\n# ----------------------------------------------------------------------------------\n\nclass CharCNNEncoder(nn.Module):\n    def __init__(self, vocab_size: int, embedding_dim: int, conv_channels: int = 128):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=_CHAR2IDX[\"\u003cpad\u003e\"])\n        self.conv = nn.Conv1d(embedding_dim, conv_channels, kernel_size=5, padding=2)\n        self.pool = nn.AdaptiveMaxPool1d(1)\n        self.output_dim = conv_channels\n\n    def forward(self, x):  # x: (B, L)\n        emb = self.embedding(x)  # (B, L, E)\n        emb = emb.transpose(1, 2)  # (B, E, L)\n        feat = torch.relu(self.conv(emb))\n        pooled = self.pool(feat).squeeze(-1)  # (B, C)\n        return pooled  # (B, output_dim)\n\n\n# ----------------------------------------------------------------------------------\n# Adapter that turns char features into fake RGB images so MobileNet can process text\n# ----------------------------------------------------------------------------------\n\nclass Char2ImageAdapter(nn.Module):\n    def __init__(self, in_dim: int, img_size: int = 32, out_channels: int = 3):\n        super().__init__()\n        self.img_size = img_size\n        self.out_channels = out_channels\n        self.proj = nn.Linear(in_dim, out_channels * img_size * img_size)\n\n    def forward(self, x):  # x: (B, F)\n        out = self.proj(x)\n        out = out.view(-1, self.out_channels, self.img_size, self.img_size)\n        return out\n\n\n# ----------------------------------------------------------------------------------\n# Patch embedding adapter for feeding images into DistilBERT\n# ----------------------------------------------------------------------------------\n\nclass LinearPatchEmbedding(nn.Module):\n    def __init__(self, patch_size: int, img_size: int, embed_dim: int = 768):\n        super().__init__()\n        self.patch_size = patch_size\n        self.img_size = img_size\n        self.embed_dim = embed_dim\n        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n        self.proj = nn.Linear(patch_size * patch_size * 3, embed_dim)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n\n    def forward(self, x):  # x: (B, 3, H, W)\n        B, C, H, W = x.shape\n        p = self.patch_size\n        assert H == self.img_size and W == self.img_size\n        # unfold -\u003e (B, C*p*p, N)\n        patches = x.unfold(2, p, p).unfold(3, p, p)\n        patches = patches.contiguous().view(B, C, -1, p, p)\n        patches = patches.permute(0, 2, 1, 3, 4)  # (B, N, C, p, p)\n        patches = patches.contiguous().view(B, -1, C * p * p)  # (B, N, patch_vec)\n        patch_embeddings = self.proj(patches)  # (B, N, D)\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        tokens = torch.cat([cls_tokens, patch_embeddings], dim=1)  # (B, N+1, D)\n        tokens = tokens + self.pos_embed  # positional encoding\n        return tokens\n\n\n# ----------------------------------------------------------------------------------\n# DistilBERT classifier that consumes image patches via LinearPatchEmbedding\n# ----------------------------------------------------------------------------------\n\nclass ImageDistilBertClassifier(nn.Module):\n    def __init__(self, num_classes: int, patch_size: int, img_size: int = 32):\n        super().__init__()\n        self.patch_embed = LinearPatchEmbedding(patch_size, img_size, embed_dim=768)\n        dbert_cfg = DistilBertConfig(\n            vocab_size=30522,  # dummy\n            max_position_embeddings=self.patch_embed.num_patches + 1,\n            n_heads=6,\n            n_layers=6,\n        )\n        self.transformer = DistilBertModel(dbert_cfg)\n        self.classifier = nn.Linear(768, num_classes)\n\n    def forward(self, x):  # x: (B, 3, H, W)\n        tokens = self.patch_embed(x)\n        attention_mask = torch.ones(tokens.shape[:2], dtype=torch.long, device=x.device)\n        outputs = self.transformer(inputs_embeds=tokens, attention_mask=attention_mask)\n        cls_repr = outputs.last_hidden_state[:, 0]  # (B, 768)\n        logits = self.classifier(cls_repr)\n        return logits\n\n\n# ----------------------------------------------------------------------------------\n# Build function exposed to training script\n# ----------------------------------------------------------------------------------\n\ndef build_model(cfg: Dict[str, Any], dataset_meta: Dict[str, Any]) -\u003e nn.Module:\n    model_cfg = cfg.model\n    name = model_cfg.name.lower()\n\n    if name.startswith(\"mobilenetv2\"):\n        width_mult = float(model_cfg.get(\"width_mult\", 1.0))\n        pretrained = model_cfg.get(\"pretrained\", False)\n        backbone = tvm.mobilenet_v2(width_mult=width_mult, pretrained=pretrained)\n        in_features = backbone.classifier[1].in_features\n        backbone.classifier[1] = nn.Linear(in_features, dataset_meta[\"num_classes\"])\n\n        if \"input_adapter\" in model_cfg:\n            adapter_cfg = model_cfg.input_adapter\n            if adapter_cfg.type == \"char_cnn\":\n                vocab_size = dataset_meta[\"vocab_size\"]\n                encoder = CharCNNEncoder(\n                    vocab_size=vocab_size,\n                    embedding_dim=adapter_cfg.embedding_dim,\n                )\n                adapter = Char2ImageAdapter(encoder.output_dim)\n\n                class ModelWrapper(nn.Module):\n                    def __init__(self, enc, adp, back):\n                        super().__init__()\n                        self.enc = enc\n                        self.adp = adp\n                        self.back = back\n\n                    def forward(self, x):  # x: (B, L)\n                        feat = self.enc(x)\n                        img = self.adp(feat)\n                        return self.back(img)\n\n                return ModelWrapper(encoder, adapter, backbone)\n            else:\n                raise ValueError(\"Unsupported MobileNet input adapter\")\n        else:\n            return backbone\n\n    elif \"distilbert\" in name:\n        if \"modality_adapter\" in model_cfg and model_cfg.modality_adapter.type == \"linear_patch_embedding\":\n            patch_size = int(model_cfg.modality_adapter.patch_size)\n            return ImageDistilBertClassifier(\n                num_classes=dataset_meta[\"num_classes\"],\n                patch_size=patch_size,\n                img_size=dataset_meta[\"input_shape\"][1],\n            )\n        else:\n            # Default: text classification using DistilBERT wordpiece tokenizer\n            from transformers import DistilBertForSequenceClassification, AutoConfig\n\n            num_classes = dataset_meta[\"num_classes\"]\n            config = AutoConfig.from_pretrained(model_cfg.name, num_labels=num_classes)\n            model = DistilBertForSequenceClassification.from_pretrained(\n                model_cfg.name, config=config\n            )\n            return model\n\n    else:\n        raise ValueError(f\"Unsupported model name {model_cfg.name}\")\n", "preprocess_py": "\"\"\"\nData-loading and preprocessing utilities for both vision (CIFAR-10) and text (alpaca-cleaned).\nAll heavy-lifting lives here so train.py remains concise.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport random\nfrom pathlib import Path\nfrom typing import Dict, Tuple, List\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, Dataset\nimport torchvision.transforms as T\nfrom torchvision.datasets import CIFAR10\n\n# ----------------------------------------------------------------------------------\n# Character vocabulary (shared across modules)\n# ----------------------------------------------------------------------------------\n_CHARSET = (\n    [\n        \"\u003cpad\u003e\",\n        \"\u003cunk\u003e\",\n    ]\n    + [chr(i) for i in range(32, 127)]  # printable ascii\n)\n_CHAR2IDX: Dict[str, int] = {ch: i for i, ch in enumerate(_CHARSET)}\n_IDX2CHAR: List[str] = _CHARSET\n_PAD_IDX = _CHAR2IDX[\"\u003cpad\u003e\"]\n_UNK_IDX = _CHAR2IDX[\"\u003cunk\u003e\"]\n\n\n# ----------------------------------------------------------------------------------\n# Utility functions for text\n# ----------------------------------------------------------------------------------\n\ndef tokenize_char(text: str, max_length: int) -\u003e List[int]:\n    tokens = [_CHAR2IDX.get(ch, _UNK_IDX) for ch in text[:max_length]]\n    if len(tokens) \u003c max_length:\n        tokens += [_PAD_IDX] * (max_length - len(tokens))\n    return tokens\n\n\n# ----------------------------------------------------------------------------------\n# Alpaca dataset (binary classification based on output length)\n# ----------------------------------------------------------------------------------\n\n\nclass AlpacaDataset(Dataset):\n    def __init__(\n        self,\n        json_path: Path,\n        max_seq_len: int,\n        tokenizer: str = \"char\",\n    ) -\u003e None:\n        if not json_path.exists():\n            print(\n                f\"[WARNING] {json_path} not found. Generating synthetic dataset for debugging.\"\n            )\n            self.samples = self._generate_synthetic(1000)\n        else:\n            with json_path.open(\"r\", encoding=\"utf-8\") as fp:\n                self.samples = [json.loads(l) for l in fp]\n\n        self.max_seq_len = max_seq_len\n        self.tokenizer = tokenizer\n\n    @staticmethod\n    def _generate_synthetic(n: int):\n        samples = []\n        for _ in range(n):\n            length = random.randint(10, 200)\n            text = \"a\" * length\n            samples.append(\n                {\n                    \"instruction\": \"generate text\",\n                    \"input\": \"\",\n                    \"output\": text,\n                }\n            )\n        return samples\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        record = self.samples[idx]\n        # Form text by concatenating instruction, input and output\n        text = (\n            record.get(\"instruction\", \"\")\n            + \" \"\n            + record.get(\"input\", \"\")\n            + \" \"\n            + record.get(\"output\", \"\")\n        ).strip()\n        label = 1 if len(record.get(\"output\", \"\")) \u003e 100 else 0\n        if self.tokenizer == \"char\":\n            input_ids = tokenize_char(text.lower(), self.max_seq_len)\n            input_ids = torch.LongTensor(input_ids)\n        else:\n            raise ValueError(\"Unsupported tokenizer\")\n\n        return input_ids, torch.tensor(label, dtype=torch.long)\n\n\n# ----------------------------------------------------------------------------------\n# Collate fns\n# ----------------------------------------------------------------------------------\n\ndef _collate_text(batch):\n    inputs, labels = zip(*batch)\n    inputs = torch.stack(inputs, dim=0)\n    labels = torch.stack(labels, dim=0)\n    return inputs, labels\n\n\n# ----------------------------------------------------------------------------------\n# High-level API called from train.py\n# ----------------------------------------------------------------------------------\n\ndef get_dataloaders(dataset_cfg, train_cfg) -\u003e Tuple[Dict[str, DataLoader], Dict]:\n    if dataset_cfg.name.lower() in {\"cifar-10\", \"cifar10\"}:\n        return _get_cifar10_loaders(dataset_cfg, train_cfg)\n    elif \"alpaca\" in dataset_cfg.name.lower():\n        return _get_alpaca_loaders(dataset_cfg, train_cfg)\n    else:\n        raise ValueError(f\"Unsupported dataset {dataset_cfg.name}\")\n\n\n# ------------------------------------ CIFAR-10 ------------------------------------\n\ndef _get_cifar10_loaders(dataset_cfg, train_cfg):\n    mean = dataset_cfg.normalization.mean\n    std = dataset_cfg.normalization.std\n\n    train_transform_ops = []\n    aug = dataset_cfg.augmentations\n    if aug.get(\"random_crop\", False):\n        train_transform_ops.append(T.RandomCrop(dataset_cfg.image_size, padding=4))\n    if aug.get(\"random_flip\", \"\") == \"horizontal\":\n        train_transform_ops.append(T.RandomHorizontalFlip())\n    train_transform_ops += [T.ToTensor(), T.Normalize(mean, std)]\n\n    test_transform = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n\n    dataset_root = Path(\"./data\")\n    full_train = CIFAR10(\n        root=dataset_root,\n        train=True,\n        download=True,\n        transform=T.Compose(train_transform_ops),\n    )\n    test_set = CIFAR10(\n        root=dataset_root,\n        train=False,\n        download=True,\n        transform=test_transform,\n    )\n\n    train_len = dataset_cfg.splits.train\n    val_len = dataset_cfg.splits.val\n    assert train_len + val_len \u003c= len(full_train), \"Invalid split sizes for CIFAR-10\"\n    train_set, val_set, _ = random_split(\n        full_train, [train_len, val_len, len(full_train) - train_len - val_len]\n    )\n\n    loader_kwargs = {\n        \"batch_size\": train_cfg.batch_size,\n        \"num_workers\": 2,\n        \"pin_memory\": torch.cuda.is_available(),\n    }\n\n    train_loader = DataLoader(train_set, shuffle=True, **loader_kwargs)\n    val_loader = DataLoader(val_set, shuffle=False, **loader_kwargs)\n    test_loader = DataLoader(test_set, shuffle=False, **loader_kwargs)\n\n    meta = {\n        \"num_classes\": 10,\n        \"input_shape\": (3, dataset_cfg.image_size, dataset_cfg.image_size),\n    }\n    return {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}, meta\n\n\n# ------------------------------------ Alpaca ---------------------------------------\n\ndef _get_alpaca_loaders(dataset_cfg, train_cfg):\n    if dataset_cfg.name == \"alpaca-cleaned\":\n        json_path = Path(dataset_cfg.get(\"path\", \"./data/alpaca_cleaned.json\"))\n    else:\n        raise ValueError(\"Only alpaca-cleaned supported in this template\")\n\n    full_dataset = AlpacaDataset(\n        json_path=json_path,\n        max_seq_len=dataset_cfg.max_seq_length,\n        tokenizer=dataset_cfg.get(\"tokenizer\", \"char\"),\n    )\n\n    total_len = len(full_dataset)\n    val_len = int(0.1 * total_len)\n    train_len = total_len - val_len\n\n    train_set, val_set = random_split(full_dataset, [train_len, val_len])\n\n    loader_kwargs = {\n        \"batch_size\": train_cfg.batch_size,\n        \"num_workers\": 0,\n        \"collate_fn\": _collate_text,\n        \"pin_memory\": torch.cuda.is_available(),\n    }\n\n    train_loader = DataLoader(train_set, shuffle=True, **loader_kwargs)\n    val_loader = DataLoader(val_set, shuffle=False, **loader_kwargs)\n    test_loader = DataLoader(val_set, shuffle=False, **loader_kwargs)  # reuse val as test\n\n    meta = {\n        \"num_classes\": 2,\n        \"vocab_size\": len(_CHARSET),\n        \"pad_idx\": _PAD_IDX,\n        \"input_shape\": (dataset_cfg.max_seq_length,),\n    }\n    return {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}, meta\n", "pyproject_toml": "[tool.poetry]\nname = \"hydra-experiment-template\"\nversion = \"0.1.0\"\ndescription = \"Reproducible research code with Hydra and WandB.\"\nauthors = [\"AI Researcher \u003cresearch@ai.lab\u003e\"]\n\n[tool.poetry.dependencies]\npython = \"^3.8\"\ntorch = \"^2.1\"\ntorchvision = \"^0.16\"\ntransformers = \"^4.35\"\nhydra-core = \"^1.3\"\nwandb = \"^0.16\"\noptuna = \"^3.4\"\nmatplotlib = \"^3.8\"\nscikit-learn = \"^1.3\"\nsentencepiece = \"^0.1\"\ntqdm = \"^4.66\"\n\n[build-system]\nrequires = [\"poetry-core\u003e=1.7.0\"]\nbuild-backend = \"poetry.core.masonry.api\"", "train_py": "\"\"\"\nTraining script for a single experiment variation.\nThis is executed as a subprocess from main.py so that every run gets its own Hydra job and\nclean logging separation.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport wandb\n\nfrom .preprocess import get_dataloaders\nfrom .model import build_model\n\n\n# --------------------------------------------------------------------------------------\n# Helper utilities\n# --------------------------------------------------------------------------------------\n\ndef _build_optimizer(model: nn.Module, cfg: DictConfig) -\u003e torch.optim.Optimizer:\n    train_cfg = cfg.training\n    params = [p for p in model.parameters() if p.requires_grad]\n    if train_cfg.optimizer.lower() == \"sgd\":\n        return torch.optim.SGD(\n            params,\n            lr=train_cfg.learning_rate,\n            momentum=train_cfg.get(\"momentum\", 0.0),\n            weight_decay=train_cfg.get(\"weight_decay\", 0.0),\n        )\n    elif train_cfg.optimizer.lower() == \"adam\":\n        return torch.optim.Adam(\n            params,\n            lr=train_cfg.learning_rate,\n            weight_decay=train_cfg.get(\"weight_decay\", 0.0),\n        )\n    elif train_cfg.optimizer.lower() == \"adamw\":\n        return torch.optim.AdamW(\n            params,\n            lr=train_cfg.learning_rate,\n            weight_decay=train_cfg.get(\"weight_decay\", 0.0),\n        )\n    else:\n        raise ValueError(f\"Unsupported optimizer {train_cfg.optimizer}\")\n\n\ndef _build_scheduler(optimizer: torch.optim.Optimizer, cfg: DictConfig):\n    sched_cfg = cfg.training.get(\"lr_scheduler\")\n    if sched_cfg is None:\n        return None\n    if sched_cfg.type == \"cosine\":\n        return torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=sched_cfg.T_max, eta_min=0.0\n        )\n    elif sched_cfg.type == \"linear\":\n        def _lr_lambda(current_step: int):\n            warmup = sched_cfg.warmup_steps\n            if current_step \u003c warmup:\n                return float(current_step) / float(max(1, warmup))\n            return max(\n                0.0,\n                float(sched_cfg.total_steps - current_step)\n                / float(max(1, sched_cfg.total_steps - warmup)),\n            )\n\n        return torch.optim.lr_scheduler.LambdaLR(optimizer, _lr_lambda)\n    else:\n        raise ValueError(f\"Unsupported scheduler type {sched_cfg.type}\")\n\n\ndef _save_checkpoint(model: nn.Module, path: Path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    torch.save(model.state_dict(), path)\n\n\n# --------------------------------------------------------------------------------------\n# Main training entry point (Hydra)\n# --------------------------------------------------------------------------------------\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef train(cfg: DictConfig) -\u003e None:  # noqa: C901  (keep single function for Hydra simplicity)\n    # ------------------------------------------------------------------\n    # Extract run specific configuration section\n    # ------------------------------------------------------------------\n    run_cfg = cfg.run\n    run_id: str = run_cfg.run_id\n\n    # ------------------------------------------------------------------\n    # Prepare result directory\n    # ------------------------------------------------------------------\n    results_dir = Path(cfg.results_dir) / run_id\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # Initialize Weights \u0026 Biases\n    # ------------------------------------------------------------------\n    wandb_cfg = run_cfg.get(\"wandb\", {})\n    wandb_run = wandb.init(\n        entity=wandb_cfg.get(\"entity\", \"gengaru617\"),\n        project=wandb_cfg.get(\"project\", \"251014-test\"),\n        name=wandb_cfg.get(\"run_name\", run_id),\n        tags=wandb_cfg.get(\"tags\", []),\n        config=OmegaConf.to_container(run_cfg, resolve=True),\n        reinit=True,\n    )\n    # Save metadata for CI tracking -----------------------------------------------------\n    iteration_idx = int(os.environ.get(\"EXPERIMENT_ITERATION\", \"1\"))\n    metadata_path = (\n        Path(\".research\") / f\"iteration{iteration_idx}\" / \"wandb_metadata.json\"\n    )\n    metadata_path.parent.mkdir(parents=True, exist_ok=True)\n    with metadata_path.open(\"w\", encoding=\"utf-8\") as fp:\n        json.dump(\n            {\n                \"wandb_entity\": wandb_run.entity,\n                \"wandb_project\": wandb_run.project,\n                \"wandb_run_id\": wandb_run.id,\n            },\n            fp,\n            indent=2,\n        )\n\n    print(f\"WandB run page: {wandb_run.url}\")  # Required by spec\n\n    # ------------------------------------------------------------------\n    # Data pipeline\n    # ------------------------------------------------------------------\n    dataloaders, dataset_meta = get_dataloaders(run_cfg.dataset, run_cfg.training)\n    train_loader: DataLoader = dataloaders[\"train\"]\n    val_loader: DataLoader = dataloaders[\"val\"]\n    test_loader: DataLoader = dataloaders[\"test\"]\n\n    # ------------------------------------------------------------------\n    # Model + Optimizer + Sched.\n    # ------------------------------------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = build_model(run_cfg, dataset_meta).to(device)\n\n    optimizer = _build_optimizer(model, run_cfg)\n    scheduler = _build_scheduler(optimizer, run_cfg)\n\n    criterion = nn.CrossEntropyLoss()\n\n    best_val_acc = 0.0\n    best_epoch = -1\n    epoch_metrics = []\n\n    num_epochs: int = run_cfg.training.epochs\n    gradient_accum_steps: int = run_cfg.training.get(\"gradient_accumulation_steps\", 1)\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        correct = 0\n        optimizer.zero_grad(set_to_none=True)\n\n        train_pbar = tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{num_epochs}\")\n        for step, (inputs, labels) in enumerate(train_pbar):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels) / gradient_accum_steps\n            loss.backward()\n\n            if (step + 1) % gradient_accum_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad(set_to_none=True)\n\n            running_loss += loss.item() * inputs.size(0) * gradient_accum_steps\n            preds = outputs.argmax(dim=1)\n            total += labels.size(0)\n            correct += (preds == labels).sum().item()\n\n        train_loss = running_loss / total\n        train_acc = correct / total\n\n        # ---------------------- Validation -----------------------------\n        model.eval()\n        val_total, val_correct, val_loss_accum = 0, 0, 0.0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss_accum += loss.item() * inputs.size(0)\n                preds = outputs.argmax(dim=1)\n                val_total += labels.size(0)\n                val_correct += (preds == labels).sum().item()\n\n        val_loss = val_loss_accum / val_total\n        val_acc = val_correct / val_total\n\n        # Learning-rate scheduler step ----------------------------------\n        if scheduler is not None:\n            scheduler.step()\n\n        # Log metrics ----------------------------------------------------\n        wandb_run.log(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"train_acc\": train_acc,\n                \"val_loss\": val_loss,\n                \"val_acc\": val_acc,\n                \"lr\": optimizer.param_groups[0][\"lr\"],\n            },\n            step=epoch,\n        )\n\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            best_epoch = epoch\n            checkpoint_path = results_dir / \"checkpoints\" / \"best.pt\"\n            _save_checkpoint(model, checkpoint_path)\n\n            # Log checkpoint as artifact --------------------------------\n            artifact = wandb.Artifact(name=f\"{run_id}-model\", type=\"model\")\n            artifact.add_file(str(checkpoint_path))\n            wandb_run.log_artifact(artifact)\n\n        epoch_metrics.append(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"train_acc\": train_acc,\n                \"val_loss\": val_loss,\n                \"val_acc\": val_acc,\n            }\n        )\n\n    # ------------------------------------------------------------------\n    # Test evaluation using best model ----------------------------------\n    # ------------------------------------------------------------------\n    best_ckpt = results_dir / \"checkpoints\" / \"best.pt\"\n    model.load_state_dict(torch.load(best_ckpt, map_location=device))\n\n    model.eval()\n    test_total, test_correct = 0, 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            preds = outputs.argmax(dim=1)\n            test_total += labels.size(0)\n            test_correct += (preds == labels).sum().item()\n\n    test_acc = test_correct / test_total\n\n    wandb_run.summary[\"best_val_acc\"] = best_val_acc\n    wandb_run.summary[\"test_acc\"] = test_acc\n    wandb_run.finish()\n\n    # ------------------------------------------------------------------\n    # Persist \u0026 print structured results -------------------------------\n    # ------------------------------------------------------------------\n    results: Dict[str, Any] = {\n        \"run_id\": run_id,\n        \"method\": run_cfg.method,\n        \"best_val_accuracy\": best_val_acc,\n        \"best_epoch\": best_epoch,\n        \"test_accuracy\": test_acc,\n        \"num_parameters\": sum(p.numel() for p in model.parameters()),\n        \"wandb_run_url\": wandb_run.url,\n        \"epoch_metrics\": epoch_metrics,\n    }\n\n    results_path = results_dir / \"results.json\"\n    with results_path.open(\"w\", encoding=\"utf-8\") as fp:\n        json.dump(results, fp, indent=2)\n\n    # Required by spec: print experiment description then JSON results\n    print(\"\\n==== Experiment Description ====\")\n    print(\n        f\"Method: {run_cfg.method} | Model: {run_cfg.model.name} | Dataset: {run_cfg.dataset.name}\"\n    )\n    print(json.dumps(results))\n\n\nif __name__ == \"__main__\":\n    train()"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "1. `train.py` – the linear learning-rate scheduler expects `training.lr_scheduler.total_steps`, but this field is not present in the DistilBERT run configs. Accessing `sched_cfg.total_steps` will raise an AttributeError and crash all DistilBERT runs.\n2. config/config.yaml – requirement says it must list ALL experiment run_ids, but the file only contains one default run; the other three runs are missing.\n3. Minor: the linear LR scheduler bug also means the code is not immediately executable for every provided run, violating the “Immediate Executability” criterion.\nFix these issues (add total_steps handling or compute it automatically, and enumerate every run_id in config/config.yaml) to make the code production-ready."
}
