
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches: `uv run python -u -m src.main run={run_id} results_dir={path}`

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations

4. **Model Implementation**:
   - Complete model architectures for baseline, proposed, and ablation methods
   - All models implemented from scratch (no placeholders)
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - No missing files from the structure
   - All functionality contained within specified files

6. **Structured Logging**:
   - train.py prints JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py prints JSON-formatted comparison results to stdout
   - main.py redirects subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log`
   - Logs are forwarded to main process stdout/stderr for GitHub Actions capture

7. **Configuration Files**:
   - config/config.yaml contains list of all experiment run_ids
   - NOTE: config/run/{run_id}.yaml files are provided separately in the "Experiment Runs" section below (not in ExperimentCode)
   - The generated code must properly reference these config files via Hydra
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation and Figures** (if WandB not used):
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/`)
   - Figures include legends, annotations, and proper labels
   - Uses `plt.tight_layout()` before saving
   - Consistent result formatting and comparison logic

9. **WandB Integration** (if WandB is used):
   - Proper WandB initialization with entity/project from config
   - Metrics logged to WandB during training
   - WandB run URL printed to stdout
   - Metadata saved to `.research/iteration{experiment_iteration}/wandb_metadata.json`
   - Figures uploaded as WandB artifacts

10. **Immediate Executability**:
    - Code can be run immediately without modifications
    - All imports and dependencies properly specified in pyproject.toml
    - No missing external resources or undefined variables
    - Proper module structure for `uv run python -m src.main` execution

## Detection of Common Issues
Flag the following problems if found:

- **Incomplete Implementation**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns
- **Truncation**: Code sections that appear shortened or simplified inappropriately
- **Missing Functionality**: Expected features not implemented
- **Configuration Issues**: Missing or incomplete run configs
- **Import Errors**: Missing dependencies or incorrect import statements
- **Not Executable**: Code that cannot be run immediately
- **Inconsistent Structure**: Files not matching required structure
- **Logging Issues**: Missing or incorrect JSON logging format

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

# Experimental Design
- Strategy: Comparative analysis of efficient model architectures across vision and language tasks
- Proposed Method: Evaluate MobileNetV2 and DistilBERT variants for efficient deployment
- Evaluation Metrics: ['accuracy', 'f1_score', 'inference_time', 'model_size']

# Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  width_mult: 0.5
  dropout: 0.2
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
  augmentations:
    random_crop: true
    random_flip: horizontal
training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.05
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0005
  lr_scheduler:
    type: cosine
    T_max: 50
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 0.001
      high: 0.1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    momentum:
      type: uniform
      low: 0.7
      high: 0.95
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  input_adapter:
    type: char_cnn
    embedding_dim: 256
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: char
training:
  epochs: 3
  batch_size: 16
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 0.01
  gradient_accumulation_steps: 2
optuna:
  n_trials: 15
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-1

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-CIFAR-10
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
  modality_adapter:
    type: linear_patch_embedding
    patch_size: 4
    sequence_length: 64
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    patch_size:
      type: categorical
      choices: [2, 4, 8]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: sentencepiece
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - run: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\n\n# List of every run id covered in this project\nall_runs:\n  - comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\n  - comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned\n  - comparative-2-DistilBERT-base-66M-CIFAR-10\n  - comparative-2-DistilBERT-base-66M-alpaca-cleaned\n\n# The run id to execute can be overridden via CLI (e.g. run=\u003crun_id\u003e or run=all)\nrun: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\n\n# Default location for results (can be overridden by CLI)\nresults_dir: ./results\n\n# WandB defaults\nwandb:\n  entity: gengaru617\n  project: 251014-test\n  tags: [\"hydra\", \"benchmark\"]\n\n# Any other global defaults can be added here\n\nversion: 1.0\n\n", "evaluate_py": "import json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport hydra\nfrom omegaconf import DictConfig\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n@hydra.main(config_path=\"../../config\", config_name=\"config\", version_base=\"1.3\")\ndef main(cfg: DictConfig):\n    results_dir = Path(cfg.results_dir)\n    run_ids: List[str] = cfg.all_runs\n\n    records: List[Dict] = []\n    for rid in run_ids:\n        res_file = results_dir / rid / \"results.json\"\n        if not res_file.exists():\n            print(f\"Warning: missing results for {rid}\")\n            continue\n        records.append(json.loads(res_file.read_text()))\n\n    if not records:\n        print(\"No result files found \u2013 aborting evaluation.\")\n        return\n\n    df = pd.DataFrame(records)\n    best = df.loc[df[\"best_val_acc\"].idxmax()]\n    summary = {\n        \"best_run\": best[\"run_id\"],\n        \"best_val_acc\": best[\"best_val_acc\"],\n        \"mean_val_acc\": df[\"best_val_acc\"].mean(),\n        \"std_val_acc\": df[\"best_val_acc\"].std(),\n    }\n\n    # Plot comparison\n    plt.figure(figsize=(8, 4))\n    plt.bar(df[\"run_id\"], df[\"best_val_acc\"], color=\"skyblue\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.title(\"Best validation accuracy across runs\")\n    plt.tight_layout()\n\n    fig_path = results_dir / \"comparison.png\"\n    plt.savefig(fig_path)\n\n    # Log to WandB if available\n    if os.getenv(\"WANDB_API_KEY\"):\n        import wandb\n        with wandb.init(project=\"251014-test\", entity=\"gengaru617\", job_type=\"evaluation\") as wb:\n            wb.log(summary)\n            wb.log({\"comparison\": wandb.Image(str(fig_path))})\n            wb.save(str(fig_path))\n            print(f\"Logged evaluation results to WandB: {wb.url}\")\n\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "import json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\n\n\n# --------------------------------------------------\n# Helper for tee-like logging\n# --------------------------------------------------\n\n\ndef _run_subprocess(cmd: List[str], run_dir: Path):\n    run_dir.mkdir(parents=True, exist_ok=True)\n    stdout_path = run_dir / \"stdout.log\"\n    stderr_path = run_dir / \"stderr.log\"\n\n    with stdout_path.open(\"w\") as out_f, stderr_path.open(\"w\") as err_f:\n        process = subprocess.Popen(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            bufsize=1,\n            universal_newlines=True,\n        )\n        # Stream output live while recording\n        for line in process.stdout:  # type: ignore\n            sys.stdout.write(line)\n            out_f.write(line)\n            out_f.flush()\n        for line in process.stderr:  # type: ignore\n            sys.stderr.write(line)\n            err_f.write(line)\n            err_f.flush()\n        process.wait()\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(process.returncode, cmd)\n\n\n@hydra.main(config_path=\"../../config\", config_name=\"config\", version_base=\"1.3\")\ndef main(cfg: DictConfig):\n    orig_cwd = Path(hydra.utils.get_original_cwd())\n    results_dir = Path(cfg.get(\"results_dir\", orig_cwd / \"results\")).absolute()\n\n    run_ids: List[str]\n    if cfg.run == \"all\":\n        run_ids = cfg.all_runs\n    else:\n        run_ids = [cfg.run]\n\n    for run_id in run_ids:\n        print(\"=\" * 80)\n        print(f\"Launching experiment {run_id}\")\n        print(\"=\" * 80)\n        cmd = [\n            sys.executable,\n            \"-u\",\n            \"-m\",\n            \"src.train\",\n            f\"run={run_id}\",\n            f\"results_dir={str(results_dir)}\",\n        ]\n        _run_subprocess(cmd, results_dir / run_id)\n\n    # After all experiments, run evaluation\n    print(\"All experiments finished. Launching evaluation...\")\n    eval_cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.evaluate\",\n        f\"results_dir={str(results_dir)}\",\n    ]\n    _run_subprocess(eval_cmd, results_dir / \"evaluation\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"Model zoo \u2013 baseline, proposed and ablation architectures.\"\"\"\n\nimport math\nfrom typing import Any\n\nimport torch\nimport torch.nn as nn\nfrom transformers import DistilBertModel, DistilBertConfig\n\n\n# -----------------------------\n# MobileNetV2 implementation \u2013 simplified width multiplier support\n# -----------------------------\n\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n        padding = (kernel_size - 1) // 2\n        super().__init__(\n            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n            nn.BatchNorm2d(out_planes),\n            nn.ReLU6(inplace=True),\n        )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super().__init__()\n        self.stride = stride\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = stride == 1 and inp == oup\n\n        layers = []\n        if expand_ratio != 1:\n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n        layers.extend(\n            [\n                ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            ]\n        )\n        self.conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=1000, width_mult=1.0, dropout=0.2):\n        super().__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * max(1.0, width_mult))\n        features = [ConvBNReLU(3, input_channel, stride=2)]\n        # building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*features)\n\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(p=dropout)\n        self.classifier = nn.Linear(self.last_channel, num_classes)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.pool(x)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n\n# -----------------------------\n# Patch embedding adapter for images -\u003e tokens\n# -----------------------------\n\n\nclass LinearPatchEmbedding(nn.Module):\n    def __init__(self, patch_size: int = 4, in_channels: int = 3, embed_dim: int = 768):\n        super().__init__()\n        self.patch_size = patch_size\n        self.embed_dim = embed_dim\n        self.proj = nn.Linear(patch_size * patch_size * in_channels, embed_dim)\n\n    def forward(self, x):\n        # x: (B, C, H, W)\n        B, C, H, W = x.shape\n        assert H % self.patch_size == 0 and W % self.patch_size == 0, \"Image dimensions must be divisible by patch size\"\n        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n        patches = patches.contiguous().view(B, C, -1, self.patch_size, self.patch_size)\n        patches = patches.permute(0, 2, 1, 3, 4).contiguous()  # (B, N, C, p, p)\n        patches = patches.view(B, patches.size(1), -1)  # (B, N, C*p*p)\n        tokens = self.proj(patches)  # (B, N, D)\n        return tokens\n\n\nclass DistilBERTClassifier(nn.Module):\n    def __init__(self, num_classes: int, pretrained_name: str = \"distilbert-base-uncased\", patch_adapter: Any = None):\n        super().__init__()\n        self.patch_adapter = patch_adapter\n        if patch_adapter is not None:\n            config = DistilBertConfig.from_pretrained(pretrained_name)\n            self.bert = DistilBertModel(config)\n        else:\n            self.bert = DistilBertModel.from_pretrained(pretrained_name)\n        hidden_size = self.bert.config.dim\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x, attention_mask=None):\n        if self.patch_adapter is not None:\n            # x expected: images tensor -\u003e tokens via patch adapter\n            x = self.patch_adapter(x)  # (B, N, D)\n            # DistilBERT expects (B, N, D) as embeddings. We\u0027ll cheat by using inputs_embeds.\n            output = self.bert(inputs_embeds=x, attention_mask=attention_mask)\n            pooled = output.last_hidden_state.mean(dim=1)\n        else:\n            # x is token ids\n            output = self.bert(x, attention_mask=attention_mask)\n            pooled = output.last_hidden_state[:, 0]  # CLS token\n        pooled = self.dropout(pooled)\n        return self.classifier(pooled)\n\n\n# -----------------------------\n# Char-level CNN encoder \u2013 for text\n# -----------------------------\n\n\nclass CharCNN(nn.Module):\n    def __init__(self, vocab_size: int, embedding_dim: int = 256, num_classes: int = 2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.conv = nn.Conv1d(embedding_dim, 128, kernel_size=5, padding=2)\n        self.pool = nn.AdaptiveMaxPool1d(1)\n        self.fc = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        # x: (B, L)\n        emb = self.embedding(x).transpose(1, 2)  # (B, E, L)\n        feat = torch.relu(self.conv(emb))\n        pooled = self.pool(feat).squeeze(-1)\n        return self.fc(pooled)\n\n\n# -----------------------------\n# Model factory\n# -----------------------------\n\n\ndef build_model(cfg, num_classes: int):\n    name = cfg.model.name.lower()\n    if \"mobilenet\" in name:\n        model = MobileNetV2(\n            num_classes=num_classes,\n            width_mult=float(cfg.model.width_mult),\n            dropout=cfg.model.dropout,\n        )\n    elif \"distilbert\" in name:\n        if cfg.model.get(\"modality_adapter\", None):\n            adapter_cfg = cfg.model.modality_adapter\n            patch_adapter = LinearPatchEmbedding(\n                patch_size=adapter_cfg.patch_size,\n                embed_dim=cfg.model.get(\"embed_dim\", 768),\n            )\n        else:\n            patch_adapter = None\n        model = DistilBERTClassifier(num_classes=num_classes, patch_adapter=patch_adapter)\n    elif cfg.model.get(\"input_adapter\", {}).get(\"type\") == \"char_cnn\":\n        vocab_size = 100  # matches the tiny vocab in preprocessing\n        model = CharCNN(vocab_size=vocab_size, embedding_dim=cfg.model.input_adapter.embedding_dim, num_classes=num_classes)\n    else:\n        raise ValueError(f\"Unsupported model {name}\")\n    return model\n", "preprocess_py": "\"\"\"Complete preprocessing utilities for all datasets.\"\"\"\n\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Tuple, List\n\nimport torch\nfrom torch.utils.data import random_split, Dataset\nfrom torchvision import transforms, datasets\n\n\nclass AlpacaDataset(Dataset):\n    \"\"\"Dataset for the alpaca-cleaned instruction following set.\n\n    Assumes JSON Lines, each line: {\"text\": str, \"label\": int}\n    \"\"\"\n\n    def __init__(self, file_path: str, tokenizer: str = \"char\", max_seq_length: int = 512):\n        self.samples = [json.loads(line) for line in Path(file_path).read_text().splitlines()]\n        self.tokenizer_type = tokenizer\n        self.max_len = max_seq_length\n        self.vocab = {c: i + 1 for i, c in enumerate(\n            \"abcdefghijklmnopqrstuvwxyz0123456789-,.;!? \\n\"  # a tiny vocab\n        )}\n\n    def __len__(self):\n        return len(self.samples)\n\n    def _char_tokenize(self, text: str) -\u003e List[int]:\n        return [self.vocab.get(c, 0) for c in text.lower()[: self.max_len]]\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        text, label = sample[\"text\"], sample[\"label\"]\n        tokens = self._char_tokenize(text)\n        # Pad / truncate\n        padded = tokens + [0] * (self.max_len - len(tokens))\n        return torch.tensor(padded, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n\n\ndef get_datasets(cfg) -\u003e Tuple[Dataset, Dataset, int]:\n    \"\"\"Factory returning (train_set, val_set, num_classes).\"\"\"\n    name = cfg.dataset.name.lower()\n    if name == \"cifar-10\":\n        transform_train = [transforms.ToTensor(), transforms.Normalize(cfg.dataset.normalization.mean, cfg.dataset.normalization.std)]\n        transform_val = [transforms.ToTensor(), transforms.Normalize(cfg.dataset.normalization.mean, cfg.dataset.normalization.std)]\n        if cfg.dataset.augmentations.random_crop:\n            transform_train.insert(0, transforms.RandomCrop(cfg.dataset.image_size, padding=4))\n        if cfg.dataset.augmentations.random_flip == \"horizontal\":\n            transform_train.insert(0, transforms.RandomHorizontalFlip())\n\n        train_data = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transforms.Compose(transform_train))\n        test_data = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transforms.Compose(transform_val))\n        val_size = cfg.dataset.splits.val\n        train_size = len(train_data) - val_size\n        train_set, val_set = random_split(train_data, [train_size, val_size])\n        num_classes = 10\n    elif name == \"alpaca-cleaned\":\n        data_path = os.getenv(\"ALPACA_PATH\", \"./data/alpaca-cleaned.jsonl\")\n        full_set = AlpacaDataset(\n            data_path,\n            tokenizer=cfg.dataset.tokenizer,\n            max_seq_length=cfg.dataset.max_seq_length,\n        )\n        val_ratio = 0.1\n        val_size = int(len(full_set) * val_ratio)\n        train_size = len(full_set) - val_size\n        train_set, val_set = random_split(full_set, [train_size, val_size])\n        num_classes = max(s[\"label\"] for s in full_set.samples) + 1  # type: ignore\n    else:\n        raise ValueError(f\"Unsupported dataset {name}\")\n\n    return train_set, val_set, num_classes\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[tool.poetry]\nname = \"research-experiment\"\nversion = \"0.1.0\"\ndescription = \"Hydra-driven research experiments\"\nlicense = \"MIT\"\n\n[tool.poetry.dependencies]\npython = \"\u003e=3.9,\u003c3.11\"\ntorch = \"*\"\ntorchvision = \"*\"\nhydra-core = \"^1.3\"\noptuna = \"*\"\ntransformers = \"*\"\nwandb = \"*\"\npandas = \"*\"\nmatplotlib = \"*\"\n\n[tool.poetry.scripts]\nrun-experiment = \"src.main:main\"\n", "train_py": "import json\nimport math\nimport os\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, Tuple, Any\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# Local imports \u2013 Hydra automatically adds the project root to PYTHONPATH\nfrom src.preprocess import get_datasets\nfrom src.model import build_model\n\n# -----------------------------\n# Helper utilities\n# -----------------------------\n\ndef _init_wandb(cfg: DictConfig) -\u003e Any:\n    \"\"\"Initialise Weights \u0026 Biases run and persist metadata.\"\"\"\n    import wandb  # heavy import \u2013 do it lazily\n\n    entity = cfg.get(\"wandb\", {}).get(\"entity\", \"gengaru617\")\n    project = cfg.get(\"wandb\", {}).get(\"project\", \"251014-test\")\n    run_name = cfg.get(\"wandb\", {}).get(\"run_name\", cfg.run_id)\n    tags = cfg.get(\"wandb\", {}).get(\"tags\", [cfg.method, cfg.model.name])\n\n    wandb_run = wandb.init(\n        entity=entity,\n        project=project,\n        name=run_name,\n        tags=tags,\n        config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n        reinit=True,\n    )\n\n    # Persist metadata for CI\n    iteration = os.getenv(\"EXPERIMENT_ITERATION\", \"0\")\n    meta_dir = Path(f\".research/iteration{iteration}\")\n    meta_dir.mkdir(parents=True, exist_ok=True)\n    metadata_path = meta_dir / \"wandb_metadata.json\"\n    metadata_path.write_text(\n        json.dumps(\n            {\n                \"wandb_entity\": entity,\n                \"wandb_project\": project,\n                \"wandb_run_id\": wandb_run.id,\n            },\n            indent=2,\n        )\n    )\n    print(f\"WandB run initialised at {wandb_run.url}\")\n    return wandb_run\n\n\ndef _build_optimizer(cfg: DictConfig, model: torch.nn.Module):\n    \"\"\"Factory for optimizer and scheduler.\"\"\"\n    params = [p for p in model.parameters() if p.requires_grad]\n    opt_type = cfg.training.optimizer.lower()\n    if opt_type == \"sgd\":\n        optimizer = optim.SGD(\n            params,\n            lr=cfg.training.learning_rate,\n            momentum=cfg.training.get(\"momentum\", 0.0),\n            weight_decay=cfg.training.get(\"weight_decay\", 0.0),\n        )\n    elif opt_type in (\"adam\", \"adamw\"):\n        adam_class = optim.AdamW if opt_type == \"adamw\" else optim.Adam\n        optimizer = adam_class(\n            params,\n            lr=cfg.training.learning_rate,\n            weight_decay=cfg.training.get(\"weight_decay\", 0.0),\n        )\n    else:\n        raise ValueError(f\"Unsupported optimizer {opt_type}\")\n\n    # Scheduler\n    sched_cfg = cfg.training.get(\"lr_scheduler\", None)\n    scheduler = None\n    if sched_cfg:\n        sched_type = sched_cfg.type.lower()\n        if sched_type == \"cosine\":\n            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n                optimizer, T_max=sched_cfg.T_max\n            )\n        elif sched_type == \"linear\":\n            warmup_steps = int(sched_cfg.get(\"warmup_steps\", 0))\n            # Auto-compute total_steps when not provided\n            total_steps = sched_cfg.get(\"total_steps\", None)\n            if total_steps is None:\n                # total_steps = epochs * iterations_per_epoch\n                # We defer the actual value computation; use Lambda LR updating per step\n                total_steps = 0  # placeholder updated later\n            def lr_lambda(current_step: int):\n                if current_step \u003c warmup_steps:\n                    return float(current_step) / float(max(1, warmup_steps))\n                progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n                return max(0.0, 1.0 - progress)\n            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n        else:\n            raise ValueError(f\"Unsupported scheduler type {sched_type}\")\n    return optimizer, scheduler\n\n\ndef _accuracy(output: torch.Tensor, target: torch.Tensor) -\u003e torch.Tensor:\n    with torch.no_grad():\n        preds = output.argmax(dim=1)\n        return (preds == target).float().mean()\n\n\ndef _save_results(run_dir: Path, results: Dict):\n    run_dir.mkdir(parents=True, exist_ok=True)\n    (run_dir / \"results.json\").write_text(json.dumps(results, indent=2))\n\n\n# -----------------------------\n# Training loop\n# -----------------------------\n\ndef run_training(cfg: DictConfig):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Datasets \u0026 loaders\n    train_set, val_set, num_classes = get_datasets(cfg)\n    g_accum = cfg.training.get(\"gradient_accumulation_steps\", 1)\n\n    train_loader = DataLoader(\n        train_set,\n        batch_size=cfg.training.batch_size,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=torch.cuda.is_available(),\n    )\n    val_loader = DataLoader(\n        val_set,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=torch.cuda.is_available(),\n    )\n\n    # Model\n    model = build_model(cfg, num_classes=num_classes).to(device)\n\n    # Criterion\n    if cfg.dataset.name.lower().startswith(\"cifar\"):\n        criterion = nn.CrossEntropyLoss()\n    else:\n        criterion = nn.CrossEntropyLoss()\n\n    # Optim / sched\n    optimizer, scheduler = _build_optimizer(cfg, model)\n\n    # Fix total_steps for linear scheduler if needed\n    if scheduler and isinstance(scheduler, optim.lr_scheduler.LambdaLR):\n        total_steps = cfg.training.epochs * math.ceil(len(train_loader.dataset) / cfg.training.batch_size / g_accum)\n        scheduler.lr_lambdas[0].__defaults__ = (  # type: ignore\n            scheduler.lr_lambdas[0].__defaults__[0],  # warmup_steps remains\n            total_steps,\n        )\n\n    # WandB\n    wb = _init_wandb(cfg)\n\n    global_step = 0\n    best_val_acc = 0.0\n    history = []\n\n    for epoch in range(1, cfg.training.epochs + 1):\n        model.train()\n        epoch_loss = 0.0\n        epoch_acc = 0.0\n        for i, (x, y) in enumerate(train_loader, start=1):\n            x, y = x.to(device), y.to(device)\n            outputs = model(x)\n            loss = criterion(outputs, y) / g_accum\n            loss.backward()\n\n            if i % g_accum == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n                if scheduler:\n                    scheduler.step()\n                global_step += 1\n\n            acc = _accuracy(outputs, y).item()\n            epoch_loss += loss.item() * g_accum\n            epoch_acc += acc\n            if i % 10 == 0:\n                wb.log({\"train/loss\": loss.item() * g_accum, \"train/acc\": acc, \"global_step\": global_step})\n\n        epoch_loss /= len(train_loader)\n        epoch_acc /= len(train_loader)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        val_acc = 0.0\n        with torch.no_grad():\n            for x, y in val_loader:\n                x, y = x.to(device), y.to(device)\n                outputs = model(x)\n                loss = criterion(outputs, y)\n                val_loss += loss.item()\n                val_acc += _accuracy(outputs, y).item()\n        val_loss /= len(val_loader)\n        val_acc /= len(val_loader)\n        best_val_acc = max(best_val_acc, val_acc)\n\n        wb.log(\n            {\n                \"epoch\": epoch,\n                \"train/epoch_loss\": epoch_loss,\n                \"train/epoch_acc\": epoch_acc,\n                \"val/loss\": val_loss,\n                \"val/acc\": val_acc,\n            }\n        )\n\n        history.append(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": epoch_loss,\n                \"train_acc\": epoch_acc,\n                \"val_loss\": val_loss,\n                \"val_acc\": val_acc,\n            }\n        )\n\n        # Checkpoint \u2013 light to save memory\n        ckpt_path = Path(cfg.results_dir) / cfg.run_id / f\"epoch{epoch}.pt\"\n        torch.save({\"model_state\": model.state_dict(), \"epoch\": epoch}, ckpt_path)\n        wb.save(str(ckpt_path))\n\n    wb.finish()\n\n    results = {\n        \"run_id\": cfg.run_id,\n        \"best_val_acc\": best_val_acc,\n        \"history\": history,\n        \"timestamp\": datetime.utcnow().isoformat(),\n    }\n\n    _save_results(Path(cfg.results_dir) / cfg.run_id, results)\n    print(json.dumps(results))\n\n\n# -----------------------------\n# Hydra main\n# -----------------------------\n\n@hydra.main(config_path=\"../../config\", config_name=\"config\", version_base=\"1.3\")\ndef main(cfg: DictConfig):\n    # The working dir is changed by Hydra: recover original path for outputs\n    orig_cwd = Path(hydra.utils.get_original_cwd())\n    cfg.results_dir = os.path.abspath(cfg.get(\"results_dir\", os.path.join(orig_cwd, \"results\")))\n    run_training(cfg)\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "Hydra configuration path is incorrect in every @hydra.main decorator (train.py, evaluate.py, main.py). They use config_path=\"../../config\" which resolves two directories above src, i.e. /config, so Hydra will not find the provided config directory that lives at <repo-root>/config. This makes the entire code base non-executable with the required CLI `uv run python -u -m src.main run=<run_id> results_dir=<path>`. The config_path should be \"../config\" (one level up from src) or \"config\" with proper package search path. Until this is fixed the experiments cannot be launched, violating Immediate Executability and Hydra Integration requirements."
}
