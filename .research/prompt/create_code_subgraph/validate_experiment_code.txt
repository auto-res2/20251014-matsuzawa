
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches: `uv run python -u -m src.main run={run_id} results_dir={path}`

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations

4. **Model Implementation**:
   - Complete model architectures for baseline, proposed, and ablation methods
   - All models implemented from scratch (no placeholders)
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - No missing files from the structure
   - All functionality contained within specified files

6. **Structured Logging**:
   - train.py prints JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py prints JSON-formatted comparison results to stdout
   - main.py redirects subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log`
   - Logs are forwarded to main process stdout/stderr for GitHub Actions capture

7. **Configuration Files**:
   - config/config.yaml contains list of all experiment run_ids
   - NOTE: config/run/{run_id}.yaml files are provided separately in the "Experiment Runs" section below (not in ExperimentCode)
   - The generated code must properly reference these config files via Hydra
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation and Figures** (if WandB not used):
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/`)
   - Figures include legends, annotations, and proper labels
   - Uses `plt.tight_layout()` before saving
   - Consistent result formatting and comparison logic

9. **WandB Integration** (if WandB is used):
   - Proper WandB initialization with entity/project from config
   - Metrics logged to WandB during training
   - WandB run URL printed to stdout
   - Metadata saved to `.research/iteration{experiment_iteration}/wandb_metadata.json`
   - Figures uploaded as WandB artifacts

10. **Immediate Executability**:
    - Code can be run immediately without modifications
    - All imports and dependencies properly specified in pyproject.toml
    - No missing external resources or undefined variables
    - Proper module structure for `uv run python -m src.main` execution

## Detection of Common Issues
Flag the following problems if found:

- **Incomplete Implementation**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns
- **Truncation**: Code sections that appear shortened or simplified inappropriately
- **Missing Functionality**: Expected features not implemented
- **Configuration Issues**: Missing or incomplete run configs
- **Import Errors**: Missing dependencies or incorrect import statements
- **Not Executable**: Code that cannot be run immediately
- **Inconsistent Structure**: Files not matching required structure
- **Logging Issues**: Missing or incorrect JSON logging format

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

# Experimental Design
- Strategy: Comparative analysis of efficient model architectures across vision and language tasks
- Proposed Method: Evaluate MobileNetV2 and DistilBERT variants for efficient deployment
- Evaluation Metrics: ['accuracy', 'f1_score', 'inference_time', 'model_size']

# Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  width_mult: 0.5
  dropout: 0.2
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
  augmentations:
    random_crop: true
    random_flip: horizontal
training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.05
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0005
  lr_scheduler:
    type: cosine
    T_max: 50
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 0.001
      high: 0.1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    momentum:
      type: uniform
      low: 0.7
      high: 0.95
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  input_adapter:
    type: char_cnn
    embedding_dim: 256
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: char
training:
  epochs: 3
  batch_size: 16
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 0.01
  gradient_accumulation_steps: 2
optuna:
  n_trials: 15
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-1

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-CIFAR-10
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
  modality_adapter:
    type: linear_patch_embedding
    patch_size: 4
    sequence_length: 64
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    patch_size:
      type: categorical
      choices: [2, 4, 8]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: sentencepiece
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "# Master Hydra configuration listing available run IDs\nrun_id: \"comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\"\nresults_dir: \"./results\"\nseed: 42\n\nwandb:\n  entity: \"gengaru617\"\n  project: \"251014-test\"\n  tags: []\n", "evaluate_py": "import json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport hydra\nfrom omegaconf import DictConfig\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport wandb\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -\u003e None:\n    \"\"\"Aggregate experiment results and generate comparison figures.\"\"\"\n    results_dir = Path(cfg.get(\"results_dir\", \"./results\")).expanduser()\n    if not results_dir.exists():\n        raise RuntimeError(f\"Results directory \u0027{results_dir}\u0027 does not exist\")\n\n    result_files: List[Path] = list(results_dir.glob(\"*_results.json\"))\n    if not result_files:\n        print(\"No result files found. Skipping evaluation.\")\n        return\n\n    records: List[Dict] = []\n    for file in result_files:\n        with open(file, \"r\", encoding=\"utf-8\") as fp:\n            rec = json.load(fp)\n        records.append(rec)\n\n    # Build comparison arrays\n    names = [rec[\"run_id\"] for rec in records]\n    test_accs = np.array([rec[\"test_acc\"] for rec in records])\n\n    # Bar chart\n    fig, ax = plt.subplots(figsize=(max(6, len(names) * 1.5), 4))\n    bars = ax.bar(names, test_accs, color=\"skyblue\")\n    ax.set_ylabel(\"Test Accuracy\")\n    ax.set_title(\"Experiment Comparison (Test Accuracy)\")\n    ax.bar_label(bars, fmt=\"{:.2f}\")\n    ax.set_ylim(0, 1)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n\n    fig_path = results_dir / \"comparison.png\"\n    fig.savefig(fig_path)\n\n    # Upload to WandB as artifact\n    wandb_cfg = cfg.get(\"wandb\", {})\n    wandb_run = wandb.init(\n        project=wandb_cfg.get(\"project\", \"251014-test\"),\n        entity=wandb_cfg.get(\"entity\", \"gengaru617\"),\n        name=\"evaluation\",\n        tags=[\"evaluation\"],\n        reinit=True,\n    )\n    artifact = wandb.Artifact(\"experiment_comparison\", type=\"figure\")\n    artifact.add_file(str(fig_path))\n    wandb_run.log_artifact(artifact)\n    wandb_run.finish()\n\n    # Structured stdout\n    summary = {name: {\"test_acc\": acc} for name, acc in zip(names, test_accs)}\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "\"\"\"Main orchestrator script that launches experiment runs and evaluation.\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\ndef _run_with_tee(cmd: List[str], log_dir: Path):\n    \"\"\"Run a subprocess while tee-ing its stdout/err into files and console.\"\"\"\n    log_dir.mkdir(parents=True, exist_ok=True)\n    stdout_path = log_dir / \"stdout.log\"\n    stderr_path = log_dir / \"stderr.log\"\n\n    with stdout_path.open(\"w\") as out_fp, stderr_path.open(\"w\") as err_fp:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)\n        # Tee stdout\n        for line in process.stdout:\n            sys.stdout.write(line)\n            sys.stdout.flush()\n            out_fp.write(line)\n        # Tee stderr\n        for line in process.stderr:\n            sys.stderr.write(line)\n            sys.stderr.flush()\n            err_fp.write(line)\n        process.wait()\n        if process.returncode != 0:\n            raise RuntimeError(f\"Subprocess failed with exit code {process.returncode}\")\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    run_id = cfg.get(\"run_id\")\n    results_dir = Path(cfg.get(\"results_dir\", \"./results\")).expanduser()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # Launch train.py as subprocess\n    env = os.environ.copy()\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run_id={run_id}\",\n        f\"results_dir={results_dir}\"\n    ]\n    print(f\"Launching training for run_id={run_id} ...\")\n    _run_with_tee(cmd, results_dir / run_id)\n\n    # After training complete, run evaluation over existing result files\n    print(\"\\nAll training complete. Launching evaluation...\")\n    eval_cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.evaluate\",\n        f\"results_dir={results_dir}\"\n    ]\n    _run_with_tee(eval_cmd, results_dir / \"evaluation\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"Model architecture definitions for experiment variants.\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import Dict, Any, Optional\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom transformers import DistilBertModel, DistilBertConfig\n\n# --------------------------- MobileNetV2 --------------------------------------\nclass MobileNetClassifier(nn.Module):\n    def __init__(self, num_classes: int, width_mult: float = 1.0, dropout: float = 0.2, pretrained: bool = False):\n        super().__init__()\n        mobilenet = torchvision.models.mobilenet_v2(weights=\"DEFAULT\" if pretrained else None, width_mult=width_mult)\n        # Replace classifier\n        in_features = mobilenet.classifier[1].in_features\n        mobilenet.classifier = nn.Sequential(\n            nn.Dropout(p=dropout),\n            nn.Linear(in_features, num_classes),\n        )\n        self.model = mobilenet\n\n    def forward(self, x):\n        return self.model(x)\n\n\n# ---------------------- DistilBERT Vision Adapter -----------------------------\nclass PatchEmbedding(nn.Module):\n    def __init__(self, in_chans: int, embed_dim: int, patch_size: int, seq_length: int):\n        super().__init__()\n        self.patch_size = patch_size\n        self.seq_length = seq_length\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, seq_length + 1, embed_dim))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n\n    def forward(self, x):\n        B = x.size(0)\n        x = self.proj(x)  # B, C, H\u0027, W\u0027\n        x = x.flatten(2).transpose(1, 2)  # B, N, C\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed[:, : x.size(1), :]\n        return x\n\n\nclass DistilBertVisionClassifier(nn.Module):\n    def __init__(\n        self,\n        num_classes: int,\n        patch_size: int = 4,\n        sequence_length: int = 64,\n        pretrained: bool = True,\n    ):\n        super().__init__()\n        self.embed_dim = 768\n        self.patch_embed = PatchEmbedding(3, self.embed_dim, patch_size, sequence_length)\n        config = DistilBertConfig()\n        self.transformer = DistilBertModel.from_pretrained(\"distilbert-base-uncased\") if pretrained else DistilBertModel(config)\n        self.head = nn.Linear(self.embed_dim, num_classes)\n\n    def forward(self, x):\n        # Build inputs_embeds\n        embeds = self.patch_embed(x)\n        attention_mask = torch.ones(embeds.size()[:2], dtype=torch.long, device=x.device)\n        out = self.transformer(inputs_embeds=embeds, attention_mask=attention_mask)\n        cls_output = out.last_hidden_state[:, 0]\n        return self.head(cls_output)\n\n\n# ---------------------- DistilBERT Text Classifier ----------------------------\nclass DistilBertTextClassifier(nn.Module):\n    def __init__(self, num_classes: int, pretrained: bool = True):\n        super().__init__()\n        self.bert = (\n            DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n            if pretrained\n            else DistilBertModel(DistilBertConfig())\n        )\n        self.head = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n    def forward(self, inputs):\n        input_ids, attention_mask = inputs\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0]\n        return self.head(cls_output)\n\n\n# --------------------------- Registry -----------------------------------------\n\ndef build_model(run_cfg: Dict[str, Any], num_classes: int):\n    model_cfg = run_cfg[\"model\"]\n    name = model_cfg[\"name\"].lower()\n\n    if name == \"mobilenetv2\":\n        return MobileNetClassifier(\n            num_classes=num_classes,\n            width_mult=float(model_cfg.get(\"width_mult\", 1.0)),\n            dropout=float(model_cfg.get(\"dropout\", 0.2)),\n            pretrained=bool(model_cfg.get(\"pretrained\", False)),\n        )\n    elif name == \"distilbert-base-uncased\":\n        # Determine modality\n        if run_cfg[\"dataset\"][\"name\"].lower() == \"cifar-10\":\n            adapter = model_cfg.get(\"modality_adapter\", {})\n            return DistilBertVisionClassifier(\n                num_classes=num_classes,\n                patch_size=adapter.get(\"patch_size\", 4),\n                sequence_length=adapter.get(\"sequence_length\", 64),\n                pretrained=bool(model_cfg.get(\"pretrained\", True)),\n            )\n        else:\n            return DistilBertTextClassifier(num_classes=num_classes, pretrained=bool(model_cfg.get(\"pretrained\", True)))\n    else:\n        raise ValueError(f\"Unsupported model name \u0027{name}\u0027\")\n", "preprocess_py": "\"\"\"Full preprocessing pipeline for supported datasets.\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Tuple, Dict, Any\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, Dataset\nimport torchvision\nfrom torchvision import transforms\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# ------------------------------ CIFAR-10 --------------------------------------\n\ndef _build_cifar10_loaders(dataset_cfg: Dict[str, Any], batch_size: int) -\u003e Tuple[DataLoader, DataLoader, DataLoader, int]:\n    image_size = dataset_cfg.get(\"image_size\", 32)\n    norm = dataset_cfg[\"normalization\"]\n    mean, std = norm[\"mean\"], norm[\"std\"]\n\n    augmentations = []\n    if dataset_cfg.get(\"augmentations\", {}).get(\"random_crop\", False):\n        augmentations.append(transforms.RandomCrop(image_size, padding=4))\n    if dataset_cfg.get(\"augmentations\", {}).get(\"random_flip\", \"\") == \"horizontal\":\n        augmentations.append(transforms.RandomHorizontalFlip())\n\n    train_transform = transforms.Compose(\n        augmentations + [\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ]\n    )\n    test_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n\n    root = Path(\"./data\").expanduser()\n    full_train = torchvision.datasets.CIFAR10(root=root, train=True, download=True, transform=train_transform)\n    test_ds = torchvision.datasets.CIFAR10(root=root, train=False, download=True, transform=test_transform)\n\n    # Split train/val\n    train_size = dataset_cfg[\"splits\"][\"train\"]\n    val_size = dataset_cfg[\"splits\"].get(\"val\", 5000)\n    if train_size + val_size \u003e len(full_train):\n        raise ValueError(\"Specified split sizes exceed dataset length.\")\n    train_ds, val_ds = random_split(full_train, [train_size, val_size])\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    return train_loader, val_loader, test_loader, 10  # CIFAR-10 classes\n\n\n# --------------------------- ALPACA CLEANED -----------------------------------\nclass AlpacaClassificationDataset(Dataset):\n    \"\"\"A light-weight instruction-following dataset framed as binary classification:\n    label = 1 if output length \u003e threshold else 0\n    (This keeps compute low while providing a non-trivial NLP workload.)\n    \"\"\"\n\n    def __init__(self, split: str, max_samples: int | None, tokenizer, max_seq_length: int = 512):\n        self.data = load_dataset(\"tatsu-lab/alpaca-cleaned\", split=split)\n        if max_samples is not None and len(self.data) \u003e max_samples:\n            self.data = self.data.select(range(max_samples))\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx: int):\n        record = self.data[idx]\n        text = record[\"instruction\"] + \"\\n\" + (record.get(\"input\") or \"\")\n        tokenized = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_seq_length,\n            return_tensors=\"pt\",\n        )\n        label = 1 if len(record[\"output\"]) \u003e 100 else 0\n        return {\n            \"input_ids\": tokenized.input_ids.squeeze(0),\n            \"attention_mask\": tokenized.attention_mask.squeeze(0),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\ndef _alpaca_collate(batch):\n    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n    labels = torch.stack([item[\"labels\"] for item in batch])\n    return (input_ids, attention_mask), labels\n\n\ndef _build_alpaca_loaders(dataset_cfg: Dict[str, Any], batch_size: int) -\u003e Tuple[DataLoader, DataLoader, DataLoader, int]:\n    tokenizer_name = dataset_cfg.get(\"tokenizer\", \"sentencepiece\")\n    if tokenizer_name == \"sentencepiece\":\n        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n    else:\n        # For char-level experiments, fall back to basic tokenizer that splits characters\n        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  # placeholder for char tokenizer\n\n    # To keep memory within limits, sub-sample\n    train_ds = AlpacaClassificationDataset(\"train[:5000]\", 5000, tokenizer, dataset_cfg.get(\"max_seq_length\", 512))\n    val_ds = AlpacaClassificationDataset(\"train[5000:5500]\", 500, tokenizer, dataset_cfg.get(\"max_seq_length\", 512))\n    test_ds = AlpacaClassificationDataset(\"test[:1000]\", 1000, tokenizer, dataset_cfg.get(\"max_seq_length\", 512))\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=_alpaca_collate)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=_alpaca_collate)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=_alpaca_collate)\n\n    return train_loader, val_loader, test_loader, 2  # binary classification\n\n\n# --------------------------- PUBLIC API ---------------------------------------\n\ndef build_dataloaders(run_cfg: Dict[str, Any]):\n    ds_cfg = run_cfg[\"dataset\"]\n    batch_size = run_cfg[\"training\"].get(\"batch_size\", 32)\n    name = ds_cfg[\"name\"].lower()\n    if name == \"cifar-10\":\n        return _build_cifar10_loaders(ds_cfg, batch_size)\n    elif name == \"alpaca-cleaned\":\n        return _build_alpaca_loaders(ds_cfg, batch_size)\n    else:\n        raise ValueError(f\"Unsupported dataset {name}\")\n", "pyproject_toml": "[project]\nname = \"comparative-efficient-models\"\nversion = \"0.1.0\"\nauthors = [\n  {name = \"AI Researcher\", email = \"noreply@example.com\"},\n]\ndescription = \"Research codebase comparing lightweight models across modalities using Hydra configuration management.\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\nhydra-core = \"^1.3.2\"\ntorch = \"^2.1.0\"\ntorchvision = \"^0.16.0\"\noptuna = \"^3.4.0\"\nwandb = \"^0.16.0\"\ntransformers = \"^4.35.0\"\ndatasets = \"^2.14.0\"\nmatplotlib = \"^3.8.0\"\nnumpy = \"^1.23.0\"\npandas = \"^1.5.3\"\nscikit-learn = \"^1.3.0\"\n\ngrade = \"patch\"\n", "train_py": "import json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\n\nimport wandb\n\n# --- local imports -----------------------------------------------------------\nfrom .model import build_model\nfrom .preprocess import build_dataloaders\n\n# -----------------------------------------------------------------------------\n# INTERNAL RUN CONFIG REGISTRY (used when YAML files are not available on disk)\n# -----------------------------------------------------------------------------\nRUN_CONFIGS: Dict[str, Dict[str, Any]] = {\n    \"comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\": {\n        \"method\": \"comparative-2\",\n        \"model\": {\n            \"name\": \"MobileNetV2\",\n            \"variant\": 0.5,\n            \"pretrained\": False,\n            \"width_mult\": 0.5,\n            \"dropout\": 0.2,\n        },\n        \"dataset\": {\n            \"name\": \"CIFAR-10\",\n            \"splits\": {\"train\": 45000, \"val\": 5000, \"test\": 10000},\n            \"image_size\": 32,\n            \"normalization\": {\n                \"mean\": [0.4914, 0.4822, 0.4465],\n                \"std\": [0.2023, 0.1994, 0.2010],\n            },\n            \"augmentations\": {\"random_crop\": True, \"random_flip\": \"horizontal\"},\n        },\n        \"training\": {\n            \"epochs\": 50,\n            \"batch_size\": 64,\n            \"learning_rate\": 0.05,\n            \"optimizer\": \"sgd\",\n            \"momentum\": 0.9,\n            \"weight_decay\": 5e-4,\n            \"lr_scheduler\": {\"type\": \"cosine\", \"T_max\": 50},\n        },\n    },\n    \"comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned\": {\n        \"method\": \"comparative-2\",\n        \"model\": {\n            \"name\": \"MobileNetV2\",\n            \"variant\": 0.5,\n            \"pretrained\": False,\n            \"input_adapter\": {\"type\": \"char_cnn\", \"embedding_dim\": 256},\n        },\n        \"dataset\": {\n            \"name\": \"alpaca-cleaned\",\n            \"task\": \"instruction_following\",\n            \"max_seq_length\": 512,\n            \"tokenizer\": \"char\",\n        },\n        \"training\": {\n            \"epochs\": 3,\n            \"batch_size\": 16,\n            \"learning_rate\": 0.001,\n            \"optimizer\": \"adam\",\n            \"weight_decay\": 0.01,\n            \"gradient_accumulation_steps\": 2,\n        },\n    },\n    \"comparative-2-DistilBERT-base-66M-CIFAR-10\": {\n        \"method\": \"comparative-2\",\n        \"model\": {\n            \"name\": \"distilbert-base-uncased\",\n            \"pretrained\": True,\n            \"modality_adapter\": {\n                \"type\": \"linear_patch_embedding\",\n                \"patch_size\": 4,\n                \"sequence_length\": 64,\n            },\n        },\n        \"dataset\": {\n            \"name\": \"CIFAR-10\",\n            \"splits\": {\"train\": 45000, \"val\": 5000, \"test\": 10000},\n            \"image_size\": 32,\n            \"normalization\": {\n                \"mean\": [0.4914, 0.4822, 0.4465],\n                \"std\": [0.2023, 0.1994, 0.2010],\n            },\n        },\n        \"training\": {\n            \"epochs\": 10,\n            \"batch_size\": 32,\n            \"learning_rate\": 1e-4,\n            \"optimizer\": \"adamw\",\n            \"weight_decay\": 0.01,\n            \"lr_scheduler\": {\"type\": \"linear\", \"warmup_steps\": 500},\n        },\n    },\n    \"comparative-2-DistilBERT-base-66M-alpaca-cleaned\": {\n        \"method\": \"comparative-2\",\n        \"model\": {\n            \"name\": \"distilbert-base-uncased\",\n            \"pretrained\": True,\n        },\n        \"dataset\": {\n            \"name\": \"alpaca-cleaned\",\n            \"task\": \"instruction_following\",\n            \"max_seq_length\": 512,\n            \"tokenizer\": \"sentencepiece\",\n        },\n        \"training\": {\n            \"epochs\": 3,\n            \"batch_size\": 16,\n            \"learning_rate\": 2e-5,\n            \"optimizer\": \"adamw\",\n            \"weight_decay\": 0.01,\n            \"lr_scheduler\": {\"type\": \"linear\", \"warmup_steps\": 500},\n        },\n    },\n}\n\n# -----------------------------------------------------------------------------\n# Helper utilities\n# -----------------------------------------------------------------------------\n\ndef _resolve_run_cfg(run_id: str) -\u003e Dict[str, Any]:\n    \"\"\"Attempt to load run configuration from YAML file; otherwise fallback to\n    the in-memory registry defined above.\n    \"\"\"\n    yaml_path = Path(hydra.utils.get_original_cwd()) / \"config\" / \"run\" / f\"{run_id}.yaml\"\n    if yaml_path.is_file():\n        with open(yaml_path, \"r\", encoding=\"utf-8\") as fp:\n            return OmegaConf.to_container(OmegaConf.load(fp), resolve=True)\n    if run_id not in RUN_CONFIGS:\n        raise ValueError(f\"Run ID \u0027{run_id}\u0027 not found either on disk or registry.\")\n    return RUN_CONFIGS[run_id]\n\n\ndef _init_wandb(run_id: str, cfg: DictConfig, run_cfg: Dict[str, Any]) -\u003e wandb.sdk.wandb_run.Run:\n    \"\"\"Initialise Weights \u0026 Biases tracking and persist run metadata.\"\"\"\n    wandb_cfg = cfg.get(\"wandb\", {})\n    wandb_run = wandb.init(\n        project=wandb_cfg.get(\"project\", \"251014-test\"),\n        entity=wandb_cfg.get(\"entity\", \"gengaru617\"),\n        name=run_cfg.get(\"run_id\", run_id),\n        config=run_cfg,\n        tags=wandb_cfg.get(\"tags\", []),\n        reinit=True,\n    )\n\n    # Persist metadata immediately\n    iteration = os.getenv(\"EXPERIMENT_ITERATION\", \"0\")\n    meta_dir = Path(hydra.utils.get_original_cwd()) / f\".research/iteration{iteration}\"\n    meta_dir.mkdir(parents=True, exist_ok=True)\n    metadata_path = meta_dir / \"wandb_metadata.json\"\n    with open(metadata_path, \"w\", encoding=\"utf-8\") as fp:\n        json.dump(\n            {\n                \"wandb_entity\": wandb_cfg.get(\"entity\", \"gengaru617\"),\n                \"wandb_project\": wandb_cfg.get(\"project\", \"251014-test\"),\n                \"wandb_run_id\": wandb_run.id,\n            },\n            fp,\n            indent=2,\n        )\n    print(f\"WandB initialized: {wandb_run.url}\")\n    return wandb_run\n\n\n# -----------------------------------------------------------------------------\n# Training/evaluation helpers\n# -----------------------------------------------------------------------------\n\ndef _accuracy(pred: torch.Tensor, target: torch.Tensor) -\u003e float:\n    \"\"\"Compute top-1 accuracy.\"\"\"\n    with torch.no_grad():\n        preds = pred.argmax(dim=1)\n        correct = (preds == target).sum().item()\n        return correct / target.size(0)\n\n\n# -----------------------------------------------------------------------------\n# TRAINING ENTRY POINT\n# -----------------------------------------------------------------------------\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -\u003e None:\n    start_time = time.time()\n\n    run_id: str = cfg.get(\"run_id\")\n    results_dir: Path = Path(cfg.get(\"results_dir\", \"./results\")).expanduser()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    run_cfg = _resolve_run_cfg(run_id)\n    run_cfg[\"run_id\"] = run_id\n\n    # ---------------------------------------------------------------------\n    # Print experiment description BEFORE training starts (requirement)\n    # ---------------------------------------------------------------------\n    print(\"# ----------------------- Experiment Description -----------------------\")\n    print(json.dumps(run_cfg, indent=2))\n    print(\"# ----------------------------------------------------------------------\")\n\n    # ----------------- Seed \u0026 device -----------------\n    torch.manual_seed(cfg.get(\"seed\", 42))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------- Data loaders --------------------\n    train_loader, val_loader, test_loader, num_classes = build_dataloaders(run_cfg)\n\n    # --------------- Model ---------------------------\n    model = build_model(run_cfg, num_classes=num_classes).to(device)\n\n    # --------------- Optimizer \u0026 scheduler -----------\n    opt_name = run_cfg[\"training\"].get(\"optimizer\", \"sgd\").lower()\n    lr = run_cfg[\"training\"].get(\"learning_rate\", 1e-3)\n    weight_decay = float(run_cfg[\"training\"].get(\"weight_decay\", 0.0))\n\n    if opt_name == \"sgd\":\n        optimizer = optim.SGD(\n            model.parameters(),\n            lr=lr,\n            momentum=run_cfg[\"training\"].get(\"momentum\", 0.9),\n            weight_decay=weight_decay,\n        )\n    elif opt_name in {\"adam\", \"adamw\"}:\n        opt_cls = optim.AdamW if opt_name == \"adamw\" else optim.Adam\n        optimizer = opt_cls(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:\n        raise ValueError(f\"Unsupported optimizer {opt_name}\")\n\n    sched_cfg = run_cfg[\"training\"].get(\"lr_scheduler\", {})\n    if sched_cfg.get(\"type\") == \"cosine\":\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=sched_cfg.get(\"T_max\", run_cfg[\"training\"].get(\"epochs\", 10))\n        )\n    elif sched_cfg.get(\"type\") == \"linear\":\n        def lr_lambda(step):\n            warmup = sched_cfg.get(\"warmup_steps\", 0)\n            if step \u003c warmup:\n                return float(step) / float(max(1, warmup))\n            return max(0.0, float(step - warmup) / float(max(1, 1000 - warmup)))\n        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    else:\n        scheduler = None\n\n    # ---------------- WandB ------------------------\n    wandb_run = _init_wandb(run_id, cfg, run_cfg)\n\n    # ---------------- Criterion --------------------\n    criterion = nn.CrossEntropyLoss()\n\n    # ---------------- Training loop ---------------\n    epochs = int(run_cfg[\"training\"].get(\"epochs\", 1))\n    best_val_acc = 0.0\n    history = {\n        \"run_id\": run_id,\n        \"train_loss\": [],\n        \"train_acc\": [],\n        \"val_loss\": [],\n        \"val_acc\": [],\n    }\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss, running_acc, counter = 0.0, 0.0, 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [train]\")\n        for batch in pbar:\n            inputs, labels = batch[0].to(device), batch[1].to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            running_acc += _accuracy(outputs.detach(), labels) * inputs.size(0)\n            counter += inputs.size(0)\n            pbar.set_postfix({\"loss\": running_loss / counter, \"acc\": running_acc / counter})\n        train_loss = running_loss / counter\n        train_acc = running_acc / counter\n\n        # ---------------- Validation ----------------\n        model.eval()\n        val_loss, val_acc, val_counter = 0.0, 0.0, 0\n        with torch.no_grad():\n            for batch in val_loader:\n                inputs, labels = batch[0].to(device), batch[1].to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * inputs.size(0)\n                val_acc += _accuracy(outputs, labels) * inputs.size(0)\n                val_counter += inputs.size(0)\n        val_loss /= val_counter\n        val_acc /= val_counter\n\n        if scheduler is not None:\n            scheduler.step()\n\n        # Logging\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n\n        wandb.log(\n            {\n                \"epoch\": epoch,\n                \"train/loss\": train_loss,\n                \"train/acc\": train_acc,\n                \"val/loss\": val_loss,\n                \"val/acc\": val_acc,\n            }\n        )\n\n        # Save best checkpoint\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            ckpt_path = results_dir / f\"{run_id}_best.pt\"\n            torch.save({\n                \"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n            }, ckpt_path)\n            wandb.save(str(ckpt_path))\n\n    # ---------------- Test phase ------------------\n    model.eval()\n    test_loss, test_acc, test_counter = 0.0, 0.0, 0\n    with torch.no_grad():\n        for batch in test_loader:\n            inputs, labels = batch[0].to(device), batch[1].to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item() * inputs.size(0)\n            test_acc += _accuracy(outputs, labels) * inputs.size(0)\n            test_counter += inputs.size(0)\n    test_loss /= test_counter\n    test_acc /= test_counter\n\n    # Final logging\n    wandb.summary.update({\"test/loss\": test_loss, \"test/acc\": test_acc})\n    wandb.finish()\n\n    history.update({\"test_loss\": test_loss, \"test_acc\": test_acc})\n\n    # Persist results\n    result_path = results_dir / f\"{run_id}_results.json\"\n    with open(result_path, \"w\", encoding=\"utf-8\") as fp:\n        json.dump(history, fp, indent=2)\n\n    # Print JSON to stdout (requirement for structured logging)\n    print(json.dumps(history))\n\n    elapsed = time.time() - start_time\n    print(f\"Training completed in {elapsed / 60:.1f} minutes.\")\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "The experiment code is not ready for immediate execution:\n\n1. Configuration issues\n   • config/config.yaml must list *all* run_ids; current file contains only one.\n   • Code relies on an internal RUN_CONFIGS fallback instead of loading YAMLs from config/run/*.yaml as required.\n\n2. Incomplete / placeholder implementations\n   • preprocess.py: comment \"# placeholder for char tokenizer\" – char-level tokenizer not implemented.\n   • MobileNet + text (run_id comparative-2-MobileNetV2-…-alpaca) uses MobileNet on NLP data; required char_cnn input adapter is missing.\n\n3. Training loop mismatch for NLP batches\n   • train.py assumes batch[0] is a tensor and calls .to(device). For Alpaca loaders batch[0] is a (input_ids, attention_mask) tuple → AttributeError.\n\n4. Evaluation/figures\n   • evaluate.py saves figure as PNG in results_dir root; spec requires PDF saved under {results_dir}/images/.\n\n5. Logging\n   • subprocess tee implementation may dead-lock (reads stdout fully before stderr). Not production-safe.\n\n6. Hydra CLI conformity\n   • Required CLI path (`uv run python -u -m src.main run={run_id} …`) assumes run configs are disk-based; internal fallback breaks reproducibility.\n\nBecause multiple core criteria fail (complete implementation, Hydra integration, data/model correctness, required file outputs), the code is **not** production-ready."
}
