
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches: `uv run python -u -m src.main run={run_id} results_dir={path}`

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations

4. **Model Implementation**:
   - Complete model architectures for baseline, proposed, and ablation methods
   - All models implemented from scratch (no placeholders)
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - No missing files from the structure
   - All functionality contained within specified files

6. **Structured Logging**:
   - train.py prints JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py prints JSON-formatted comparison results to stdout
   - main.py redirects subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log`
   - Logs are forwarded to main process stdout/stderr for GitHub Actions capture

7. **Configuration Files**:
   - config/config.yaml contains list of all experiment run_ids
   - NOTE: config/run/{run_id}.yaml files are provided separately in the "Experiment Runs" section below (not in ExperimentCode)
   - The generated code must properly reference these config files via Hydra
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation and Figures** (if WandB not used):
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/`)
   - Figures include legends, annotations, and proper labels
   - Uses `plt.tight_layout()` before saving
   - Consistent result formatting and comparison logic

9. **WandB Integration** (if WandB is used):
   - Proper WandB initialization with entity/project from config
   - Metrics logged to WandB during training
   - WandB run URL printed to stdout
   - Metadata saved to `.research/iteration{experiment_iteration}/wandb_metadata.json`
   - Figures uploaded as WandB artifacts

10. **Immediate Executability**:
    - Code can be run immediately without modifications
    - All imports and dependencies properly specified in pyproject.toml
    - No missing external resources or undefined variables
    - Proper module structure for `uv run python -m src.main` execution

## Detection of Common Issues
Flag the following problems if found:

- **Incomplete Implementation**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns
- **Truncation**: Code sections that appear shortened or simplified inappropriately
- **Missing Functionality**: Expected features not implemented
- **Configuration Issues**: Missing or incomplete run configs
- **Import Errors**: Missing dependencies or incorrect import statements
- **Not Executable**: Code that cannot be run immediately
- **Inconsistent Structure**: Files not matching required structure
- **Logging Issues**: Missing or incorrect JSON logging format

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

# Experimental Design
- Strategy: Comparative analysis of efficient model architectures across vision and language tasks
- Proposed Method: Evaluate MobileNetV2 and DistilBERT variants for efficient deployment
- Evaluation Metrics: ['accuracy', 'f1_score', 'inference_time', 'model_size']

# Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  width_mult: 0.5
  dropout: 0.2
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
  augmentations:
    random_crop: true
    random_flip: horizontal
training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.05
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0005
  lr_scheduler:
    type: cosine
    T_max: 50
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 0.001
      high: 0.1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    momentum:
      type: uniform
      low: 0.7
      high: 0.95
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  input_adapter:
    type: char_cnn
    embedding_dim: 256
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: char
training:
  epochs: 3
  batch_size: 16
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 0.01
  gradient_accumulation_steps: 2
optuna:
  n_trials: 15
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-1

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-CIFAR-10
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
  modality_adapter:
    type: linear_patch_embedding
    patch_size: 4
    sequence_length: 64
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    patch_size:
      type: categorical
      choices: [2, 4, 8]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: sentencepiece
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "# config/config.yaml\n# Global Hydra configuration.  Only two command-line overrides are expected:\n#    1. `run`          \u2013 identifier of the experiment to run (must match YAML in config/run)\n#    2. `results_dir`  \u2013 output directory path provided by CI or user\n\nrun: \"\"\nresults_dir: \"results\"\n", "evaluate_py": "\"\"\"src/evaluate.py\nAggregates the ``results.json`` files produced by individual experiment runs,\nprints a machine-readable summary **and** uploads a comparison figure to WandB.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport hydra\nimport matplotlib.pyplot as plt\nimport wandb\nfrom omegaconf import DictConfig\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -\u003e None:  # pragma: no cover \u2013 entry-point\n    results_dir = Path(cfg.results_dir)\n    if not results_dir.exists():\n        raise FileNotFoundError(results_dir)\n\n    summaries: List[Dict[str, Any]] = []\n    for res_file in results_dir.rglob(\"results.json\"):\n        with open(res_file) as fp:\n            summaries.append(json.load(fp))\n\n    if not summaries:\n        print(\"{}\")\n        return\n\n    # Build bar-chart of test metrics\n    run_ids = [s[\"run_id\"] for s in summaries]\n    metrics = [s[\"test_metric\"] for s in summaries]\n\n    fig, ax = plt.subplots(figsize=(max(6, len(run_ids)), 4))\n    bars = ax.bar(run_ids, metrics, color=\"skyblue\")\n    ax.set_xticklabels(run_ids, rotation=45, ha=\"right\")\n    ax.set_ylabel(\"Test metric (accuracy \u2191 / perplexity \u2193)\")\n    ax.set_title(\"Experimental comparison across all runs\")\n\n    for bar, metric in zip(bars, metrics):\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width() / 2, height, f\"{metric:.2f}\", ha=\"center\", va=\"bottom\")\n\n    fig.tight_layout()\n    fig_path = results_dir / \"comparison.png\"\n    fig.savefig(fig_path)\n\n    # Upload to WandB as artefact if in a WandB run context\n    if wandb.run is not None:\n        artefact = wandb.Artifact(name=\"comparison-figure\", type=\"figure\")\n        artefact.add_file(str(fig_path))\n        wandb.run.log_artifact(artefact)\n\n    # Console output -----------------------------------------------------\n    output = {\n        \"num_runs\": len(summaries),\n        \"runs\": {s[\"run_id\"]: s[\"test_metric\"] for s in summaries},\n    }\n    print(json.dumps(output))\n\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "\"\"\"src/main.py\nOrchestrator that launches *train.py* as a dedicated subprocess, pipes all\nstdout/stderr into per-run log files **and** the console, and finally invokes\n``evaluate.py`` for aggregated analysis.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import TextIO\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\ndef tee(stream: TextIO, *targets: TextIO):  # helper to replicate `tee(1)`\n    for line in iter(stream.readline, \"\"):\n        for tgt in targets:\n            tgt.write(line)\n            tgt.flush()\n    stream.close()\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -\u003e None:  # pragma: no cover \u2013 entry-point\n    run_id = cfg.run\n    results_dir = Path(cfg.results_dir).absolute()\n\n    run_out_dir = results_dir / run_id\n    run_out_dir.mkdir(parents=True, exist_ok=True)\n\n    stdout_log = open(run_out_dir / \"stdout.log\", \"w\")\n    stderr_log = open(run_out_dir / \"stderr.log\", \"w\")\n\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={run_id}\",\n        f\"results_dir={results_dir}\",\n    ]\n    print(\"Executing:\", \" \".join(cmd))\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n\n    # Launch tee threads for stdout/stderr\n    threads = [\n        threading.Thread(target=tee, args=(proc.stdout, sys.stdout, stdout_log)),\n        threading.Thread(target=tee, args=(proc.stderr, sys.stderr, stderr_log)),\n    ]\n    for t in threads:\n        t.start()\n\n    proc.wait()\n    for t in threads:\n        t.join()\n    stdout_log.close()\n    stderr_log.close()\n\n    if proc.returncode != 0:\n        raise subprocess.CalledProcessError(proc.returncode, cmd)\n\n    # Optionally run evaluator ---------------------------------------------------\n    eval_cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.evaluate\",\n        f\"results_dir={results_dir}\",\n    ]\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"src/model.py\nAll *model* building blocks live in this file.  The public factory function\n``create_model`` inspects the run configuration and returns a ready-to-train\n``torch.nn.Module`` instance.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as tvm\nfrom transformers import DistilBertConfig, DistilBertModel\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Character-level language-model (baseline for *alpaca-cleaned*)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nclass CharRNNLM(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int = 256, hidden_dim: int = 512, num_layers: int = 2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, x: torch.Tensor) -\u003e torch.Tensor:  # x: [B, T]\n        emb = self.embed(x)\n        out, _ = self.rnn(emb)\n        logits = self.fc(out)\n        return logits  # [B, T, V]\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# MobileNetV2 (image classification)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nclass MobileNetV2Classifier(nn.Module):\n    def __init__(self, num_classes: int, width_mult: float = 1.0, dropout: float = 0.2, pretrained: bool = False):\n        super().__init__()\n        mobilenet = tvm.mobilenet_v2(pretrained=pretrained, width_mult=width_mult)\n        in_features = mobilenet.classifier[1].in_features\n        features = list(mobilenet.classifier.children())[:-1]  # drop original head\n        self.features = nn.Sequential(mobilenet.features, nn.Dropout(dropout))\n        self.classifier = nn.Linear(in_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -\u003e torch.Tensor:\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        return self.classifier(x)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Image \u2192 patch \u2192 DistilBERT classifier (*vision-language bridging*)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size: int = 32, patch_size: int = 4, in_chans: int = 3, embed_dim: int = 768):\n        super().__init__()\n        assert img_size % patch_size == 0, \"Img size must be divisible by patch size\"\n        self.num_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x: torch.Tensor) -\u003e torch.Tensor:  # x: [B, C, H, W]\n        x = self.proj(x)  # [B, E, H\u0027, W\u0027]\n        x = x.flatten(2).transpose(1, 2)  # [B, N, E]\n        return x\n\n\nclass DistilBertPatchClassifier(nn.Module):\n    def __init__(\n        self,\n        num_classes: int,\n        patch_size: int = 4,\n        img_size: int = 32,\n        sequence_length: int = 64,\n        pretrained: bool = True,\n    ):\n        super().__init__()\n        embed_dim = 768\n        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n        bert_cfg = DistilBertConfig()\n        self.bert = DistilBertModel(bert_cfg) if not pretrained else DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.cls_head = nn.Linear(embed_dim, num_classes)\n        # Positional embedding so that seq_len matches\n        self.pos_embed = nn.Parameter(torch.zeros(1, sequence_length, embed_dim))\n\n    def forward(self, x: torch.Tensor) -\u003e torch.Tensor:\n        tokens = self.patch_embed(x)\n        # Resize / interpolate positional embedding if needed\n        if tokens.size(1) != self.pos_embed.size(1):\n            pos = nn.functional.interpolate(self.pos_embed.permute(0, 2, 1), size=tokens.size(1), mode=\"nearest\")\n            pos = pos.permute(0, 2, 1)\n        else:\n            pos = self.pos_embed\n        out = self.bert(inputs_embeds=tokens + pos).last_hidden_state[:, 0]  # CLS token = first token embedding\n        return self.cls_head(out)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Public model factory helper\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef create_model(cfg_run, *, num_classes: Optional[int], vocab_size: Optional[int]):\n    model_cfg = cfg_run.model\n\n    # ------------------------------------------------------------------\n    # Vision branch (MobileNetV2)\n    # ------------------------------------------------------------------\n    if model_cfg.name.lower().startswith(\"mobilenetv2\"):\n        if cfg_run.dataset.name == \"CIFAR-10\":\n            assert num_classes is not None\n            return MobileNetV2Classifier(\n                num_classes=num_classes,\n                width_mult=model_cfg.width_mult,\n                dropout=model_cfg.dropout,\n                pretrained=model_cfg.pretrained,\n            )\n        else:  # text domain \u2192 fall back to simple CharRNNLM\n            assert vocab_size is not None, \"Text task requires vocab size\"\n            adapter_cfg = model_cfg.input_adapter\n            return CharRNNLM(vocab_size, embed_dim=adapter_cfg.embedding_dim)\n\n    # ------------------------------------------------------------------\n    # DistilBERT variants\n    # ------------------------------------------------------------------\n    if model_cfg.name.lower().startswith(\"distilbert\"):\n        if cfg_run.dataset.name == \"alpaca-cleaned\":\n            # Standard DistilBERT LM head fine-tuning for text\n            assert vocab_size is not None\n            bert_cfg = DistilBertConfig(vocab_size=vocab_size)\n            model = torch.hub.load(\"huggingface/pytorch-transformers\", \"modelForMaskedLM\", \"distilbert-base-uncased\")\n            # Resize embeddings in case vocab differs\n            model.resize_token_embeddings(vocab_size)\n            return model\n        else:  # CIFAR-10 via patch embedding\n            assert num_classes is not None\n            return DistilBertPatchClassifier(\n                num_classes=num_classes,\n                patch_size=model_cfg.modality_adapter.patch_size,\n                sequence_length=model_cfg.modality_adapter.sequence_length,\n                pretrained=model_cfg.pretrained,\n            )\n\n    raise ValueError(f\"Unsupported model configuration: {model_cfg}\")\n", "preprocess_py": "\"\"\"src/preprocess.py\nUniversal data-pipeline factory.  Returns domain-specific *DataLoaders* plus an\noptional *tokenizer* for language tasks.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport random\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision.datasets import CIFAR10\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Tokeniser for character-level language modelling\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nclass CharTokenizer:\n    def __init__(self) -\u003e None:\n        # Printable ASCII plus a handful of specials\n        ascii_chars = [chr(i) for i in range(32, 127)]\n        self.pad_token = \"\u003cpad\u003e\"\n        self.bos_token = \"\u003cbos\u003e\"\n        self.eos_token = \"\u003ceos\u003e\"\n        self.idx2char = [self.pad_token, self.bos_token, self.eos_token] + ascii_chars\n        self.char2idx = {ch: idx for idx, ch in enumerate(self.idx2char)}\n        self.pad_id = self.char2idx[self.pad_token]\n        self.bos_id = self.char2idx[self.bos_token]\n        self.eos_id = self.char2idx[self.eos_token]\n\n    def encode(self, text: str, add_special_tokens: bool = True) -\u003e list[int]:\n        ids = [self.char2idx.get(ch, self.eos_id) for ch in text]\n        if add_special_tokens:\n            ids = [self.bos_id] + ids + [self.eos_id]\n        return ids\n\n    def decode(self, ids: list[int], skip_special_tokens: bool = True) -\u003e str:\n        chars = []\n        for idx in ids:\n            ch = self.idx2char[idx]\n            if skip_special_tokens and ch in {self.pad_token, self.bos_token, self.eos_token}:\n                continue\n            chars.append(ch)\n        return \"\".join(chars)\n\n    @property\n    def vocab_size(self) -\u003e int:\n        return len(self.idx2char)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Language-modelling dataset (Alpaca-cleaned)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nclass AlpacaCharDataset(Dataset):\n    def __init__(self, file_path: str | Path, tokenizer: CharTokenizer, max_seq_len: int = 512):\n        self.samples = []\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n        # ------------------------------------------------------------------\n        # Expected file format: one *JSON* object per line with fields\n        #   \"instruction\", \"input\", \"output\"      (same as the original dataset)\n        # ------------------------------------------------------------------\n        with open(file_path) as fp:\n            for line in fp:\n                obj = json.loads(line)\n                prompt = obj.get(\"instruction\", \"\") + \"\\n\" + obj.get(\"input\", \"\")\n                target = obj.get(\"output\", \"\")\n                # We train a *causal* LM: feed prompt+bos, predict full target+eos\n                src_ids = tokenizer.encode(prompt)\n                tgt_ids = tokenizer.encode(target)\n                ids = (src_ids + tgt_ids)[: max_seq_len]\n                self.samples.append(ids)\n\n    def __len__(self) -\u003e int:\n        return len(self.samples)\n\n    def __getitem__(self, idx: int):\n        ids = self.samples[idx]\n        inp = ids[:-1]\n        tgt = ids[1:]\n        return {\n            \"input_ids\": torch.tensor(inp, dtype=torch.long),\n            \"target_ids\": torch.tensor(tgt, dtype=torch.long),\n        }\n\n    @property\n    def num_classes(self):  # not a classification task\n        return None\n\n\ndef alpaca_collate(batch):\n    pad_id = CharTokenizer().pad_id  # instantiate temporary to get pad id\n    max_len = max(item[\"input_ids\"].size(0) for item in batch)\n    inp_tensor = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    tgt_tensor = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    for i, item in enumerate(batch):\n        n = item[\"input_ids\"].size(0)\n        inp_tensor[i, :n] = item[\"input_ids\"]\n        tgt_tensor[i, :n] = item[\"target_ids\"]\n    return {\"inputs\": inp_tensor, \"targets\": tgt_tensor}\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Computer-vision (CIFAR-10) helpers\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef build_cifar_transforms(cfg_dataset) -\u003e Tuple[T.Compose, T.Compose]:\n    mean = cfg_dataset.normalization.mean\n    std = cfg_dataset.normalization.std\n    img_size = cfg_dataset.image_size\n\n    train_tf = [T.RandomCrop(img_size, padding=4)] if cfg_dataset.augmentations.random_crop else []\n    if cfg_dataset.augmentations.random_flip == \"horizontal\":\n        train_tf.append(T.RandomHorizontalFlip())\n    train_tf += [T.ToTensor(), T.Normalize(mean, std)]\n\n    val_tf = [T.ToTensor(), T.Normalize(mean, std)]\n    return T.Compose(train_tf), T.Compose(val_tf)\n\n\ndef get_cifar10_dataloaders(cfg_run) -\u003e Tuple[DataLoader, DataLoader, DataLoader]:\n    dataset_cfg = cfg_run.dataset\n    training_cfg = cfg_run.training\n\n    train_tf, test_tf = build_cifar_transforms(dataset_cfg)\n    data_root = Path(\"data\")\n    train_full = CIFAR10(root=data_root, train=True, download=True, transform=train_tf)\n    test_set = CIFAR10(root=data_root, train=False, download=True, transform=test_tf)\n\n    # Train / val split as specified\n    train_size = dataset_cfg.splits.train\n    val_size = dataset_cfg.splits.val\n    assert train_size + val_size == len(train_full), \"Train/val split does not match dataset length\".\n    train_set, val_set = random_split(train_full, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n\n    dl_kwargs = dict(batch_size=training_cfg.batch_size, num_workers=2, pin_memory=True)\n    train_loader = DataLoader(train_set, shuffle=True, **dl_kwargs, collate_fn=lambda b: {\"inputs\": torch.stack([x[0] for x in b]), \"labels\": torch.tensor([x[1] for x in b])})\n    val_loader = DataLoader(val_set, shuffle=False, **dl_kwargs, collate_fn=lambda b: {\"inputs\": torch.stack([x[0] for x in b]), \"labels\": torch.tensor([x[1] for x in b])})\n    test_loader = DataLoader(test_set, shuffle=False, **dl_kwargs, collate_fn=lambda b: {\"inputs\": torch.stack([x[0] for x in b]), \"labels\": torch.tensor([x[1] for x in b])})\n\n    return train_loader, val_loader, test_loader\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Public factory\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef get_dataloaders(cfg_run):\n    if cfg_run.dataset.name == \"CIFAR-10\":\n        train, val, test = get_cifar10_dataloaders(cfg_run)\n        return train, val, test, None  # no tokenizer\n    elif cfg_run.dataset.name == \"alpaca-cleaned\":\n        tokenizer = CharTokenizer()\n        file_path = cfg_run.dataset.get(\"file_path\", \"data/alpaca-cleaned.json\")\n        ds_full = AlpacaCharDataset(file_path, tokenizer, cfg_run.dataset.max_seq_length)\n        n_total = len(ds_full)\n        train_size = int(0.9 * n_total)\n        val_size = int(0.05 * n_total)\n        test_size = n_total - train_size - val_size\n        train_set, val_set, test_set = random_split(ds_full, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n        dl_kwargs = dict(batch_size=cfg_run.training.batch_size, shuffle=True, num_workers=2, pin_memory=True, collate_fn=alpaca_collate)\n        return (\n            DataLoader(train_set, **dl_kwargs),\n            DataLoader(val_set, **{**dl_kwargs, \"shuffle\": False}),\n            DataLoader(test_set, **{**dl_kwargs, \"shuffle\": False}),\n            tokenizer,\n        )\n    else:\n        raise ValueError(f\"Unsupported dataset: {cfg_run.dataset.name}\")\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=42\", \"wheel\"]\n\n[project]\nname = \"research-experiment\"\nversion = \"0.1.0\"\ndescription = \"Hydra-managed research experiments with WandB tracking\"\nauthors = [\n    {name = \"AI Generated\", email = \"ai@example.com\"}\n]\nrequires-python = \"\u003e=3.8\"\ndependencies = [\n    \"torch\u003e=1.13\",\n    \"torchvision\u003e=0.14\",\n    \"transformers\u003e=4.35\",\n    \"omegaconf\u003e=2.3\",\n    \"hydra-core\u003e=1.3\",\n    \"wandb\u003e=0.15\",\n    \"matplotlib\u003e=3.7\",\n]\n\n", "train_py": "\"\"\"src/train.py\nThis module executes a single experiment run.  All parameters are provided through a\nHydra run-level configuration file located in ``config/run``.  The script\n\n1. loads the run configuration,\n2. builds the complete data pipeline (pre-processing + loaders),\n3. instantiates the requested model,\n4. trains / validates / tests it,\n5. tracks everything with in-depth Weights \u0026 Biases logging, and finally\n6. stores all relevant metrics in ``\u003cresults_dir\u003e/\u003crun_id\u003e/results.json``.\n\nThe script prints a machine-readable JSON line **containing *all* numerical\nresults** as its last line on *stdout* so that automated tooling can reliably\npick it up.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport random\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, Tuple\n\nimport hydra\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport wandb\nfrom hydra.utils import to_absolute_path\nfrom omegaconf import DictConfig, OmegaConf\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Utility helpers\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef set_deterministic(seed: int = 42) -\u003e None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n@torch.no_grad()\ndef evaluate_classification(model: nn.Module, loader: torch.utils.data.DataLoader, device: torch.device) -\u003e float:\n    \"\"\"Return *top-1* accuracy.\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    criterion = nn.CrossEntropyLoss()\n    running_loss = 0.0\n    for batch in loader:\n        inputs, targets = batch[\"inputs\"].to(device), batch[\"labels\"].to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        running_loss += loss.item() * inputs.size(0)\n        preds = outputs.argmax(dim=1)\n        correct += (preds == targets).sum().item()\n        total += targets.size(0)\n    acc = correct / total * 100.0\n    return acc, running_loss / total\n\n\n@torch.no_grad()\ndef evaluate_lm(model: nn.Module, loader: torch.utils.data.DataLoader, device: torch.device, pad_id: int) -\u003e float:\n    \"\"\"Return *perplexity* for language-model\u2010style tasks.\"\"\"\n    model.eval()\n    criterion = nn.CrossEntropyLoss(ignore_index=pad_id, reduction=\"sum\")\n    total_tokens = 0\n    total_loss = 0.0\n    for batch in loader:\n        inputs = batch[\"inputs\"].to(device)\n        targets = batch[\"targets\"].to(device)\n        logits = model(inputs)\n        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n        total_loss += loss.item()\n        total_tokens += (targets != pad_id).sum().item()\n    ppl = np.exp(total_loss / total_tokens)\n    return float(ppl)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Main experiment entry-point\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -\u003e None:  # pragma: no cover \u2013 entry-point\n    \"\"\"Single experiment run controlled via *Hydra*.\n\n    The top-level Hydra config only carries two keys \u2013 ``run`` and\n    ``results_dir``.  Everything else is provided by the dedicated run-level\n    YAML file that we load *manually* from ``config/run``.\n    \"\"\"\n    # ---------------------------------------------------------------------\n    # Locate and parse the run-specific configuration file\n    # ---------------------------------------------------------------------\n    run_id: str = cfg.run\n    base_dir = Path(__file__).resolve().parent.parent\n    run_cfg_file = base_dir / \"config\" / \"run\" / f\"{run_id}.yaml\"\n    if not run_cfg_file.exists():\n        raise FileNotFoundError(f\"Run-configuration file not found: {run_cfg_file}\")\n\n    run_cfg: DictConfig = OmegaConf.load(run_cfg_file)\n    # Merge command-line overrides (e.g. results_dir)\n    run_cfg.results_dir = to_absolute_path(cfg.results_dir)\n\n    # Ensure deterministic behaviour *before* touching anything else\n    seed = run_cfg.get(\"seed\", 42)\n    set_deterministic(seed)\n\n    # ------------------------------------------------------------------\n    # I/O setup\n    # ------------------------------------------------------------------\n    out_dir = Path(run_cfg.results_dir) / run_id\n    out_dir.mkdir(parents=True, exist_ok=True)\n    # Persist a copy of the exact configuration that produced this run\n    with open(out_dir / \"config.yaml\", \"w\") as fp:\n        OmegaConf.save(config=run_cfg, f=fp.name)\n\n    # ------------------------------------------------------------------\n    # Device configuration\n    # ------------------------------------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # Build the complete data pipeline (pre-processing + DLs)\n    # ------------------------------------------------------------------\n    from src.preprocess import get_dataloaders  # local import avoids circularity\n\n    train_loader, val_loader, test_loader, tokenizer = get_dataloaders(run_cfg)\n\n    # ------------------------------------------------------------------\n    # Model instantiation\n    # ------------------------------------------------------------------\n    from src.model import create_model\n\n    num_classes = getattr(train_loader.dataset, \"num_classes\", None)\n    vocab_size = getattr(tokenizer, \"vocab_size\", None)\n\n    model = create_model(run_cfg, num_classes=num_classes, vocab_size=vocab_size)\n    model.to(device)\n\n    # ------------------------------------------------------------------\n    # Optimiser \u0026 LR scheduler\n    # ------------------------------------------------------------------\n    training_cfg = run_cfg.training\n    if training_cfg.optimizer.lower() == \"sgd\":\n        optimizer = optim.SGD(\n            model.parameters(),\n            lr=training_cfg.learning_rate,\n            momentum=training_cfg.get(\"momentum\", 0.0),\n            weight_decay=training_cfg.weight_decay,\n        )\n    elif training_cfg.optimizer.lower() in {\"adam\", \"adamw\"}:\n        optim_class = optim.AdamW if training_cfg.optimizer.lower() == \"adamw\" else optim.Adam\n        optimizer = optim_class(\n            model.parameters(),\n            lr=training_cfg.learning_rate,\n            weight_decay=training_cfg.weight_decay,\n        )\n    else:\n        raise ValueError(f\"Unsupported optimizer {training_cfg.optimizer}\")\n\n    # LR scheduler (optional)\n    scheduler = None\n    if \"lr_scheduler\" in training_cfg and training_cfg.lr_scheduler is not None:\n        sched_cfg = training_cfg.lr_scheduler\n        if sched_cfg.type == \"cosine\":\n            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=sched_cfg.T_max)\n        elif sched_cfg.type == \"linear\":\n            total_steps = len(train_loader) * training_cfg.epochs\n            warmup_steps = sched_cfg.get(\"warmup_steps\", 0)\n            def lr_lambda(current_step: int):\n                if current_step \u003c warmup_steps:\n                    return float(current_step) / float(max(1, warmup_steps))\n                return max(0.0, float(total_steps - current_step) / float(max(1, total_steps - warmup_steps)))\n            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n        else:\n            raise ValueError(f\"Unsupported scheduler type: {sched_cfg.type}\")\n\n    # Criterion is chosen dynamically\n    if num_classes is not None:\n        criterion = nn.CrossEntropyLoss()\n    else:\n        pad_id = tokenizer.pad_id  # type: ignore[attr-defined]\n        criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n\n    # ------------------------------------------------------------------\n    # Weights \u0026 Biases initialisation \u2013 must happen *after* we have the full cfg\n    # ------------------------------------------------------------------\n    wandb_cfg: Dict[str, Any] = run_cfg.get(\"wandb\", {})\n    wandb_entity = wandb_cfg.get(\"entity\", \"gengaru617\")\n    wandb_project = wandb_cfg.get(\"project\", \"251014-test\")\n    wandb_run = wandb.init(\n        project=wandb_project,\n        entity=wandb_entity,\n        name=run_id,\n        config=OmegaConf.to_container(run_cfg, resolve=True),\n        reinit=False,\n    )\n\n    # Persist WandB metadata in the prescribed location\n    experiment_iteration = os.getenv(\"EXPERIMENT_ITERATION\", \"1\")\n    meta_dir = Path(\".research\") / f\"iteration{experiment_iteration}\"\n    meta_dir.mkdir(parents=True, exist_ok=True)\n    with open(meta_dir / \"wandb_metadata.json\", \"w\") as fp:\n        json.dump(\n            {\n                \"wandb_entity\": wandb_entity,\n                \"wandb_project\": wandb_project,\n                \"wandb_run_id\": wandb_run.id,\n            },\n            fp,\n            indent=2,\n        )\n\n    print(f\"WandB URL: {wandb_run.url}\")\n\n    # ------------------------------------------------------------------\n    #                               Training loop\n    # ------------------------------------------------------------------\n    best_val_score = -float(\"inf\") if num_classes is not None else float(\"inf\")\n    history = []  # keeps per-epoch metrics for output\n\n    for epoch in range(1, training_cfg.epochs + 1):\n        model.train()\n        epoch_loss = 0.0\n        epoch_correct = 0\n        epoch_total = 0\n        start_time = time.time()\n\n        for step, batch in enumerate(train_loader, 1):\n            optimizer.zero_grad()\n\n            if num_classes is not None:  # image / classification branch\n                inputs = batch[\"inputs\"].to(device)\n                labels = batch[\"labels\"].to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                epoch_loss += loss.item() * inputs.size(0)\n                preds = outputs.argmax(dim=1)\n                epoch_correct += (preds == labels).sum().item()\n                epoch_total += labels.size(0)\n\n            else:  # language-model branch\n                inputs = batch[\"inputs\"].to(device)\n                targets = batch[\"targets\"].to(device)\n                logits = model(inputs)\n                loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n                loss.backward()\n                optimizer.step()\n\n                valid_tokens = (targets != pad_id).sum().item()\n                epoch_loss += loss.item()\n                epoch_total += valid_tokens\n\n            if scheduler is not None:\n                scheduler.step()\n\n        epoch_time = time.time() - start_time\n        if num_classes is not None:\n            train_acc = epoch_correct / epoch_total * 100.0\n            train_metric = train_acc\n        else:\n            train_ppl = np.exp(epoch_loss / epoch_total)\n            train_metric = train_ppl\n\n        # --------------------------------------------------------------\n        # Validation\n        # --------------------------------------------------------------\n        if num_classes is not None:\n            val_acc, val_loss = evaluate_classification(model, val_loader, device)\n            current_val_metric = val_acc\n        else:\n            val_ppl = evaluate_lm(model, val_loader, device, pad_id)\n            val_loss = None\n            current_val_metric = -val_ppl  # lower is better \u2192 negate\n\n        # Save best model\n        if current_val_metric \u003e best_val_score:\n            best_val_score = current_val_metric\n            torch.save(model.state_dict(), out_dir / \"best_model.pt\")\n\n        # WandB logging -------------------------------------------------\n        log_dict = {\n            \"epoch\": epoch,\n            \"train_metric\": train_metric,\n            \"val_metric\": -current_val_metric if num_classes is None else current_val_metric,\n            \"train_loss\": epoch_loss / epoch_total if num_classes is None else epoch_loss / len(train_loader.dataset),\n            \"epoch_time\": epoch_time,\n        }\n        wandb.log(log_dict, step=epoch)\n        history.append(log_dict)\n\n        print(json.dumps({\"run_id\": run_id, \"epoch\": epoch, **log_dict}))\n        sys.stdout.flush()\n\n    # ------------------------------------------------------------------\n    #                               Test set\n    # ------------------------------------------------------------------\n    if num_classes is not None:\n        test_acc, _ = evaluate_classification(model, test_loader, device)\n        test_metric = test_acc\n    else:\n        test_ppl = evaluate_lm(model, test_loader, device, pad_id)\n        test_metric = test_ppl\n    wandb.log({\"test_metric\": test_metric})\n\n    # Save final artefacts ------------------------------------------------\n    artefact = wandb.Artifact(name=f\"{run_id}-model\", type=\"model\")\n    artefact.add_file(out_dir / \"best_model.pt\")\n    wandb.run.log_artifact(artefact)\n\n    # ------------------------------------------------------------------\n    # Persist *all* numerical results in a structured JSON file\n    # ------------------------------------------------------------------\n    results: Dict[str, Any] = {\n        \"run_id\": run_id,\n        \"best_val_metric\": -best_val_score if num_classes is None else best_val_score,\n        \"test_metric\": test_metric,\n        \"epochs\": training_cfg.epochs,\n        \"seed\": seed,\n        \"device\": str(device),\n        \"history\": history,\n    }\n\n    with open(out_dir / \"results.json\", \"w\") as fp:\n        json.dump(results, fp, indent=2)\n\n    # Final console output \u2013 machine-readable --------------------------------------------------\n    print(json.dumps(results))\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "The package cannot be executed as-is:\n1. preprocess.py contains a syntax error – there is a stray dot after the assert statement (\"…length\".) causing immediate SyntaxError at import time.\n2. config/config.yaml must list all run_ids according to the spec; it only contains an empty string placeholder, so Hydra cannot enumerate experiments (Req. 7).\n3. The Alpaca data pipeline hard-codes the path \"data/alpaca-cleaned.json\" which is not provided; code crashes with FileNotFoundError, violating immediate-executability (Req. 3, 10).\n4. SentencePiece tokenisation requested in run configs is ignored; only a CharTokenizer is implemented, so the DistilBERT-alpaca runs are inconsistent with their configuration (Req. 3/4).\n5. DistilBERT text model is loaded through torch.hub (\"huggingface/pytorch-transformers\"), which is deprecated and may fail offline; proper use of transformers.AutoModel is required (Req. 4, 10).\nBecause of these issues the code is not production-ready yet."
}
