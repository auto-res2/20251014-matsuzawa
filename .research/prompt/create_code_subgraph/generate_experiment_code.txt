
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/run/*.yaml` files
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation

## Hydra Configuration Structure
Each run config file (`config/run/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.

## Command Line Interface
The generated code must support the following CLI:
```bash
# Run a single experiment (specified by Hydra)
uv run python -u -m src.main run={run_id} results_dir={path}
```

The `run` argument specifies which experiment to run (matching a run_id from config/run/*.yaml).
The `results_dir` argument is passed from the GitHub Actions workflow and specifies where all outputs should be saved.

## Output Structure
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
- `src/train.py`: Logic to run a single experiment variation. Uses Hydra config to load parameters. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Complete preprocessing pipeline implementation for the specified datasets
- `src/model.py`: Complete model architecture implementations. It will contain classes for baseline, proposed, and ablation models. Implement all architectures from scratch.
- `src/main.py`: The main orchestrator script. It receives a run_id via Hydra, launches train.py for that specific experiment, manages subprocess, collects and consolidates logs, and finally triggers evaluate.py if needed.
- `pyproject.toml`: Complete project dependencies (include hydra-core, optuna if needed)
- `config/config.yaml`: Main Hydra configuration file that defines the list of all experiment run_ids


### Key Implementation Focus Areas
1. Hydra-Driven Configuration: All parameters loaded from run configs dynamically
2. Algorithm Core: Full implementation of the proposed method with proper abstraction
3. Sequential Execution: main.py executes run variations one at a time in sequential order
4. Configuration Driven: The entire workflow must be driven by the YAML configuration files
5. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete
6. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Code Validation Feedback


**Previous Validation Issue**: Code is not ready for execution. Major blocking issues:
1. Incomplete / incorrect imports:
   • src/evaluate.py uses os.environ but never imports os.
2. NameError in model builder:
   • src/model.py references _CHAR2IDX without importing it from src.preprocess.
3. Log redirection implementation broken:
   • _stream_redirect in src/main.py calls Path.write_text(decoded, append=True) – Path.write_text has no “append” argument, causing a TypeError at runtime. No correct file‐append logic is implemented.
4. WandB artifact logging incorrect:
   • train.py calls wandb_run.log_artifact(str(path), name=…) but log_artifact expects a wandb.Artifact object.
5. CharCNNAdapter shape logic very likely invalid (forces reshape to 3×32×32 regardless of embedding_dim) and will raise a runtime error if embedding_dim ≠ 3072.
6. evaluate.py does not satisfy structured logging requirement fully and lacks run_id field; also figure upload path uses os (missing import).

Because of these errors the code will raise exceptions and cannot be executed immediately, violating “Immediate Executability”, “Complete Implementation”, and “Structured Logging” criteria.
**Action Required**: Address this issue in the implementation.



# Experimental Environment
cpu
RAM：500 MB

# Current Research Method (Target for Experiment Design)
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

# Experimental Design
- Strategy: 

# Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10
  Config File: config/run/comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  width_mult: 0.5
  dropout: 0.2
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
  augmentations:
    random_crop: true
    random_flip: horizontal
training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.05
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0005
  lr_scheduler:
    type: cosine
    T_max: 50
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 0.001
      high: 0.1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    momentum:
      type: uniform
      low: 0.7
      high: 0.95
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned
  Config File: config/run/comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  input_adapter:
    type: char_cnn
    embedding_dim: 256
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: char
training:
  epochs: 3
  batch_size: 16
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 0.01
  gradient_accumulation_steps: 2
optuna:
  n_trials: 15
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-1

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  Config File: config/run/comparative-2-DistilBERT-base-66M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-CIFAR-10
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
  modality_adapter:
    type: linear_patch_embedding
    patch_size: 4
    sequence_length: 64
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    patch_size:
      type: categorical
      choices: [2, 4, 8]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  Config File: config/run/comparative-2-DistilBERT-base-66M-alpaca-cleaned.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: sentencepiece
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  


# External Resources (Use these for implementation)



# ========================================
# WandB Integration Requirements
# ========================================

## Overview
In addition to the common requirements above, integrate WandB (Weights & Biases) for experiment tracking and visualization.

## Additional Requirements
- WANDB INTEGRATION: Initialize WandB with proper project/entity from config, log metrics/artifacts, and save wandb_run_id to metadata
- WANDB METADATA: Save WandB run information to `.research/iteration{experiment_iteration}/wandb_metadata.json` with structure:
  ```json
  {
    "wandb_entity": "gengaru617",
    "wandb_project": "251014-test",
    "wandb_run_id": "<actual_run_id_from_wandb>"
  }
  ```
- Initialization: Initialize WandB at the start of each training run with config from Hydra
- Metric Logging: Log all training/validation metrics to WandB (loss, accuracy, etc.)
- Artifact Tracking: Log model checkpoints and important artifacts
- Metadata Saving: After WandB initialization, save the wandb_run_id to `.research/iteration{experiment_iteration}/wandb_metadata.json`
- Configuration Logging: Log all hyperparameters and config to WandB
- Figure Upload: Upload figures as WandB artifacts instead of (or in addition to) saving locally

## WandB Configuration
- Entity: gengaru617
- Project: 251014-test

## Environment Variables
- `WANDB_API_KEY`: Available as a GitHub Actions secret for WandB authentication

## Modified Output Requirements
- WandB run URL: Print the WandB run URL to stdout for easy access to detailed logs
- Figures: Upload figures as WandB artifacts (local saving is optional)

## Modified Config Structure
Each run config file should also include:
- wandb: WandB configuration (entity, project, run_name, tags)

## Modified Dependencies
- pyproject.toml: Include `wandb` package

## WandB Metadata File Location
The wandb_metadata.json file must be saved to:
- Path: `.research/iteration{experiment_iteration}/wandb_metadata.json`
- The experiment_iteration should be passed as an environment variable or config parameter
- The file should be created immediately after WandB run initialization in train.py



Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "\"\"\"\nTraining script for a single experiment variation.\nThis is executed as a subprocess from main.py so that every run gets its own Hydra job and\nclean logging separation.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport wandb\n\nfrom .preprocess import get_dataloaders\nfrom .model import build_model\n\n\n# --------------------------------------------------------------------------------------\n# Helper utilities\n# --------------------------------------------------------------------------------------\n\ndef _build_optimizer(model: nn.Module, cfg: DictConfig) -> torch.optim.Optimizer:\n    train_cfg = cfg.training\n    params = [p for p in model.parameters() if p.requires_grad]\n    if train_cfg.optimizer.lower() == \"sgd\":\n        return torch.optim.SGD(\n            params,\n            lr=train_cfg.learning_rate,\n            momentum=train_cfg.get(\"momentum\", 0.0),\n            weight_decay=train_cfg.get(\"weight_decay\", 0.0),\n        )\n    elif train_cfg.optimizer.lower() == \"adam\":\n        return torch.optim.Adam(\n            params,\n            lr=train_cfg.learning_rate,\n            weight_decay=train_cfg.get(\"weight_decay\", 0.0),\n        )\n    elif train_cfg.optimizer.lower() == \"adamw\":\n        return torch.optim.AdamW(\n            params,\n            lr=train_cfg.learning_rate,\n            weight_decay=train_cfg.get(\"weight_decay\", 0.0),\n        )\n    else:\n        raise ValueError(f\"Unsupported optimizer {train_cfg.optimizer}\")\n\n\ndef _build_scheduler(optimizer: torch.optim.Optimizer, cfg: DictConfig):\n    sched_cfg = cfg.training.get(\"lr_scheduler\")\n    if sched_cfg is None:\n        return None\n    if sched_cfg.type == \"cosine\":\n        return torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=sched_cfg.T_max, eta_min=0.0\n        )\n    elif sched_cfg.type == \"linear\":\n        def _lr_lambda(current_step: int):\n            warmup = sched_cfg.warmup_steps\n            if current_step < warmup:\n                return float(current_step) / float(max(1, warmup))\n            return max(\n                0.0,\n                float(sched_cfg.total_steps - current_step)\n                / float(max(1, sched_cfg.total_steps - warmup)),\n            )\n\n        return torch.optim.lr_scheduler.LambdaLR(optimizer, _lr_lambda)\n    else:\n        raise ValueError(f\"Unsupported scheduler type {sched_cfg.type}\")\n\n\ndef _save_checkpoint(model: nn.Module, path: Path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    torch.save(model.state_dict(), path)\n\n\n# --------------------------------------------------------------------------------------\n# Main training entry point (Hydra)\n# --------------------------------------------------------------------------------------\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef train(cfg: DictConfig) -> None:  # noqa: C901  (keep single function for Hydra simplicity)\n    # ------------------------------------------------------------------\n    # Extract run specific configuration section\n    # ------------------------------------------------------------------\n    run_cfg = cfg.run\n    run_id: str = run_cfg.run_id\n\n    # ------------------------------------------------------------------\n    # Prepare result directory\n    # ------------------------------------------------------------------\n    results_dir = Path(cfg.results_dir) / run_id\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # Initialize Weights & Biases\n    # ------------------------------------------------------------------\n    wandb_cfg = run_cfg.get(\"wandb\", {})\n    wandb_run = wandb.init(\n        entity=wandb_cfg.get(\"entity\", \"gengaru617\"),\n        project=wandb_cfg.get(\"project\", \"251014-test\"),\n        name=wandb_cfg.get(\"run_name\", run_id),\n        tags=wandb_cfg.get(\"tags\", []),\n        config=OmegaConf.to_container(run_cfg, resolve=True),\n        reinit=True,\n    )\n    # Save metadata for CI tracking -----------------------------------------------------\n    iteration_idx = int(os.environ.get(\"EXPERIMENT_ITERATION\", \"1\"))\n    metadata_path = (\n        Path(\".research\") / f\"iteration{iteration_idx}\" / \"wandb_metadata.json\"\n    )\n    metadata_path.parent.mkdir(parents=True, exist_ok=True)\n    with metadata_path.open(\"w\", encoding=\"utf-8\") as fp:\n        json.dump(\n            {\n                \"wandb_entity\": wandb_run.entity,\n                \"wandb_project\": wandb_run.project,\n                \"wandb_run_id\": wandb_run.id,\n            },\n            fp,\n            indent=2,\n        )\n\n    print(f\"WandB run page: {wandb_run.url}\")  # Required by spec\n\n    # ------------------------------------------------------------------\n    # Data pipeline\n    # ------------------------------------------------------------------\n    dataloaders, dataset_meta = get_dataloaders(run_cfg.dataset, run_cfg.training)\n    train_loader: DataLoader = dataloaders[\"train\"]\n    val_loader: DataLoader = dataloaders[\"val\"]\n    test_loader: DataLoader = dataloaders[\"test\"]\n\n    # ------------------------------------------------------------------\n    # Model + Optimizer + Sched.\n    # ------------------------------------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = build_model(run_cfg, dataset_meta).to(device)\n\n    optimizer = _build_optimizer(model, run_cfg)\n    scheduler = _build_scheduler(optimizer, run_cfg)\n\n    criterion = nn.CrossEntropyLoss()\n\n    best_val_acc = 0.0\n    best_epoch = -1\n    epoch_metrics = []\n\n    num_epochs: int = run_cfg.training.epochs\n    gradient_accum_steps: int = run_cfg.training.get(\"gradient_accumulation_steps\", 1)\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n        total = 0\n        correct = 0\n        optimizer.zero_grad(set_to_none=True)\n\n        train_pbar = tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{num_epochs}\")\n        for step, (inputs, labels) in enumerate(train_pbar):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels) / gradient_accum_steps\n            loss.backward()\n\n            if (step + 1) % gradient_accum_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad(set_to_none=True)\n\n            running_loss += loss.item() * inputs.size(0) * gradient_accum_steps\n            preds = outputs.argmax(dim=1)\n            total += labels.size(0)\n            correct += (preds == labels).sum().item()\n\n        train_loss = running_loss / total\n        train_acc = correct / total\n\n        # ---------------------- Validation -----------------------------\n        model.eval()\n        val_total, val_correct, val_loss_accum = 0, 0, 0.0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss_accum += loss.item() * inputs.size(0)\n                preds = outputs.argmax(dim=1)\n                val_total += labels.size(0)\n                val_correct += (preds == labels).sum().item()\n\n        val_loss = val_loss_accum / val_total\n        val_acc = val_correct / val_total\n\n        # Learning-rate scheduler step ----------------------------------\n        if scheduler is not None:\n            scheduler.step()\n\n        # Log metrics ----------------------------------------------------\n        wandb_run.log(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"train_acc\": train_acc,\n                \"val_loss\": val_loss,\n                \"val_acc\": val_acc,\n                \"lr\": optimizer.param_groups[0][\"lr\"],\n            },\n            step=epoch,\n        )\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_epoch = epoch\n            checkpoint_path = results_dir / \"checkpoints\" / \"best.pt\"\n            _save_checkpoint(model, checkpoint_path)\n\n            # Log checkpoint as artifact --------------------------------\n            artifact = wandb.Artifact(name=f\"{run_id}-model\", type=\"model\")\n            artifact.add_file(str(checkpoint_path))\n            wandb_run.log_artifact(artifact)\n\n        epoch_metrics.append(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"train_acc\": train_acc,\n                \"val_loss\": val_loss,\n                \"val_acc\": val_acc,\n            }\n        )\n\n    # ------------------------------------------------------------------\n    # Test evaluation using best model ----------------------------------\n    # ------------------------------------------------------------------\n    best_ckpt = results_dir / \"checkpoints\" / \"best.pt\"\n    model.load_state_dict(torch.load(best_ckpt, map_location=device))\n\n    model.eval()\n    test_total, test_correct = 0, 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            preds = outputs.argmax(dim=1)\n            test_total += labels.size(0)\n            test_correct += (preds == labels).sum().item()\n\n    test_acc = test_correct / test_total\n\n    wandb_run.summary[\"best_val_acc\"] = best_val_acc\n    wandb_run.summary[\"test_acc\"] = test_acc\n    wandb_run.finish()\n\n    # ------------------------------------------------------------------\n    # Persist & print structured results -------------------------------\n    # ------------------------------------------------------------------\n    results: Dict[str, Any] = {\n        \"run_id\": run_id,\n        \"method\": run_cfg.method,\n        \"best_val_accuracy\": best_val_acc,\n        \"best_epoch\": best_epoch,\n        \"test_accuracy\": test_acc,\n        \"num_parameters\": sum(p.numel() for p in model.parameters()),\n        \"wandb_run_url\": wandb_run.url,\n        \"epoch_metrics\": epoch_metrics,\n    }\n\n    results_path = results_dir / \"results.json\"\n    with results_path.open(\"w\", encoding=\"utf-8\") as fp:\n        json.dump(results, fp, indent=2)\n\n    # Required by spec: print experiment description then JSON results\n    print(\"\\n==== Experiment Description ====\")\n    print(\n        f\"Method: {run_cfg.method} | Model: {run_cfg.model.name} | Dataset: {run_cfg.dataset.name}\"\n    )\n    print(json.dumps(results))\n\n\nif __name__ == \"__main__\":\n    train()",
    "evaluate_py": "\"\"\"\nEvaluation / comparison script.\nAggregates individual result.json files produced by train.py, generates a bar chart\nand logs a W&B artifact so results can be inspected remotely.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict\n\nimport hydra\nfrom omegaconf import DictConfig\nimport matplotlib\n\nmatplotlib.use(\"Agg\")  # Use non-interactive backend for CI\nimport matplotlib.pyplot as plt  # noqa: E402  (after Agg set)\nimport wandb  # noqa: E402\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef evaluate(cfg: DictConfig) -> None:\n    results_dir = Path(cfg.results_dir)\n    assert results_dir.exists(), f\"results_dir {results_dir} not found\"\n\n    comparison: Dict[str, Dict] = {}\n    run_ids = []\n    test_accs = []\n\n    for sub in results_dir.iterdir():\n        if not sub.is_dir():\n            continue\n        result_file = sub / \"results.json\"\n        if not result_file.exists():\n            continue\n        with result_file.open(\"r\", encoding=\"utf-8\") as fp:\n            res = json.load(fp)\n        run_id = res[\"run_id\"]\n        comparison[run_id] = res\n        run_ids.append(run_id)\n        test_accs.append(res.get(\"test_accuracy\", 0.0))\n\n    # ------------------ Plotting ------------------------------------------------------\n    fig, ax = plt.subplots(figsize=(10, 4))\n    ax.bar(run_ids, test_accs)\n    ax.set_ylabel(\"Test Accuracy\")\n    ax.set_title(\"Comparison of Test Accuracy across Runs\")\n    ax.set_xticklabels(run_ids, rotation=45, ha=\"right\")\n    plt.tight_layout()\n\n    fig_path = results_dir / \"comparison.png\"\n    fig.savefig(fig_path, dpi=150)\n\n    # --------------- Log to W&B -------------------------------------------------------\n    wandb_run = wandb.init(\n        project=\"251014-test\",\n        entity=\"gengaru617\",\n        name=\"evaluation\",\n        reinit=True,\n        config={\"evaluated_runs\": run_ids},\n    )\n    artifact = wandb.Artifact(name=\"comparison-figure\", type=\"figure\")\n    artifact.add_file(str(fig_path))\n    wandb_run.log_artifact(artifact)\n    wandb_run.finish()\n\n    # --------------- Structured logging output ---------------------------------------\n    print(json.dumps({\"run_id\": \"evaluation\", \"comparison\": comparison}))\n\n\nif __name__ == \"__main__\":\n    evaluate()",
    "preprocess_py": "\"\"\"\nData-loading and preprocessing utilities for both vision (CIFAR-10) and text (alpaca-cleaned).\nAll heavy-lifting lives here so train.py remains concise.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport random\nfrom pathlib import Path\nfrom typing import Dict, Tuple, List\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, Dataset\nimport torchvision.transforms as T\nfrom torchvision.datasets import CIFAR10\n\n# ----------------------------------------------------------------------------------\n# Character vocabulary (shared across modules)\n# ----------------------------------------------------------------------------------\n_CHARSET = (\n    [\n        \"<pad>\",\n        \"<unk>\",\n    ]\n    + [chr(i) for i in range(32, 127)]  # printable ascii\n)\n_CHAR2IDX: Dict[str, int] = {ch: i for i, ch in enumerate(_CHARSET)}\n_IDX2CHAR: List[str] = _CHARSET\n_PAD_IDX = _CHAR2IDX[\"<pad>\"]\n_UNK_IDX = _CHAR2IDX[\"<unk>\"]\n\n\n# ----------------------------------------------------------------------------------\n# Utility functions for text\n# ----------------------------------------------------------------------------------\n\ndef tokenize_char(text: str, max_length: int) -> List[int]:\n    tokens = [_CHAR2IDX.get(ch, _UNK_IDX) for ch in text[:max_length]]\n    if len(tokens) < max_length:\n        tokens += [_PAD_IDX] * (max_length - len(tokens))\n    return tokens\n\n\n# ----------------------------------------------------------------------------------\n# Alpaca dataset (binary classification based on output length)\n# ----------------------------------------------------------------------------------\n\n\nclass AlpacaDataset(Dataset):\n    def __init__(\n        self,\n        json_path: Path,\n        max_seq_len: int,\n        tokenizer: str = \"char\",\n    ) -> None:\n        if not json_path.exists():\n            print(\n                f\"[WARNING] {json_path} not found. Generating synthetic dataset for debugging.\"\n            )\n            self.samples = self._generate_synthetic(1000)\n        else:\n            with json_path.open(\"r\", encoding=\"utf-8\") as fp:\n                self.samples = [json.loads(l) for l in fp]\n\n        self.max_seq_len = max_seq_len\n        self.tokenizer = tokenizer\n\n    @staticmethod\n    def _generate_synthetic(n: int):\n        samples = []\n        for _ in range(n):\n            length = random.randint(10, 200)\n            text = \"a\" * length\n            samples.append(\n                {\n                    \"instruction\": \"generate text\",\n                    \"input\": \"\",\n                    \"output\": text,\n                }\n            )\n        return samples\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        record = self.samples[idx]\n        # Form text by concatenating instruction, input and output\n        text = (\n            record.get(\"instruction\", \"\")\n            + \" \"\n            + record.get(\"input\", \"\")\n            + \" \"\n            + record.get(\"output\", \"\")\n        ).strip()\n        label = 1 if len(record.get(\"output\", \"\")) > 100 else 0\n        if self.tokenizer == \"char\":\n            input_ids = tokenize_char(text.lower(), self.max_seq_len)\n            input_ids = torch.LongTensor(input_ids)\n        else:\n            raise ValueError(\"Unsupported tokenizer\")\n\n        return input_ids, torch.tensor(label, dtype=torch.long)\n\n\n# ----------------------------------------------------------------------------------\n# Collate fns\n# ----------------------------------------------------------------------------------\n\ndef _collate_text(batch):\n    inputs, labels = zip(*batch)\n    inputs = torch.stack(inputs, dim=0)\n    labels = torch.stack(labels, dim=0)\n    return inputs, labels\n\n\n# ----------------------------------------------------------------------------------\n# High-level API called from train.py\n# ----------------------------------------------------------------------------------\n\ndef get_dataloaders(dataset_cfg, train_cfg) -> Tuple[Dict[str, DataLoader], Dict]:\n    if dataset_cfg.name.lower() in {\"cifar-10\", \"cifar10\"}:\n        return _get_cifar10_loaders(dataset_cfg, train_cfg)\n    elif \"alpaca\" in dataset_cfg.name.lower():\n        return _get_alpaca_loaders(dataset_cfg, train_cfg)\n    else:\n        raise ValueError(f\"Unsupported dataset {dataset_cfg.name}\")\n\n\n# ------------------------------------ CIFAR-10 ------------------------------------\n\ndef _get_cifar10_loaders(dataset_cfg, train_cfg):\n    mean = dataset_cfg.normalization.mean\n    std = dataset_cfg.normalization.std\n\n    train_transform_ops = []\n    aug = dataset_cfg.augmentations\n    if aug.get(\"random_crop\", False):\n        train_transform_ops.append(T.RandomCrop(dataset_cfg.image_size, padding=4))\n    if aug.get(\"random_flip\", \"\") == \"horizontal\":\n        train_transform_ops.append(T.RandomHorizontalFlip())\n    train_transform_ops += [T.ToTensor(), T.Normalize(mean, std)]\n\n    test_transform = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n\n    dataset_root = Path(\"./data\")\n    full_train = CIFAR10(\n        root=dataset_root,\n        train=True,\n        download=True,\n        transform=T.Compose(train_transform_ops),\n    )\n    test_set = CIFAR10(\n        root=dataset_root,\n        train=False,\n        download=True,\n        transform=test_transform,\n    )\n\n    train_len = dataset_cfg.splits.train\n    val_len = dataset_cfg.splits.val\n    assert train_len + val_len <= len(full_train), \"Invalid split sizes for CIFAR-10\"\n    train_set, val_set, _ = random_split(\n        full_train, [train_len, val_len, len(full_train) - train_len - val_len]\n    )\n\n    loader_kwargs = {\n        \"batch_size\": train_cfg.batch_size,\n        \"num_workers\": 2,\n        \"pin_memory\": torch.cuda.is_available(),\n    }\n\n    train_loader = DataLoader(train_set, shuffle=True, **loader_kwargs)\n    val_loader = DataLoader(val_set, shuffle=False, **loader_kwargs)\n    test_loader = DataLoader(test_set, shuffle=False, **loader_kwargs)\n\n    meta = {\n        \"num_classes\": 10,\n        \"input_shape\": (3, dataset_cfg.image_size, dataset_cfg.image_size),\n    }\n    return {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}, meta\n\n\n# ------------------------------------ Alpaca ---------------------------------------\n\ndef _get_alpaca_loaders(dataset_cfg, train_cfg):\n    if dataset_cfg.name == \"alpaca-cleaned\":\n        json_path = Path(dataset_cfg.get(\"path\", \"./data/alpaca_cleaned.json\"))\n    else:\n        raise ValueError(\"Only alpaca-cleaned supported in this template\")\n\n    full_dataset = AlpacaDataset(\n        json_path=json_path,\n        max_seq_len=dataset_cfg.max_seq_length,\n        tokenizer=dataset_cfg.get(\"tokenizer\", \"char\"),\n    )\n\n    total_len = len(full_dataset)\n    val_len = int(0.1 * total_len)\n    train_len = total_len - val_len\n\n    train_set, val_set = random_split(full_dataset, [train_len, val_len])\n\n    loader_kwargs = {\n        \"batch_size\": train_cfg.batch_size,\n        \"num_workers\": 0,\n        \"collate_fn\": _collate_text,\n        \"pin_memory\": torch.cuda.is_available(),\n    }\n\n    train_loader = DataLoader(train_set, shuffle=True, **loader_kwargs)\n    val_loader = DataLoader(val_set, shuffle=False, **loader_kwargs)\n    test_loader = DataLoader(val_set, shuffle=False, **loader_kwargs)  # reuse val as test\n\n    meta = {\n        \"num_classes\": 2,\n        \"vocab_size\": len(_CHARSET),\n        \"pad_idx\": _PAD_IDX,\n        \"input_shape\": (dataset_cfg.max_seq_length,),\n    }\n    return {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}, meta\n",
    "model_py": "\"\"\"\nModel definitions and builders.\nIncludes MobileNetV2, DistilBERT adapters and a simple CharCNN-based classifier.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import Any, Dict\n\nimport torch\nfrom torch import nn\nimport torchvision.models as tvm\nfrom transformers import DistilBertConfig, DistilBertModel\n\nfrom .preprocess import _CHAR2IDX  # Re-use global vocabulary\n\n\n# ----------------------------------------------------------------------------------\n# Character-level CNN encoder (for text)\n# ----------------------------------------------------------------------------------\n\nclass CharCNNEncoder(nn.Module):\n    def __init__(self, vocab_size: int, embedding_dim: int, conv_channels: int = 128):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=_CHAR2IDX[\"<pad>\"])\n        self.conv = nn.Conv1d(embedding_dim, conv_channels, kernel_size=5, padding=2)\n        self.pool = nn.AdaptiveMaxPool1d(1)\n        self.output_dim = conv_channels\n\n    def forward(self, x):  # x: (B, L)\n        emb = self.embedding(x)  # (B, L, E)\n        emb = emb.transpose(1, 2)  # (B, E, L)\n        feat = torch.relu(self.conv(emb))\n        pooled = self.pool(feat).squeeze(-1)  # (B, C)\n        return pooled  # (B, output_dim)\n\n\n# ----------------------------------------------------------------------------------\n# Adapter that turns char features into fake RGB images so MobileNet can process text\n# ----------------------------------------------------------------------------------\n\nclass Char2ImageAdapter(nn.Module):\n    def __init__(self, in_dim: int, img_size: int = 32, out_channels: int = 3):\n        super().__init__()\n        self.img_size = img_size\n        self.out_channels = out_channels\n        self.proj = nn.Linear(in_dim, out_channels * img_size * img_size)\n\n    def forward(self, x):  # x: (B, F)\n        out = self.proj(x)\n        out = out.view(-1, self.out_channels, self.img_size, self.img_size)\n        return out\n\n\n# ----------------------------------------------------------------------------------\n# Patch embedding adapter for feeding images into DistilBERT\n# ----------------------------------------------------------------------------------\n\nclass LinearPatchEmbedding(nn.Module):\n    def __init__(self, patch_size: int, img_size: int, embed_dim: int = 768):\n        super().__init__()\n        self.patch_size = patch_size\n        self.img_size = img_size\n        self.embed_dim = embed_dim\n        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n        self.proj = nn.Linear(patch_size * patch_size * 3, embed_dim)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n\n    def forward(self, x):  # x: (B, 3, H, W)\n        B, C, H, W = x.shape\n        p = self.patch_size\n        assert H == self.img_size and W == self.img_size\n        # unfold -> (B, C*p*p, N)\n        patches = x.unfold(2, p, p).unfold(3, p, p)\n        patches = patches.contiguous().view(B, C, -1, p, p)\n        patches = patches.permute(0, 2, 1, 3, 4)  # (B, N, C, p, p)\n        patches = patches.contiguous().view(B, -1, C * p * p)  # (B, N, patch_vec)\n        patch_embeddings = self.proj(patches)  # (B, N, D)\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        tokens = torch.cat([cls_tokens, patch_embeddings], dim=1)  # (B, N+1, D)\n        tokens = tokens + self.pos_embed  # positional encoding\n        return tokens\n\n\n# ----------------------------------------------------------------------------------\n# DistilBERT classifier that consumes image patches via LinearPatchEmbedding\n# ----------------------------------------------------------------------------------\n\nclass ImageDistilBertClassifier(nn.Module):\n    def __init__(self, num_classes: int, patch_size: int, img_size: int = 32):\n        super().__init__()\n        self.patch_embed = LinearPatchEmbedding(patch_size, img_size, embed_dim=768)\n        dbert_cfg = DistilBertConfig(\n            vocab_size=30522,  # dummy\n            max_position_embeddings=self.patch_embed.num_patches + 1,\n            n_heads=6,\n            n_layers=6,\n        )\n        self.transformer = DistilBertModel(dbert_cfg)\n        self.classifier = nn.Linear(768, num_classes)\n\n    def forward(self, x):  # x: (B, 3, H, W)\n        tokens = self.patch_embed(x)\n        attention_mask = torch.ones(tokens.shape[:2], dtype=torch.long, device=x.device)\n        outputs = self.transformer(inputs_embeds=tokens, attention_mask=attention_mask)\n        cls_repr = outputs.last_hidden_state[:, 0]  # (B, 768)\n        logits = self.classifier(cls_repr)\n        return logits\n\n\n# ----------------------------------------------------------------------------------\n# Build function exposed to training script\n# ----------------------------------------------------------------------------------\n\ndef build_model(cfg: Dict[str, Any], dataset_meta: Dict[str, Any]) -> nn.Module:\n    model_cfg = cfg.model\n    name = model_cfg.name.lower()\n\n    if name.startswith(\"mobilenetv2\"):\n        width_mult = float(model_cfg.get(\"width_mult\", 1.0))\n        pretrained = model_cfg.get(\"pretrained\", False)\n        backbone = tvm.mobilenet_v2(width_mult=width_mult, pretrained=pretrained)\n        in_features = backbone.classifier[1].in_features\n        backbone.classifier[1] = nn.Linear(in_features, dataset_meta[\"num_classes\"])\n\n        if \"input_adapter\" in model_cfg:\n            adapter_cfg = model_cfg.input_adapter\n            if adapter_cfg.type == \"char_cnn\":\n                vocab_size = dataset_meta[\"vocab_size\"]\n                encoder = CharCNNEncoder(\n                    vocab_size=vocab_size,\n                    embedding_dim=adapter_cfg.embedding_dim,\n                )\n                adapter = Char2ImageAdapter(encoder.output_dim)\n\n                class ModelWrapper(nn.Module):\n                    def __init__(self, enc, adp, back):\n                        super().__init__()\n                        self.enc = enc\n                        self.adp = adp\n                        self.back = back\n\n                    def forward(self, x):  # x: (B, L)\n                        feat = self.enc(x)\n                        img = self.adp(feat)\n                        return self.back(img)\n\n                return ModelWrapper(encoder, adapter, backbone)\n            else:\n                raise ValueError(\"Unsupported MobileNet input adapter\")\n        else:\n            return backbone\n\n    elif \"distilbert\" in name:\n        if \"modality_adapter\" in model_cfg and model_cfg.modality_adapter.type == \"linear_patch_embedding\":\n            patch_size = int(model_cfg.modality_adapter.patch_size)\n            return ImageDistilBertClassifier(\n                num_classes=dataset_meta[\"num_classes\"],\n                patch_size=patch_size,\n                img_size=dataset_meta[\"input_shape\"][1],\n            )\n        else:\n            # Default: text classification using DistilBERT wordpiece tokenizer\n            from transformers import DistilBertForSequenceClassification, AutoConfig\n\n            num_classes = dataset_meta[\"num_classes\"]\n            config = AutoConfig.from_pretrained(model_cfg.name, num_labels=num_classes)\n            model = DistilBertForSequenceClassification.from_pretrained(\n                model_cfg.name, config=config\n            )\n            return model\n\n    else:\n        raise ValueError(f\"Unsupported model name {model_cfg.name}\")\n",
    "main_py": "\"\"\"\nMain orchestrator script.\nReceives run_id & results_dir via Hydra command line override, spawns train.py as\nsubprocess, handles log redirection and optionally triggers evaluate.py.\n\"\"\"\nfrom __future__ import annotations\n\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import IO\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\ndef _tee_stream(stream: IO[bytes], log_path: Path, is_stdout: bool):\n    \"\"\"Read bytes from *stream*, write to both console and *log_path*.\"\"\"\n\n    log_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(log_path, \"ab\") as fp:\n        for line in iter(stream.readline, b\"\"):\n            fp.write(line)\n            fp.flush()\n            text = line.decode(errors=\"replace\")\n            if is_stdout:\n                sys.stdout.write(text)\n                sys.stdout.flush()\n            else:\n                sys.stderr.write(text)\n                sys.stderr.flush()\n\n\ndef _launch_train(run_id: str, results_dir: Path):\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={run_id}\",\n        f\"results_dir={str(results_dir)}\",\n    ]\n\n    process = subprocess.Popen(\n        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=1\n    )\n\n    stdout_path = results_dir / run_id / \"stdout.log\"\n    stderr_path = results_dir / run_id / \"stderr.log\"\n\n    t_out = threading.Thread(\n        target=_tee_stream, args=(process.stdout, stdout_path, True), daemon=True\n    )\n    t_err = threading.Thread(\n        target=_tee_stream, args=(process.stderr, stderr_path, False), daemon=True\n    )\n    t_out.start()\n    t_err.start()\n\n    process.wait()\n    t_out.join()\n    t_err.join()\n\n    if process.returncode != 0:\n        raise RuntimeError(f\"Training subprocess for {run_id} exited with code {process.returncode}\")\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    run_id = cfg.get(\"run\") if isinstance(cfg.get(\"run\"), str) else cfg.run.run_id\n    results_dir = Path(cfg.results_dir).absolute()\n\n    _launch_train(run_id, results_dir)\n\n    if cfg.get(\"run_evaluation\", True):\n        # Trigger evaluation across all available results\n        eval_cmd = [\n            sys.executable,\n            \"-u\",\n            \"-m\",\n            \"src.evaluate\",\n            f\"results_dir={str(results_dir)}\",\n        ]\n        subprocess.check_call(eval_cmd)\n\n\nif __name__ == \"__main__\":\n    main()",
    "pyproject_toml": "[tool.poetry]\nname = \"hydra-experiment-template\"\nversion = \"0.1.0\"\ndescription = \"Reproducible research code with Hydra and WandB.\"\nauthors = [\"AI Researcher <research@ai.lab>\"]\n\n[tool.poetry.dependencies]\npython = \"^3.8\"\ntorch = \"^2.1\"\ntorchvision = \"^0.16\"\ntransformers = \"^4.35\"\nhydra-core = \"^1.3\"\nwandb = \"^0.16\"\noptuna = \"^3.4\"\nmatplotlib = \"^3.8\"\nscikit-learn = \"^1.3\"\nsentencepiece = \"^0.1\"\ntqdm = \"^4.66\"\n\n[build-system]\nrequires = [\"poetry-core>=1.7.0\"]\nbuild-backend = \"poetry.core.masonry.api\"",
    "config_yaml": "# ------------------------------------------------------------------------------------\n# Top-level Hydra configuration file.\n# The `run` entry will be overridden from the command-line to select which experiment\n# variation to execute (maps to config/run/*.yaml files).\n# ------------------------------------------------------------------------------------\n\ndefaults:\n  - run: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10  # default run, override via CLI\n  - _self_\n\n# Directory that GitHub Actions passes; where artefacts & logs are stored\nresults_dir: ./outputs\n\n# Whether main.py should execute evaluate.py after the train subprocess finishes\nrun_evaluation: true\n\n# The following keys are placeholders that get populated by the included run-specific file\nrun:\n  run_id: PLACEHOLDER_TO_BE_OVERRIDDEN\n  method: ???\n  model: {}\n  dataset: {}\n  training: {}\n\n# Allow sweeps / Optuna etc. (unused here)\noptuna: {}\n\n# End of config.yaml\n"
}
