
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/run/*.yaml` files
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation

## Hydra Configuration Structure
Each run config file (`config/run/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.

## Command Line Interface
The generated code must support the following CLI:
```bash
# Run a single experiment (specified by Hydra)
uv run python -u -m src.main run={run_id} results_dir={path}
```

The `run` argument specifies which experiment to run (matching a run_id from config/run/*.yaml).
The `results_dir` argument is passed from the GitHub Actions workflow and specifies where all outputs should be saved.

## Output Structure
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
- `src/train.py`: Logic to run a single experiment variation. Uses Hydra config to load parameters. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Complete preprocessing pipeline implementation for the specified datasets
- `src/model.py`: Complete model architecture implementations. It will contain classes for baseline, proposed, and ablation models. Implement all architectures from scratch.
- `src/main.py`: The main orchestrator script. It receives a run_id via Hydra, launches train.py for that specific experiment, manages subprocess, collects and consolidates logs, and finally triggers evaluate.py if needed.
- `pyproject.toml`: Complete project dependencies (include hydra-core, optuna if needed)
- `config/config.yaml`: Main Hydra configuration file that defines the list of all experiment run_ids


### Key Implementation Focus Areas
1. Hydra-Driven Configuration: All parameters loaded from run configs dynamically
2. Algorithm Core: Full implementation of the proposed method with proper abstraction
3. Sequential Execution: main.py executes run variations one at a time in sequential order
4. Configuration Driven: The entire workflow must be driven by the YAML configuration files
5. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete
6. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Code Validation Feedback


**Previous Validation Issue**: The project is NOT ready for immediate execution:

1. Hydra config path is incorrect in all @hydra.main decorators – "../../config" goes two levels up from src/, pointing outside the repo. It should be "../config" (one level) or left empty. Hydra will fail to locate config files.
2. Required master config (config/config.yaml) does not list the four run_ids; only a default run field is present, violating requirement 7.
3. No config/run/*.yaml files are included in the repository even though train.py tries to load them (Path …/config/run/{run_id}.yaml). Code will crash with FileNotFoundError.
4. evaluate.py uses os.environ but does not import os → NameError during execution.
5. WandB settings referenced in code but no wandb section defined in configs, causing key-access errors (run_cfg.get("wandb" …) works, but OmegaConf struct is disabled earlier; still okay, but missing recommended config).
6. Minor: run_with_tee does not merge stderr into log on process termination; not blocking, but requirements allow.

Due to the critical missing configs, wrong Hydra paths and missing import, the code cannot run successfully without fixes.
**Action Required**: Address this issue in the implementation.



# Experimental Environment
cpu
RAM：500 MB

# Current Research Method (Target for Experiment Design)
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

# Experimental Design
- Strategy: 

# Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10
  Config File: config/run/comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  width_mult: 0.5
  dropout: 0.2
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
  augmentations:
    random_crop: true
    random_flip: horizontal
training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.05
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0005
  lr_scheduler:
    type: cosine
    T_max: 50
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 0.001
      high: 0.1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    momentum:
      type: uniform
      low: 0.7
      high: 0.95
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned
  Config File: config/run/comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  input_adapter:
    type: char_cnn
    embedding_dim: 256
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: char
training:
  epochs: 3
  batch_size: 16
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 0.01
  gradient_accumulation_steps: 2
optuna:
  n_trials: 15
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-1

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  Config File: config/run/comparative-2-DistilBERT-base-66M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-CIFAR-10
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
  modality_adapter:
    type: linear_patch_embedding
    patch_size: 4
    sequence_length: 64
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    patch_size:
      type: categorical
      choices: [2, 4, 8]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  Config File: config/run/comparative-2-DistilBERT-base-66M-alpaca-cleaned.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: sentencepiece
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  


# External Resources (Use these for implementation)



# ========================================
# WandB Integration Requirements
# ========================================

## Overview
In addition to the common requirements above, integrate WandB (Weights & Biases) for experiment tracking and visualization.

## Additional Requirements
- WANDB INTEGRATION: Initialize WandB with proper project/entity from config, log metrics/artifacts, and save wandb_run_id to metadata
- WANDB METADATA: Save WandB run information to `.research/iteration{experiment_iteration}/wandb_metadata.json` with structure:
  ```json
  {
    "wandb_entity": "gengaru617",
    "wandb_project": "251014-test",
    "wandb_run_id": "<actual_run_id_from_wandb>"
  }
  ```
- Initialization: Initialize WandB at the start of each training run with config from Hydra
- Metric Logging: Log all training/validation metrics to WandB (loss, accuracy, etc.)
- Artifact Tracking: Log model checkpoints and important artifacts
- Metadata Saving: After WandB initialization, save the wandb_run_id to `.research/iteration{experiment_iteration}/wandb_metadata.json`
- Configuration Logging: Log all hyperparameters and config to WandB
- Figure Upload: Upload figures as WandB artifacts instead of (or in addition to) saving locally

## WandB Configuration
- Entity: gengaru617
- Project: 251014-test

## Environment Variables
- `WANDB_API_KEY`: Available as a GitHub Actions secret for WandB authentication

## Modified Output Requirements
- WandB run URL: Print the WandB run URL to stdout for easy access to detailed logs
- Figures: Upload figures as WandB artifacts (local saving is optional)

## Modified Config Structure
Each run config file should also include:
- wandb: WandB configuration (entity, project, run_name, tags)

## Modified Dependencies
- pyproject.toml: Include `wandb` package

## WandB Metadata File Location
The wandb_metadata.json file must be saved to:
- Path: `.research/iteration{experiment_iteration}/wandb_metadata.json`
- The experiment_iteration should be passed as an environment variable or config parameter
- The file should be created immediately after WandB run initialization in train.py



Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "import json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\n\nimport wandb\n\n# --- local imports -----------------------------------------------------------\nfrom .model import build_model\nfrom .preprocess import build_dataloaders\n\n# -----------------------------------------------------------------------------\n# INTERNAL RUN CONFIG REGISTRY (used when YAML files are not available on disk)\n# -----------------------------------------------------------------------------\nRUN_CONFIGS: Dict[str, Dict[str, Any]] = {\n    \"comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\": {\n        \"method\": \"comparative-2\",\n        \"model\": {\n            \"name\": \"MobileNetV2\",\n            \"variant\": 0.5,\n            \"pretrained\": False,\n            \"width_mult\": 0.5,\n            \"dropout\": 0.2,\n        },\n        \"dataset\": {\n            \"name\": \"CIFAR-10\",\n            \"splits\": {\"train\": 45000, \"val\": 5000, \"test\": 10000},\n            \"image_size\": 32,\n            \"normalization\": {\n                \"mean\": [0.4914, 0.4822, 0.4465],\n                \"std\": [0.2023, 0.1994, 0.2010],\n            },\n            \"augmentations\": {\"random_crop\": True, \"random_flip\": \"horizontal\"},\n        },\n        \"training\": {\n            \"epochs\": 50,\n            \"batch_size\": 64,\n            \"learning_rate\": 0.05,\n            \"optimizer\": \"sgd\",\n            \"momentum\": 0.9,\n            \"weight_decay\": 5e-4,\n            \"lr_scheduler\": {\"type\": \"cosine\", \"T_max\": 50},\n        },\n    },\n    \"comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned\": {\n        \"method\": \"comparative-2\",\n        \"model\": {\n            \"name\": \"MobileNetV2\",\n            \"variant\": 0.5,\n            \"pretrained\": False,\n            \"input_adapter\": {\"type\": \"char_cnn\", \"embedding_dim\": 256},\n        },\n        \"dataset\": {\n            \"name\": \"alpaca-cleaned\",\n            \"task\": \"instruction_following\",\n            \"max_seq_length\": 512,\n            \"tokenizer\": \"char\",\n        },\n        \"training\": {\n            \"epochs\": 3,\n            \"batch_size\": 16,\n            \"learning_rate\": 0.001,\n            \"optimizer\": \"adam\",\n            \"weight_decay\": 0.01,\n            \"gradient_accumulation_steps\": 2,\n        },\n    },\n    \"comparative-2-DistilBERT-base-66M-CIFAR-10\": {\n        \"method\": \"comparative-2\",\n        \"model\": {\n            \"name\": \"distilbert-base-uncased\",\n            \"pretrained\": True,\n            \"modality_adapter\": {\n                \"type\": \"linear_patch_embedding\",\n                \"patch_size\": 4,\n                \"sequence_length\": 64,\n            },\n        },\n        \"dataset\": {\n            \"name\": \"CIFAR-10\",\n            \"splits\": {\"train\": 45000, \"val\": 5000, \"test\": 10000},\n            \"image_size\": 32,\n            \"normalization\": {\n                \"mean\": [0.4914, 0.4822, 0.4465],\n                \"std\": [0.2023, 0.1994, 0.2010],\n            },\n        },\n        \"training\": {\n            \"epochs\": 10,\n            \"batch_size\": 32,\n            \"learning_rate\": 1e-4,\n            \"optimizer\": \"adamw\",\n            \"weight_decay\": 0.01,\n            \"lr_scheduler\": {\"type\": \"linear\", \"warmup_steps\": 500},\n        },\n    },\n    \"comparative-2-DistilBERT-base-66M-alpaca-cleaned\": {\n        \"method\": \"comparative-2\",\n        \"model\": {\n            \"name\": \"distilbert-base-uncased\",\n            \"pretrained\": True,\n        },\n        \"dataset\": {\n            \"name\": \"alpaca-cleaned\",\n            \"task\": \"instruction_following\",\n            \"max_seq_length\": 512,\n            \"tokenizer\": \"sentencepiece\",\n        },\n        \"training\": {\n            \"epochs\": 3,\n            \"batch_size\": 16,\n            \"learning_rate\": 2e-5,\n            \"optimizer\": \"adamw\",\n            \"weight_decay\": 0.01,\n            \"lr_scheduler\": {\"type\": \"linear\", \"warmup_steps\": 500},\n        },\n    },\n}\n\n# -----------------------------------------------------------------------------\n# Helper utilities\n# -----------------------------------------------------------------------------\n\ndef _resolve_run_cfg(run_id: str) -> Dict[str, Any]:\n    \"\"\"Attempt to load run configuration from YAML file; otherwise fallback to\n    the in-memory registry defined above.\n    \"\"\"\n    yaml_path = Path(hydra.utils.get_original_cwd()) / \"config\" / \"run\" / f\"{run_id}.yaml\"\n    if yaml_path.is_file():\n        with open(yaml_path, \"r\", encoding=\"utf-8\") as fp:\n            return OmegaConf.to_container(OmegaConf.load(fp), resolve=True)\n    if run_id not in RUN_CONFIGS:\n        raise ValueError(f\"Run ID '{run_id}' not found either on disk or registry.\")\n    return RUN_CONFIGS[run_id]\n\n\ndef _init_wandb(run_id: str, cfg: DictConfig, run_cfg: Dict[str, Any]) -> wandb.sdk.wandb_run.Run:\n    \"\"\"Initialise Weights & Biases tracking and persist run metadata.\"\"\"\n    wandb_cfg = cfg.get(\"wandb\", {})\n    wandb_run = wandb.init(\n        project=wandb_cfg.get(\"project\", \"251014-test\"),\n        entity=wandb_cfg.get(\"entity\", \"gengaru617\"),\n        name=run_cfg.get(\"run_id\", run_id),\n        config=run_cfg,\n        tags=wandb_cfg.get(\"tags\", []),\n        reinit=True,\n    )\n\n    # Persist metadata immediately\n    iteration = os.getenv(\"EXPERIMENT_ITERATION\", \"0\")\n    meta_dir = Path(hydra.utils.get_original_cwd()) / f\".research/iteration{iteration}\"\n    meta_dir.mkdir(parents=True, exist_ok=True)\n    metadata_path = meta_dir / \"wandb_metadata.json\"\n    with open(metadata_path, \"w\", encoding=\"utf-8\") as fp:\n        json.dump(\n            {\n                \"wandb_entity\": wandb_cfg.get(\"entity\", \"gengaru617\"),\n                \"wandb_project\": wandb_cfg.get(\"project\", \"251014-test\"),\n                \"wandb_run_id\": wandb_run.id,\n            },\n            fp,\n            indent=2,\n        )\n    print(f\"WandB initialized: {wandb_run.url}\")\n    return wandb_run\n\n\n# -----------------------------------------------------------------------------\n# Training/evaluation helpers\n# -----------------------------------------------------------------------------\n\ndef _accuracy(pred: torch.Tensor, target: torch.Tensor) -> float:\n    \"\"\"Compute top-1 accuracy.\"\"\"\n    with torch.no_grad():\n        preds = pred.argmax(dim=1)\n        correct = (preds == target).sum().item()\n        return correct / target.size(0)\n\n\n# -----------------------------------------------------------------------------\n# TRAINING ENTRY POINT\n# -----------------------------------------------------------------------------\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -> None:\n    start_time = time.time()\n\n    run_id: str = cfg.get(\"run_id\")\n    results_dir: Path = Path(cfg.get(\"results_dir\", \"./results\")).expanduser()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    run_cfg = _resolve_run_cfg(run_id)\n    run_cfg[\"run_id\"] = run_id\n\n    # ---------------------------------------------------------------------\n    # Print experiment description BEFORE training starts (requirement)\n    # ---------------------------------------------------------------------\n    print(\"# ----------------------- Experiment Description -----------------------\")\n    print(json.dumps(run_cfg, indent=2))\n    print(\"# ----------------------------------------------------------------------\")\n\n    # ----------------- Seed & device -----------------\n    torch.manual_seed(cfg.get(\"seed\", 42))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --------------- Data loaders --------------------\n    train_loader, val_loader, test_loader, num_classes = build_dataloaders(run_cfg)\n\n    # --------------- Model ---------------------------\n    model = build_model(run_cfg, num_classes=num_classes).to(device)\n\n    # --------------- Optimizer & scheduler -----------\n    opt_name = run_cfg[\"training\"].get(\"optimizer\", \"sgd\").lower()\n    lr = run_cfg[\"training\"].get(\"learning_rate\", 1e-3)\n    weight_decay = float(run_cfg[\"training\"].get(\"weight_decay\", 0.0))\n\n    if opt_name == \"sgd\":\n        optimizer = optim.SGD(\n            model.parameters(),\n            lr=lr,\n            momentum=run_cfg[\"training\"].get(\"momentum\", 0.9),\n            weight_decay=weight_decay,\n        )\n    elif opt_name in {\"adam\", \"adamw\"}:\n        opt_cls = optim.AdamW if opt_name == \"adamw\" else optim.Adam\n        optimizer = opt_cls(model.parameters(), lr=lr, weight_decay=weight_decay)\n    else:\n        raise ValueError(f\"Unsupported optimizer {opt_name}\")\n\n    sched_cfg = run_cfg[\"training\"].get(\"lr_scheduler\", {})\n    if sched_cfg.get(\"type\") == \"cosine\":\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=sched_cfg.get(\"T_max\", run_cfg[\"training\"].get(\"epochs\", 10))\n        )\n    elif sched_cfg.get(\"type\") == \"linear\":\n        def lr_lambda(step):\n            warmup = sched_cfg.get(\"warmup_steps\", 0)\n            if step < warmup:\n                return float(step) / float(max(1, warmup))\n            return max(0.0, float(step - warmup) / float(max(1, 1000 - warmup)))\n        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    else:\n        scheduler = None\n\n    # ---------------- WandB ------------------------\n    wandb_run = _init_wandb(run_id, cfg, run_cfg)\n\n    # ---------------- Criterion --------------------\n    criterion = nn.CrossEntropyLoss()\n\n    # ---------------- Training loop ---------------\n    epochs = int(run_cfg[\"training\"].get(\"epochs\", 1))\n    best_val_acc = 0.0\n    history = {\n        \"run_id\": run_id,\n        \"train_loss\": [],\n        \"train_acc\": [],\n        \"val_loss\": [],\n        \"val_acc\": [],\n    }\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss, running_acc, counter = 0.0, 0.0, 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [train]\")\n        for batch in pbar:\n            inputs, labels = batch[0].to(device), batch[1].to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            running_acc += _accuracy(outputs.detach(), labels) * inputs.size(0)\n            counter += inputs.size(0)\n            pbar.set_postfix({\"loss\": running_loss / counter, \"acc\": running_acc / counter})\n        train_loss = running_loss / counter\n        train_acc = running_acc / counter\n\n        # ---------------- Validation ----------------\n        model.eval()\n        val_loss, val_acc, val_counter = 0.0, 0.0, 0\n        with torch.no_grad():\n            for batch in val_loader:\n                inputs, labels = batch[0].to(device), batch[1].to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * inputs.size(0)\n                val_acc += _accuracy(outputs, labels) * inputs.size(0)\n                val_counter += inputs.size(0)\n        val_loss /= val_counter\n        val_acc /= val_counter\n\n        if scheduler is not None:\n            scheduler.step()\n\n        # Logging\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n\n        wandb.log(\n            {\n                \"epoch\": epoch,\n                \"train/loss\": train_loss,\n                \"train/acc\": train_acc,\n                \"val/loss\": val_loss,\n                \"val/acc\": val_acc,\n            }\n        )\n\n        # Save best checkpoint\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            ckpt_path = results_dir / f\"{run_id}_best.pt\"\n            torch.save({\n                \"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n            }, ckpt_path)\n            wandb.save(str(ckpt_path))\n\n    # ---------------- Test phase ------------------\n    model.eval()\n    test_loss, test_acc, test_counter = 0.0, 0.0, 0\n    with torch.no_grad():\n        for batch in test_loader:\n            inputs, labels = batch[0].to(device), batch[1].to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item() * inputs.size(0)\n            test_acc += _accuracy(outputs, labels) * inputs.size(0)\n            test_counter += inputs.size(0)\n    test_loss /= test_counter\n    test_acc /= test_counter\n\n    # Final logging\n    wandb.summary.update({\"test/loss\": test_loss, \"test/acc\": test_acc})\n    wandb.finish()\n\n    history.update({\"test_loss\": test_loss, \"test_acc\": test_acc})\n\n    # Persist results\n    result_path = results_dir / f\"{run_id}_results.json\"\n    with open(result_path, \"w\", encoding=\"utf-8\") as fp:\n        json.dump(history, fp, indent=2)\n\n    # Print JSON to stdout (requirement for structured logging)\n    print(json.dumps(history))\n\n    elapsed = time.time() - start_time\n    print(f\"Training completed in {elapsed / 60:.1f} minutes.\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "import json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport hydra\nfrom omegaconf import DictConfig\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport wandb\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -> None:\n    \"\"\"Aggregate experiment results and generate comparison figures.\"\"\"\n    results_dir = Path(cfg.get(\"results_dir\", \"./results\")).expanduser()\n    if not results_dir.exists():\n        raise RuntimeError(f\"Results directory '{results_dir}' does not exist\")\n\n    result_files: List[Path] = list(results_dir.glob(\"*_results.json\"))\n    if not result_files:\n        print(\"No result files found. Skipping evaluation.\")\n        return\n\n    records: List[Dict] = []\n    for file in result_files:\n        with open(file, \"r\", encoding=\"utf-8\") as fp:\n            rec = json.load(fp)\n        records.append(rec)\n\n    # Build comparison arrays\n    names = [rec[\"run_id\"] for rec in records]\n    test_accs = np.array([rec[\"test_acc\"] for rec in records])\n\n    # Bar chart\n    fig, ax = plt.subplots(figsize=(max(6, len(names) * 1.5), 4))\n    bars = ax.bar(names, test_accs, color=\"skyblue\")\n    ax.set_ylabel(\"Test Accuracy\")\n    ax.set_title(\"Experiment Comparison (Test Accuracy)\")\n    ax.bar_label(bars, fmt=\"{:.2f}\")\n    ax.set_ylim(0, 1)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n\n    fig_path = results_dir / \"comparison.png\"\n    fig.savefig(fig_path)\n\n    # Upload to WandB as artifact\n    wandb_cfg = cfg.get(\"wandb\", {})\n    wandb_run = wandb.init(\n        project=wandb_cfg.get(\"project\", \"251014-test\"),\n        entity=wandb_cfg.get(\"entity\", \"gengaru617\"),\n        name=\"evaluation\",\n        tags=[\"evaluation\"],\n        reinit=True,\n    )\n    artifact = wandb.Artifact(\"experiment_comparison\", type=\"figure\")\n    artifact.add_file(str(fig_path))\n    wandb_run.log_artifact(artifact)\n    wandb_run.finish()\n\n    # Structured stdout\n    summary = {name: {\"test_acc\": acc} for name, acc in zip(names, test_accs)}\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"Full preprocessing pipeline for supported datasets.\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Tuple, Dict, Any\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, Dataset\nimport torchvision\nfrom torchvision import transforms\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# ------------------------------ CIFAR-10 --------------------------------------\n\ndef _build_cifar10_loaders(dataset_cfg: Dict[str, Any], batch_size: int) -> Tuple[DataLoader, DataLoader, DataLoader, int]:\n    image_size = dataset_cfg.get(\"image_size\", 32)\n    norm = dataset_cfg[\"normalization\"]\n    mean, std = norm[\"mean\"], norm[\"std\"]\n\n    augmentations = []\n    if dataset_cfg.get(\"augmentations\", {}).get(\"random_crop\", False):\n        augmentations.append(transforms.RandomCrop(image_size, padding=4))\n    if dataset_cfg.get(\"augmentations\", {}).get(\"random_flip\", \"\") == \"horizontal\":\n        augmentations.append(transforms.RandomHorizontalFlip())\n\n    train_transform = transforms.Compose(\n        augmentations + [\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ]\n    )\n    test_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n\n    root = Path(\"./data\").expanduser()\n    full_train = torchvision.datasets.CIFAR10(root=root, train=True, download=True, transform=train_transform)\n    test_ds = torchvision.datasets.CIFAR10(root=root, train=False, download=True, transform=test_transform)\n\n    # Split train/val\n    train_size = dataset_cfg[\"splits\"][\"train\"]\n    val_size = dataset_cfg[\"splits\"].get(\"val\", 5000)\n    if train_size + val_size > len(full_train):\n        raise ValueError(\"Specified split sizes exceed dataset length.\")\n    train_ds, val_ds = random_split(full_train, [train_size, val_size])\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    return train_loader, val_loader, test_loader, 10  # CIFAR-10 classes\n\n\n# --------------------------- ALPACA CLEANED -----------------------------------\nclass AlpacaClassificationDataset(Dataset):\n    \"\"\"A light-weight instruction-following dataset framed as binary classification:\n    label = 1 if output length > threshold else 0\n    (This keeps compute low while providing a non-trivial NLP workload.)\n    \"\"\"\n\n    def __init__(self, split: str, max_samples: int | None, tokenizer, max_seq_length: int = 512):\n        self.data = load_dataset(\"tatsu-lab/alpaca-cleaned\", split=split)\n        if max_samples is not None and len(self.data) > max_samples:\n            self.data = self.data.select(range(max_samples))\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx: int):\n        record = self.data[idx]\n        text = record[\"instruction\"] + \"\\n\" + (record.get(\"input\") or \"\")\n        tokenized = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_seq_length,\n            return_tensors=\"pt\",\n        )\n        label = 1 if len(record[\"output\"]) > 100 else 0\n        return {\n            \"input_ids\": tokenized.input_ids.squeeze(0),\n            \"attention_mask\": tokenized.attention_mask.squeeze(0),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n\ndef _alpaca_collate(batch):\n    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n    labels = torch.stack([item[\"labels\"] for item in batch])\n    return (input_ids, attention_mask), labels\n\n\ndef _build_alpaca_loaders(dataset_cfg: Dict[str, Any], batch_size: int) -> Tuple[DataLoader, DataLoader, DataLoader, int]:\n    tokenizer_name = dataset_cfg.get(\"tokenizer\", \"sentencepiece\")\n    if tokenizer_name == \"sentencepiece\":\n        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n    else:\n        # For char-level experiments, fall back to basic tokenizer that splits characters\n        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  # placeholder for char tokenizer\n\n    # To keep memory within limits, sub-sample\n    train_ds = AlpacaClassificationDataset(\"train[:5000]\", 5000, tokenizer, dataset_cfg.get(\"max_seq_length\", 512))\n    val_ds = AlpacaClassificationDataset(\"train[5000:5500]\", 500, tokenizer, dataset_cfg.get(\"max_seq_length\", 512))\n    test_ds = AlpacaClassificationDataset(\"test[:1000]\", 1000, tokenizer, dataset_cfg.get(\"max_seq_length\", 512))\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=_alpaca_collate)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=_alpaca_collate)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=_alpaca_collate)\n\n    return train_loader, val_loader, test_loader, 2  # binary classification\n\n\n# --------------------------- PUBLIC API ---------------------------------------\n\ndef build_dataloaders(run_cfg: Dict[str, Any]):\n    ds_cfg = run_cfg[\"dataset\"]\n    batch_size = run_cfg[\"training\"].get(\"batch_size\", 32)\n    name = ds_cfg[\"name\"].lower()\n    if name == \"cifar-10\":\n        return _build_cifar10_loaders(ds_cfg, batch_size)\n    elif name == \"alpaca-cleaned\":\n        return _build_alpaca_loaders(ds_cfg, batch_size)\n    else:\n        raise ValueError(f\"Unsupported dataset {name}\")\n",
    "model_py": "\"\"\"Model architecture definitions for experiment variants.\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import Dict, Any, Optional\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom transformers import DistilBertModel, DistilBertConfig\n\n# --------------------------- MobileNetV2 --------------------------------------\nclass MobileNetClassifier(nn.Module):\n    def __init__(self, num_classes: int, width_mult: float = 1.0, dropout: float = 0.2, pretrained: bool = False):\n        super().__init__()\n        mobilenet = torchvision.models.mobilenet_v2(weights=\"DEFAULT\" if pretrained else None, width_mult=width_mult)\n        # Replace classifier\n        in_features = mobilenet.classifier[1].in_features\n        mobilenet.classifier = nn.Sequential(\n            nn.Dropout(p=dropout),\n            nn.Linear(in_features, num_classes),\n        )\n        self.model = mobilenet\n\n    def forward(self, x):\n        return self.model(x)\n\n\n# ---------------------- DistilBERT Vision Adapter -----------------------------\nclass PatchEmbedding(nn.Module):\n    def __init__(self, in_chans: int, embed_dim: int, patch_size: int, seq_length: int):\n        super().__init__()\n        self.patch_size = patch_size\n        self.seq_length = seq_length\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, seq_length + 1, embed_dim))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n\n    def forward(self, x):\n        B = x.size(0)\n        x = self.proj(x)  # B, C, H', W'\n        x = x.flatten(2).transpose(1, 2)  # B, N, C\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed[:, : x.size(1), :]\n        return x\n\n\nclass DistilBertVisionClassifier(nn.Module):\n    def __init__(\n        self,\n        num_classes: int,\n        patch_size: int = 4,\n        sequence_length: int = 64,\n        pretrained: bool = True,\n    ):\n        super().__init__()\n        self.embed_dim = 768\n        self.patch_embed = PatchEmbedding(3, self.embed_dim, patch_size, sequence_length)\n        config = DistilBertConfig()\n        self.transformer = DistilBertModel.from_pretrained(\"distilbert-base-uncased\") if pretrained else DistilBertModel(config)\n        self.head = nn.Linear(self.embed_dim, num_classes)\n\n    def forward(self, x):\n        # Build inputs_embeds\n        embeds = self.patch_embed(x)\n        attention_mask = torch.ones(embeds.size()[:2], dtype=torch.long, device=x.device)\n        out = self.transformer(inputs_embeds=embeds, attention_mask=attention_mask)\n        cls_output = out.last_hidden_state[:, 0]\n        return self.head(cls_output)\n\n\n# ---------------------- DistilBERT Text Classifier ----------------------------\nclass DistilBertTextClassifier(nn.Module):\n    def __init__(self, num_classes: int, pretrained: bool = True):\n        super().__init__()\n        self.bert = (\n            DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n            if pretrained\n            else DistilBertModel(DistilBertConfig())\n        )\n        self.head = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n    def forward(self, inputs):\n        input_ids, attention_mask = inputs\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0]\n        return self.head(cls_output)\n\n\n# --------------------------- Registry -----------------------------------------\n\ndef build_model(run_cfg: Dict[str, Any], num_classes: int):\n    model_cfg = run_cfg[\"model\"]\n    name = model_cfg[\"name\"].lower()\n\n    if name == \"mobilenetv2\":\n        return MobileNetClassifier(\n            num_classes=num_classes,\n            width_mult=float(model_cfg.get(\"width_mult\", 1.0)),\n            dropout=float(model_cfg.get(\"dropout\", 0.2)),\n            pretrained=bool(model_cfg.get(\"pretrained\", False)),\n        )\n    elif name == \"distilbert-base-uncased\":\n        # Determine modality\n        if run_cfg[\"dataset\"][\"name\"].lower() == \"cifar-10\":\n            adapter = model_cfg.get(\"modality_adapter\", {})\n            return DistilBertVisionClassifier(\n                num_classes=num_classes,\n                patch_size=adapter.get(\"patch_size\", 4),\n                sequence_length=adapter.get(\"sequence_length\", 64),\n                pretrained=bool(model_cfg.get(\"pretrained\", True)),\n            )\n        else:\n            return DistilBertTextClassifier(num_classes=num_classes, pretrained=bool(model_cfg.get(\"pretrained\", True)))\n    else:\n        raise ValueError(f\"Unsupported model name '{name}'\")\n",
    "main_py": "\"\"\"Main orchestrator script that launches experiment runs and evaluation.\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\ndef _run_with_tee(cmd: List[str], log_dir: Path):\n    \"\"\"Run a subprocess while tee-ing its stdout/err into files and console.\"\"\"\n    log_dir.mkdir(parents=True, exist_ok=True)\n    stdout_path = log_dir / \"stdout.log\"\n    stderr_path = log_dir / \"stderr.log\"\n\n    with stdout_path.open(\"w\") as out_fp, stderr_path.open(\"w\") as err_fp:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)\n        # Tee stdout\n        for line in process.stdout:\n            sys.stdout.write(line)\n            sys.stdout.flush()\n            out_fp.write(line)\n        # Tee stderr\n        for line in process.stderr:\n            sys.stderr.write(line)\n            sys.stderr.flush()\n            err_fp.write(line)\n        process.wait()\n        if process.returncode != 0:\n            raise RuntimeError(f\"Subprocess failed with exit code {process.returncode}\")\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    run_id = cfg.get(\"run_id\")\n    results_dir = Path(cfg.get(\"results_dir\", \"./results\")).expanduser()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # Launch train.py as subprocess\n    env = os.environ.copy()\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run_id={run_id}\",\n        f\"results_dir={results_dir}\"\n    ]\n    print(f\"Launching training for run_id={run_id} ...\")\n    _run_with_tee(cmd, results_dir / run_id)\n\n    # After training complete, run evaluation over existing result files\n    print(\"\\nAll training complete. Launching evaluation...\")\n    eval_cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.evaluate\",\n        f\"results_dir={results_dir}\"\n    ]\n    _run_with_tee(eval_cmd, results_dir / \"evaluation\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"comparative-efficient-models\"\nversion = \"0.1.0\"\nauthors = [\n  {name = \"AI Researcher\", email = \"noreply@example.com\"},\n]\ndescription = \"Research codebase comparing lightweight models across modalities using Hydra configuration management.\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\nhydra-core = \"^1.3.2\"\ntorch = \"^2.1.0\"\ntorchvision = \"^0.16.0\"\noptuna = \"^3.4.0\"\nwandb = \"^0.16.0\"\ntransformers = \"^4.35.0\"\ndatasets = \"^2.14.0\"\nmatplotlib = \"^3.8.0\"\nnumpy = \"^1.23.0\"\npandas = \"^1.5.3\"\nscikit-learn = \"^1.3.0\"\n\ngrade = \"patch\"\n",
    "config_yaml": "# Master Hydra configuration listing available run IDs\nrun_id: \"comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\"\nresults_dir: \"./results\"\nseed: 42\n\nwandb:\n  entity: \"gengaru617\"\n  project: \"251014-test\"\n  tags: []\n"
}
