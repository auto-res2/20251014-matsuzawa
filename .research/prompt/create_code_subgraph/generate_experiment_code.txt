
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/run/*.yaml` files
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation

## Hydra Configuration Structure
Each run config file (`config/run/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.

## Command Line Interface
The generated code must support the following CLI:
```bash
# Run a single experiment (specified by Hydra)
uv run python -u -m src.main run={run_id} results_dir={path}
```

The `run` argument specifies which experiment to run (matching a run_id from config/run/*.yaml).
The `results_dir` argument is passed from the GitHub Actions workflow and specifies where all outputs should be saved.

## Output Structure
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
- `src/train.py`: Logic to run a single experiment variation. Uses Hydra config to load parameters. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Complete preprocessing pipeline implementation for the specified datasets
- `src/model.py`: Complete model architecture implementations. It will contain classes for baseline, proposed, and ablation models. Implement all architectures from scratch.
- `src/main.py`: The main orchestrator script. It receives a run_id via Hydra, launches train.py for that specific experiment, manages subprocess, collects and consolidates logs, and finally triggers evaluate.py if needed.
- `pyproject.toml`: Complete project dependencies (include hydra-core, optuna if needed)
- `config/config.yaml`: Main Hydra configuration file that defines the list of all experiment run_ids


### Key Implementation Focus Areas
1. Hydra-Driven Configuration: All parameters loaded from run configs dynamically
2. Algorithm Core: Full implementation of the proposed method with proper abstraction
3. Sequential Execution: main.py executes run variations one at a time in sequential order
4. Configuration Driven: The entire workflow must be driven by the YAML configuration files
5. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete
6. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Code Validation Feedback


**Previous Validation Issue**: The experiment code is not ready for immediate execution:

1. Configuration issues
   • config/config.yaml must list *all* run_ids; current file contains only one.
   • Code relies on an internal RUN_CONFIGS fallback instead of loading YAMLs from config/run/*.yaml as required.

2. Incomplete / placeholder implementations
   • preprocess.py: comment "# placeholder for char tokenizer" – char-level tokenizer not implemented.
   • MobileNet + text (run_id comparative-2-MobileNetV2-…-alpaca) uses MobileNet on NLP data; required char_cnn input adapter is missing.

3. Training loop mismatch for NLP batches
   • train.py assumes batch[0] is a tensor and calls .to(device). For Alpaca loaders batch[0] is a (input_ids, attention_mask) tuple → AttributeError.

4. Evaluation/figures
   • evaluate.py saves figure as PNG in results_dir root; spec requires PDF saved under {results_dir}/images/.

5. Logging
   • subprocess tee implementation may dead-lock (reads stdout fully before stderr). Not production-safe.

6. Hydra CLI conformity
   • Required CLI path (`uv run python -u -m src.main run={run_id} …`) assumes run configs are disk-based; internal fallback breaks reproducibility.

Because multiple core criteria fail (complete implementation, Hydra integration, data/model correctness, required file outputs), the code is **not** production-ready.
**Action Required**: Address this issue in the implementation.



# Experimental Environment
cpu
RAM：500 MB

# Current Research Method (Target for Experiment Design)
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

# Experimental Design
- Strategy: 

# Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10
  Config File: config/run/comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  width_mult: 0.5
  dropout: 0.2
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
  augmentations:
    random_crop: true
    random_flip: horizontal
training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.05
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0005
  lr_scheduler:
    type: cosine
    T_max: 50
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 0.001
      high: 0.1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    momentum:
      type: uniform
      low: 0.7
      high: 0.95
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned
  Config File: config/run/comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  input_adapter:
    type: char_cnn
    embedding_dim: 256
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: char
training:
  epochs: 3
  batch_size: 16
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 0.01
  gradient_accumulation_steps: 2
optuna:
  n_trials: 15
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-1

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  Config File: config/run/comparative-2-DistilBERT-base-66M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-CIFAR-10
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
  modality_adapter:
    type: linear_patch_embedding
    patch_size: 4
    sequence_length: 64
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    patch_size:
      type: categorical
      choices: [2, 4, 8]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  Config File: config/run/comparative-2-DistilBERT-base-66M-alpaca-cleaned.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: sentencepiece
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  


# External Resources (Use these for implementation)



# ========================================
# WandB Integration Requirements
# ========================================

## Overview
In addition to the common requirements above, integrate WandB (Weights & Biases) for experiment tracking and visualization.

## Additional Requirements
- WANDB INTEGRATION: Initialize WandB with proper project/entity from config, log metrics/artifacts, and save wandb_run_id to metadata
- WANDB METADATA: Save WandB run information to `.research/iteration{experiment_iteration}/wandb_metadata.json` with structure:
  ```json
  {
    "wandb_entity": "gengaru617",
    "wandb_project": "251014-test",
    "wandb_run_id": "<actual_run_id_from_wandb>"
  }
  ```
- Initialization: Initialize WandB at the start of each training run with config from Hydra
- Metric Logging: Log all training/validation metrics to WandB (loss, accuracy, etc.)
- Artifact Tracking: Log model checkpoints and important artifacts
- Metadata Saving: After WandB initialization, save the wandb_run_id to `.research/iteration{experiment_iteration}/wandb_metadata.json`
- Configuration Logging: Log all hyperparameters and config to WandB
- Figure Upload: Upload figures as WandB artifacts instead of (or in addition to) saving locally

## WandB Configuration
- Entity: gengaru617
- Project: 251014-test

## Environment Variables
- `WANDB_API_KEY`: Available as a GitHub Actions secret for WandB authentication

## Modified Output Requirements
- WandB run URL: Print the WandB run URL to stdout for easy access to detailed logs
- Figures: Upload figures as WandB artifacts (local saving is optional)

## Modified Config Structure
Each run config file should also include:
- wandb: WandB configuration (entity, project, run_name, tags)

## Modified Dependencies
- pyproject.toml: Include `wandb` package

## WandB Metadata File Location
The wandb_metadata.json file must be saved to:
- Path: `.research/iteration{experiment_iteration}/wandb_metadata.json`
- The experiment_iteration should be passed as an environment variable or config parameter
- The file should be created immediately after WandB run initialization in train.py



Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "import os\nimport sys\nimport json\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport hydra\nfrom hydra.utils import to_absolute_path\nfrom omegaconf import DictConfig, OmegaConf\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom .preprocess import DataModule\nfrom .model import build_model\n\n# ---- W&B -------------------------------------------------------------------\nimport wandb\n\n\n# -----------------------------------------------------------------------------\n# Utility helpers\n# -----------------------------------------------------------------------------\n\ndef _setup_wandb(run_cfg: DictConfig, global_cfg: DictConfig, results_dir: Path) -> wandb.sdk.wandb_run.Run:\n    \"\"\"Initialise Weights & Biases and persist metadata to disk.\"\"\"\n    wandb_cfg = run_cfg.get(\"wandb\", {})\n    wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))\n    tags = wandb_cfg.get(\"tags\", [])\n    wandb_run = wandb.init(\n        entity=wandb_cfg.get(\"entity\", \"gengaru617\"),\n        project=wandb_cfg.get(\"project\", \"251014-test\"),\n        name=wandb_cfg.get(\"run_name\", run_cfg.run_id),\n        config=OmegaConf.to_container(run_cfg, resolve=True),\n        tags=tags,\n        reinit=True,\n    )\n\n    # Save metadata → .research/iterationX/wandb_metadata.json\n    iteration = os.environ.get(\"EXPERIMENT_ITERATION\", \"0\")\n    metadata_path = Path(f\".research/iteration{iteration}\")\n    metadata_path.mkdir(parents=True, exist_ok=True)\n    with open(metadata_path / \"wandb_metadata.json\", \"w\", encoding=\"utf8\") as fp:\n        json.dump({\n            \"wandb_entity\": wandb_run.entity,\n            \"wandb_project\": wandb_run.project,\n            \"wandb_run_id\": wandb_run.id,\n        }, fp, indent=2)\n\n    # Also symlink or copy to results_dir for convenience\n    try:\n        (results_dir / \"wandb_metadata.json\").write_text((metadata_path / \"wandb_metadata.json\").read_text())\n    except Exception:\n        pass\n\n    print(f\"[WandB] Run initialised: {wandb_run.url}\")\n    return wandb_run\n\n\n# -----------------------------------------------------------------------------\n# Train loop\n# -----------------------------------------------------------------------------\n\ndef train_one_epoch(model: nn.Module, loader, loss_fn, optimiser, device):\n    model.train()\n    running_loss, running_correct = 0.0, 0\n    n_samples = 0\n    for batch in loader:\n        optimiser.zero_grad()\n        inputs, labels = batch\n        if isinstance(inputs, (tuple, list)):\n            inputs = [x.to(device) for x in inputs]\n        else:\n            inputs = inputs.to(device)\n        labels = labels.to(device)\n        outputs = model(inputs)  # see model.forward signature\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        optimiser.step()\n\n        running_loss += loss.item() * labels.size(0)\n        if outputs.ndim == 2:  # classification\n            preds = outputs.argmax(1)\n            running_correct += (preds == labels).sum().item()\n        n_samples += labels.size(0)\n\n    epoch_loss = running_loss / n_samples\n    epoch_acc = running_correct / n_samples if running_correct else 0.0\n    return epoch_loss, epoch_acc\n\n\ndef evaluate(model: nn.Module, loader, loss_fn, device):\n    model.eval()\n    running_loss, running_correct = 0.0, 0\n    n_samples = 0\n    with torch.no_grad():\n        for batch in loader:\n            inputs, labels = batch\n            if isinstance(inputs, (tuple, list)):\n                inputs = [x.to(device) for x in inputs]\n            else:\n                inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, labels)\n            running_loss += loss.item() * labels.size(0)\n            if outputs.ndim == 2:\n                preds = outputs.argmax(1)\n                running_correct += (preds == labels).sum().item()\n            n_samples += labels.size(0)\n\n    epoch_loss = running_loss / n_samples\n    epoch_acc = running_correct / n_samples if running_correct else 0.0\n    return epoch_loss, epoch_acc\n\n\n# -----------------------------------------------------------------------------\n# Hydra entry point\n# -----------------------------------------------------------------------------\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    # ------------------------------------------------------------------\n    results_dir = Path(to_absolute_path(cfg.results_dir)).expanduser().absolute()\n    run_id: str = cfg.run\n    run_cfg_path = Path(__file__).resolve().parent.parent / \"config\" / \"run\" / f\"{run_id}.yaml\"\n    if not run_cfg_path.exists():\n        raise FileNotFoundError(f\"Run config not found: {run_cfg_path}\")\n\n    run_cfg: DictConfig = OmegaConf.load(run_cfg_path)\n    # Merge top-level overrides (like results_dir) back into run_cfg\n    merged_cfg = OmegaConf.merge(run_cfg, {\"results_dir\": str(results_dir)})\n\n    # ------------------------------------------------------------------\n    # Print experiment description BEFORE any numeric output\n    # ------------------------------------------------------------------\n    print(\"=\" * 80)\n    print(\"Experiment description\")\n    print(\"Run ID          :\", run_id)\n    print(\"Method          :\", merged_cfg.method)\n    print(\"Model           :\", merged_cfg.model)\n    print(\"Dataset         :\", merged_cfg.dataset.name)\n    print(\"Training params :\", merged_cfg.training)\n    print(\"=\" * 80)\n\n    # ------------------------------------------------------------------\n    # Prepare environment\n    # ------------------------------------------------------------------\n    torch.manual_seed(42)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    datamodule = DataModule(merged_cfg)\n    datamodule.setup()\n\n    model = build_model(merged_cfg).to(device)\n\n    # ------------------------------------------------------------------\n    # Optimiser / LR scheduler\n    # ------------------------------------------------------------------\n    train_cfg = merged_cfg.training\n    if train_cfg.optimizer == \"sgd\":\n        optimiser = optim.SGD(model.parameters(), lr=train_cfg.learning_rate, momentum=train_cfg.momentum, weight_decay=train_cfg.weight_decay)\n    elif train_cfg.optimizer in {\"adam\", \"adamw\"}:\n        optimiser_cls = optim.AdamW if train_cfg.optimizer == \"adamw\" else optim.Adam\n        optimiser = optimiser_cls(model.parameters(), lr=train_cfg.learning_rate, weight_decay=train_cfg.weight_decay)\n    else:\n        raise ValueError(f\"Unsupported optimiser: {train_cfg.optimizer}\")\n\n    if \"lr_scheduler\" in train_cfg and train_cfg.lr_scheduler.get(\"type\") == \"cosine\":\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=train_cfg.lr_scheduler.T_max)\n    else:\n        scheduler = None\n\n    # ------------------------------------------------------------------\n    # Criterion\n    # ------------------------------------------------------------------\n    num_classes = getattr(model, \"num_classes\", None)\n    loss_fn = nn.CrossEntropyLoss() if num_classes else nn.MSELoss()\n\n    # ------------------------------------------------------------------\n    # W&B\n    # ------------------------------------------------------------------\n    wandb_run = _setup_wandb(merged_cfg, cfg, results_dir)\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    history = []\n    best_val_acc = 0.0\n    epochs = train_cfg.epochs\n    for epoch in range(1, epochs + 1):\n        start_time = time.time()\n        train_loss, train_acc = train_one_epoch(model, datamodule.train_dataloader(), loss_fn, optimiser, device)\n        val_loss, val_acc = evaluate(model, datamodule.val_dataloader(), loss_fn, device)\n        if scheduler is not None:\n            scheduler.step()\n        epoch_time = time.time() - start_time\n\n        # Logging\n        metrics = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n            \"epoch_time\": epoch_time,\n        }\n        history.append(metrics)\n        wandb_run.log(metrics, step=epoch)\n        print(json.dumps({\"run_id\": run_id, **metrics}))\n\n        # Checkpoint best\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            ckpt_path = results_dir / run_id / \"best_model.pt\"\n            ckpt_path.parent.mkdir(parents=True, exist_ok=True)\n            torch.save(model.state_dict(), ckpt_path)\n            wandb_run.log_artifact(str(ckpt_path), name=f\"{run_id}-best-model\")\n\n    # ------------------------------------------------------------------\n    # Save final metrics\n    # ------------------------------------------------------------------\n    final_results: Dict[str, Any] = {\n        \"run_id\": run_id,\n        \"best_val_acc\": best_val_acc,\n        \"history\": history,\n        \"wandb_url\": wandb_run.url,\n    }\n    out_path = results_dir / run_id / \"results.json\"\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    out_path.write_text(json.dumps(final_results, indent=2))\n    print(json.dumps(final_results))\n\n    wandb_run.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "import json\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom hydra.utils import to_absolute_path\nfrom omegaconf import DictConfig\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport wandb\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    results_dir = Path(to_absolute_path(cfg.results_dir)).expanduser()\n    run_ids: List[str] = cfg.get(\"evaluate_run_ids\", []) or [p.name for p in results_dir.iterdir() if p.is_dir()]\n\n    records = []\n    for rid in run_ids:\n        res_path = results_dir / rid / \"results.json\"\n        if not res_path.exists():\n            print(f\"Warning: missing results for {rid}\")\n            continue\n        data = json.loads(res_path.read_text())\n        records.append({\"run_id\": rid, \"best_val_acc\": data.get(\"best_val_acc\", 0)})\n\n    df = pd.DataFrame.from_records(records)\n    print(json.dumps({\"comparison\": df.to_dict(orient=\"records\")}))\n\n    # ---- Figure -------------------------------------------------------------\n    fig, ax = plt.subplots(figsize=(10, 4))\n    df.sort_values(\"best_val_acc\", inplace=True)\n    ax.barh(df.run_id, df.best_val_acc)\n    ax.set_xlabel(\"Best Validation Accuracy\")\n    fig.tight_layout()\n\n    img_dir = results_dir / \"images\"\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / \"comparison.pdf\"\n    fig.savefig(fig_path)\n    print(f\"Saved comparison figure → {fig_path}\")\n\n    # Upload as W&B artifact where possible\n    if \"WANDB_API_KEY\" in os.environ:\n        wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n        run = wandb.init(project=\"251014-test\", entity=\"gengaru617\", name=\"evaluation\", reinit=True)\n        artifact = wandb.Artifact(\"comparison-figure\", type=\"figure\")\n        artifact.add_file(str(fig_path))\n        run.log_artifact(artifact)\n        run.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "from pathlib import Path\nfrom typing import List, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Subset\nfrom torch.utils.data import Dataset as TorchDataset\nimport torchvision.transforms as T\nfrom torchvision.datasets import CIFAR10\n\nfrom omegaconf import DictConfig\n\n# Optional heavy libs are imported lazily to keep memory footprint low\n\n# -----------------------------------------------------------------------------\n# Character-level tokenizer and utilities\n# -----------------------------------------------------------------------------\n\n_CHAR_VOCAB = list(\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{} \\\\t\\\\n\")\n_CHAR2IDX = {c: i + 1 for i, c in enumerate(_CHAR_VOCAB)}  # 0 → padding\n\n\ndef char_tokenize(text: str, max_len: int) -> torch.Tensor:\n    text = text.lower()[:max_len]\n    indices = [_CHAR2IDX.get(ch, 0) for ch in text]\n    if len(indices) < max_len:\n        indices.extend([0] * (max_len - len(indices)))\n    return torch.tensor(indices, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------\n# Alpaca dataset (huggingface datasets library)\n# -----------------------------------------------------------------------------\n\nclass AlpacaDataset(TorchDataset):\n    \"\"\"Alpaca dataset wrapper supporting char or sentencepiece tokenisation.\"\"\"\n\n    def __init__(self, cfg: DictConfig, split: str):\n        from datasets import load_dataset  # lazy import\n        self.cfg = cfg\n        name = cfg.dataset.name\n        self.raw = load_dataset(\"yahma/alpaca-cleaned\", split=split)\n\n        # Determine tokenizer type\n        self.tokeniser_type = cfg.dataset.tokenizer\n        if self.tokeniser_type == \"char\":\n            self.max_len = cfg.dataset.max_seq_length\n        else:\n            from transformers import DistilBertTokenizerFast\n            self.tokeniser = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n            self.max_len = cfg.dataset.max_seq_length\n\n    def __len__(self):\n        return len(self.raw)\n\n    def __getitem__(self, idx):\n        ex = self.raw[idx]\n        text = (ex[\"instruction\"].strip() + \" \" + ex[\"input\"].strip()).strip()\n        label = 1 if len(ex[\"output\"]) > 100 else 0  # simple heuristic label for demonstration\n\n        if self.tokeniser_type == \"char\":\n            return char_tokenize(text, self.max_len), torch.tensor(label, dtype=torch.long)\n        else:\n            tokens = self.tokeniser(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n            input_ids = tokens[\"input_ids\"].squeeze(0)\n            attention_mask = tokens[\"attention_mask\"].squeeze(0)\n            return (input_ids, attention_mask), torch.tensor(label, dtype=torch.long)\n\n\n# -----------------------------------------------------------------------------\n# DataModule\n# -----------------------------------------------------------------------------\n\nclass DataModule:\n    def __init__(self, cfg: DictConfig):\n        self.cfg = cfg\n        self.train_dataset = None\n        self.val_dataset = None\n        self.test_dataset = None\n\n    # ------------------------------------------------------------------\n    # Setup\n    # ------------------------------------------------------------------\n    def setup(self):\n        if self.cfg.dataset.name == \"CIFAR-10\":\n            self._setup_cifar10()\n        elif self.cfg.dataset.name == \"alpaca-cleaned\":\n            self._setup_alpaca()\n        else:\n            raise ValueError(f\"Unsupported dataset: {self.cfg.dataset.name}\")\n\n    # ------------------------------------------------------------------\n    # CIFAR\n    # ------------------------------------------------------------------\n    def _setup_cifar10(self):\n        cfg = self.cfg.dataset\n        mean, std = cfg.normalization.mean, cfg.normalization.std\n        train_aug = [T.RandomCrop(32, padding=4)] if cfg.augmentations.random_crop else []\n        if cfg.augmentations.random_flip == \"horizontal\":\n            train_aug.append(T.RandomHorizontalFlip())\n        train_aug += [T.ToTensor(), T.Normalize(mean, std)]\n        test_aug = [T.ToTensor(), T.Normalize(mean, std)]\n\n        full_train = CIFAR10(root=Path(\"data\"), train=True, download=True, transform=T.Compose(train_aug))\n        full_indices = list(range(len(full_train)))\n        train_count = cfg.splits.train\n        val_count = cfg.splits.val\n        train_indices = full_indices[:train_count]\n        val_indices = full_indices[train_count:train_count + val_count]\n\n        self.train_dataset = Subset(full_train, train_indices)\n        self.val_dataset = Subset(CIFAR10(root=Path(\"data\"), train=True, download=True, transform=T.Compose(test_aug)), val_indices)\n        self.test_dataset = CIFAR10(root=Path(\"data\"), train=False, download=True, transform=T.Compose(test_aug))\n\n    # ------------------------------------------------------------------\n    # Alpaca\n    # ------------------------------------------------------------------\n    def _setup_alpaca(self):\n        self.train_dataset = AlpacaDataset(self.cfg, split=\"train[:90%]\")\n        self.val_dataset = AlpacaDataset(self.cfg, split=\"train[90%:95%]\")\n        self.test_dataset = AlpacaDataset(self.cfg, split=\"train[95%:]\")\n\n    # ------------------------------------------------------------------\n    # Dataloaders\n    # ------------------------------------------------------------------\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.cfg.training.batch_size, shuffle=True, num_workers=2, collate_fn=self._collate)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.cfg.training.batch_size, shuffle=False, num_workers=2, collate_fn=self._collate)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.cfg.training.batch_size, shuffle=False, num_workers=2, collate_fn=self._collate)\n\n    # ------------------------------------------------------------------\n    # Collate\n    # ------------------------------------------------------------------\n    def _collate(self, batch: List):\n        if self.cfg.dataset.name == \"alpaca-cleaned\" and self.cfg.dataset.tokenizer != \"char\":\n            # ( (input_ids, attn_mask), label )\n            input_ids = torch.stack([item[0][0] for item in batch])\n            attn_mask = torch.stack([item[0][1] for item in batch])\n            labels = torch.stack([item[1] for item in batch])\n            return (input_ids, attn_mask), labels\n        else:\n            x = torch.stack([item[0] for item in batch]) if isinstance(batch[0][0], torch.Tensor) else [item[0] for item in batch]\n            y = torch.stack([item[1] for item in batch])\n            return x, y\n",
    "model_py": "import math\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n\nfrom transformers import DistilBertModel, DistilBertConfig\nfrom omegaconf import DictConfig\n\n# -----------------------------------------------------------------------------\n# MobileNetV2 based classifiers\n# -----------------------------------------------------------------------------\n\n\nclass CIFARMobileNetV2Classifier(nn.Module):\n    def __init__(self, num_classes: int, width_mult: float = 1.0, dropout: float = 0.0, pretrained: bool = False):\n        super().__init__()\n        weights = MobileNet_V2_Weights.DEFAULT if pretrained else None\n        self.backbone = mobilenet_v2(weights=weights, width_mult=width_mult)\n        self.backbone.classifier[1] = nn.Linear(self.backbone.last_channel, num_classes)\n        if dropout > 0:\n            self.backbone.classifier.insert(1, nn.Dropout(p=dropout))\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        return self.backbone(x)\n\n\n# -----------------------------------------------------------------------------\n# Char CNN adapter to MobileNetV2 for text inputs\n# -----------------------------------------------------------------------------\n\nclass CharCNNAdapter(nn.Module):\n    \"\"\"Convert a sequence of character indices into a 3x32x32 pseudo-image.\"\"\"\n\n    def __init__(self, vocab_size: int, embedding_dim: int):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.pool = nn.AdaptiveAvgPool1d(3072)  # 3*32*32\n\n    def forward(self, char_ids: torch.Tensor):  # [B, L]\n        emb = self.embedding(char_ids).permute(0, 2, 1)  # B x E x L\n        flat = self.pool(emb)  # B x E x 3072/E (E=embedding_dim) but we force output size\n        flat = flat.view(char_ids.size(0), 3, 32, 32)\n        return flat\n\n\nclass CharMobileNetClassifier(nn.Module):\n    def __init__(self, num_classes: int, vocab_size: int, embedding_dim: int, width_mult: float):\n        super().__init__()\n        self.adapter = CharCNNAdapter(vocab_size, embedding_dim)\n        self.mobilenet = CIFARMobileNetV2Classifier(num_classes=num_classes, width_mult=width_mult, pretrained=False)\n        self.num_classes = num_classes\n\n    def forward(self, char_ids):\n        imgs = self.adapter(char_ids)\n        return self.mobilenet(imgs)\n\n\n# -----------------------------------------------------------------------------\n# DistilBERT for images via patch embedding\n# -----------------------------------------------------------------------------\n\nclass LinearPatchEmbedding(nn.Module):\n    def __init__(self, in_channels: int, patch_size: int, emb_dim: int, img_size: int):\n        super().__init__()\n        self.patch_size = patch_size\n        num_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Linear(in_channels * patch_size * patch_size, emb_dim)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, emb_dim))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n\n    def forward(self, x):  # B x C x H x W\n        B, C, H, W = x.size()\n        p = self.patch_size\n        x = x.unfold(2, p, p).unfold(3, p, p)  # B C h w p p\n        x = x.contiguous().view(B, C, -1, p, p)  # B C N p p\n        x = x.flatten(3)  # B C N p*p\n        x = x.permute(0, 2, 1, 3).flatten(2)  # B N C*p*p\n        tokens = self.proj(x)  # B N D\n        cls = self.cls_token.expand(B, -1, -1)\n        tokens = torch.cat((cls, tokens), dim=1) + self.pos_embed\n        return tokens  # B seq D\n\n\nclass DistilBERTImageClassifier(nn.Module):\n    def __init__(self, num_classes: int, patch_size: int, img_size: int = 32):\n        super().__init__()\n        self.num_classes = num_classes\n        self.embed_dim = 768\n        self.patch_embed = LinearPatchEmbedding(3, patch_size, self.embed_dim, img_size)\n        self.transformer = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.classifier = nn.Linear(self.embed_dim, num_classes)\n\n    def forward(self, x):  # x images\n        embeds = self.patch_embed(x)  # B seq D\n        outputs = self.transformer(inputs_embeds=embeds)\n        cls_tok = outputs.last_hidden_state[:, 0]  # B D\n        logits = self.classifier(cls_tok)\n        return logits\n\n\n# -----------------------------------------------------------------------------\n# DistilBERT text classifier\n# -----------------------------------------------------------------------------\n\nclass DistilBERTTextClassifier(nn.Module):\n    def __init__(self, num_classes: int = 2):\n        super().__init__()\n        self.transformer = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_classes)\n        self.num_classes = num_classes\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]):\n        input_ids, attention_mask = inputs\n        out = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n        cls_tok = out.last_hidden_state[:, 0]\n        return self.classifier(cls_tok)\n\n\n# -----------------------------------------------------------------------------\n# Builder\n# -----------------------------------------------------------------------------\n\ndef build_model(cfg: DictConfig) -> nn.Module:\n    dataset_name = cfg.dataset.name\n    model_name = cfg.model.name.lower()\n\n    if model_name == \"mobilenetv2\":\n        if dataset_name == \"CIFAR-10\":\n            return CIFARMobileNetV2Classifier(num_classes=10,\n                                              width_mult=cfg.model.width_mult,\n                                              dropout=cfg.model.get(\"dropout\", 0.0),\n                                              pretrained=cfg.model.pretrained)\n        else:\n            # char-CNN adapter path\n            return CharMobileNetClassifier(num_classes=2,  # fixed for demo\n                                           vocab_size=len(_CHAR2IDX) + 1,\n                                           embedding_dim=cfg.model.input_adapter.embedding_dim,\n                                           width_mult=cfg.model.variant)\n    elif \"distilbert\" in model_name:\n        if dataset_name == \"CIFAR-10\":\n            return DistilBERTImageClassifier(num_classes=10, patch_size=cfg.model.modality_adapter.patch_size)\n        else:\n            return DistilBERTTextClassifier(num_classes=2)\n    else:\n        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "main_py": "import json\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom hydra.utils import to_absolute_path\nfrom omegaconf import DictConfig, OmegaConf\n\n# -----------------------------------------------------------------------------\n# Helpers\n# -----------------------------------------------------------------------------\n\ndef _stream_redirect(stream, paths: List[Path]):\n    \"\"\"Continuously read from `stream` and write to each file in `paths` and stdout.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        decoded = line.decode()\n        for p in paths:\n            p.write_text(decoded, append=True) if hasattr(p, \"write_text\") else p.write(decoded)\n        sys.stdout.write(decoded)\n    stream.close()\n\n\ndef _launch(run_id: str, results_dir: Path):\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    stdout_log = run_dir / \"stdout.log\"\n    stderr_log = run_dir / \"stderr.log\"\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\", f\"run={run_id}\", f\"results_dir={str(results_dir)}\"]\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # Redirect outputs using threads\n    threads = [\n        threading.Thread(target=_stream_redirect, args=(proc.stdout, [stdout_log])),\n        threading.Thread(target=_stream_redirect, args=(proc.stderr, [stderr_log])),\n    ]\n    for t in threads:\n        t.start()\n    for t in threads:\n        t.join()\n\n    proc.wait()\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Subprocess for {run_id} failed with return code {proc.returncode}\")\n\n\n# -----------------------------------------------------------------------------\n# Main Hydra entry point\n# -----------------------------------------------------------------------------\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    results_dir = Path(to_absolute_path(cfg.results_dir)).expanduser().absolute()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    if cfg.run == \"all\":\n        run_ids = cfg.runs_list\n    else:\n        run_ids = [cfg.run]\n\n    for run_id in run_ids:\n        print(f\"=== Running {run_id} ===\")\n        _launch(run_id, results_dir)\n\n    # After all runs, launch evaluation\n    subprocess.run([sys.executable, \"-u\", \"-m\", \"src.evaluate\", f\"results_dir={str(results_dir)}\"], check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=42\", \"wheel\"]\n\n[project]\nname = \"comparison_experiments\"\nversion = \"0.1.0\"\ndescription = \"Lightweight model comparison experiments with Hydra & W&B\"\nrequires-python = \">=3.8\"\ndependencies = [\n    \"torch>=2.0.0\",\n    \"torchvision>=0.15.0\",\n    \"hydra-core>=1.3.2\",\n    \"omegaconf>=2.3.0\",\n    \"transformers>=4.33.0\",\n    \"datasets>=2.12.0\",\n    \"wandb>=0.15.0\",\n    \"matplotlib>=3.7.0\",\n    \"pandas>=2.0.0\",\n]\n",
    "config_yaml": "runs_list:\n  - comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\n  - comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned\n  - comparative-2-DistilBERT-base-66M-CIFAR-10\n  - comparative-2-DistilBERT-base-66M-alpaca-cleaned\n\nrun: all  # default run selection; pass run={run_id} from CLI to override\nresults_dir: outputs\n\ndefaults: []\n\n# You can add global WandB defaults here if desired\nwandb:\n  entity: gengaru617\n  project: 251014-test\n  tags: [hydra, comparison]\n  run_name: null\n  "
}
