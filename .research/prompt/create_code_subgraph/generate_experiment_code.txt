
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/run/*.yaml` files
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation

## Hydra Configuration Structure
Each run config file (`config/run/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.

## Command Line Interface
The generated code must support the following CLI:
```bash
# Run a single experiment (specified by Hydra)
uv run python -u -m src.main run={run_id} results_dir={path}
```

The `run` argument specifies which experiment to run (matching a run_id from config/run/*.yaml).
The `results_dir` argument is passed from the GitHub Actions workflow and specifies where all outputs should be saved.

## Output Structure
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
- `src/train.py`: Logic to run a single experiment variation. Uses Hydra config to load parameters. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Complete preprocessing pipeline implementation for the specified datasets
- `src/model.py`: Complete model architecture implementations. It will contain classes for baseline, proposed, and ablation models. Implement all architectures from scratch.
- `src/main.py`: The main orchestrator script. It receives a run_id via Hydra, launches train.py for that specific experiment, manages subprocess, collects and consolidates logs, and finally triggers evaluate.py if needed.
- `pyproject.toml`: Complete project dependencies (include hydra-core, optuna if needed)
- `config/config.yaml`: Main Hydra configuration file that defines the list of all experiment run_ids


### Key Implementation Focus Areas
1. Hydra-Driven Configuration: All parameters loaded from run configs dynamically
2. Algorithm Core: Full implementation of the proposed method with proper abstraction
3. Sequential Execution: main.py executes run variations one at a time in sequential order
4. Configuration Driven: The entire workflow must be driven by the YAML configuration files
5. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete
6. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Code Validation Feedback




# Experimental Environment
cpu
RAM：500 MB

# Current Research Method (Target for Experiment Design)
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

# Experimental Design
- Strategy: 

# Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10
  Config File: config/run/comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  width_mult: 0.5
  dropout: 0.2
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
  augmentations:
    random_crop: true
    random_flip: horizontal
training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.05
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0005
  lr_scheduler:
    type: cosine
    T_max: 50
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 0.001
      high: 0.1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    momentum:
      type: uniform
      low: 0.7
      high: 0.95
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned
  Config File: config/run/comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  input_adapter:
    type: char_cnn
    embedding_dim: 256
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: char
training:
  epochs: 3
  batch_size: 16
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 0.01
  gradient_accumulation_steps: 2
optuna:
  n_trials: 15
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-1

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  Config File: config/run/comparative-2-DistilBERT-base-66M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-CIFAR-10
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
  modality_adapter:
    type: linear_patch_embedding
    patch_size: 4
    sequence_length: 64
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    patch_size:
      type: categorical
      choices: [2, 4, 8]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  Config File: config/run/comparative-2-DistilBERT-base-66M-alpaca-cleaned.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: sentencepiece
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  


# External Resources (Use these for implementation)



# ========================================
# WandB Integration Requirements
# ========================================

## Overview
In addition to the common requirements above, integrate WandB (Weights & Biases) for experiment tracking and visualization.

## Additional Requirements
- WANDB INTEGRATION: Initialize WandB with proper project/entity from config, log metrics/artifacts, and save wandb_run_id to metadata
- WANDB METADATA: Save WandB run information to `.research/iteration{experiment_iteration}/wandb_metadata.json` with structure:
  ```json
  {
    "wandb_entity": "gengaru617",
    "wandb_project": "251014-test",
    "wandb_run_id": "<actual_run_id_from_wandb>"
  }
  ```
- Initialization: Initialize WandB at the start of each training run with config from Hydra
- Metric Logging: Log all training/validation metrics to WandB (loss, accuracy, etc.)
- Artifact Tracking: Log model checkpoints and important artifacts
- Metadata Saving: After WandB initialization, save the wandb_run_id to `.research/iteration{experiment_iteration}/wandb_metadata.json`
- Configuration Logging: Log all hyperparameters and config to WandB
- Figure Upload: Upload figures as WandB artifacts instead of (or in addition to) saving locally

## WandB Configuration
- Entity: gengaru617
- Project: 251014-test

## Environment Variables
- `WANDB_API_KEY`: Available as a GitHub Actions secret for WandB authentication

## Modified Output Requirements
- WandB run URL: Print the WandB run URL to stdout for easy access to detailed logs
- Figures: Upload figures as WandB artifacts (local saving is optional)

## Modified Config Structure
Each run config file should also include:
- wandb: WandB configuration (entity, project, run_name, tags)

## Modified Dependencies
- pyproject.toml: Include `wandb` package

## WandB Metadata File Location
The wandb_metadata.json file must be saved to:
- Path: `.research/iteration{experiment_iteration}/wandb_metadata.json`
- The experiment_iteration should be passed as an environment variable or config parameter
- The file should be created immediately after WandB run initialization in train.py



Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "\"\"\"src/train.py\nThis module executes a single experiment run.  All parameters are provided through a\nHydra run-level configuration file located in ``config/run``.  The script\n\n1. loads the run configuration,\n2. builds the complete data pipeline (pre-processing + loaders),\n3. instantiates the requested model,\n4. trains / validates / tests it,\n5. tracks everything with in-depth Weights & Biases logging, and finally\n6. stores all relevant metrics in ``<results_dir>/<run_id>/results.json``.\n\nThe script prints a machine-readable JSON line **containing *all* numerical\nresults** as its last line on *stdout* so that automated tooling can reliably\npick it up.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport random\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, Tuple\n\nimport hydra\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport wandb\nfrom hydra.utils import to_absolute_path\nfrom omegaconf import DictConfig, OmegaConf\n\n# ────────────────────────────────────────────────────────────────────────────────\n# Utility helpers\n# ────────────────────────────────────────────────────────────────────────────────\n\ndef set_deterministic(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n@torch.no_grad()\ndef evaluate_classification(model: nn.Module, loader: torch.utils.data.DataLoader, device: torch.device) -> float:\n    \"\"\"Return *top-1* accuracy.\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    criterion = nn.CrossEntropyLoss()\n    running_loss = 0.0\n    for batch in loader:\n        inputs, targets = batch[\"inputs\"].to(device), batch[\"labels\"].to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        running_loss += loss.item() * inputs.size(0)\n        preds = outputs.argmax(dim=1)\n        correct += (preds == targets).sum().item()\n        total += targets.size(0)\n    acc = correct / total * 100.0\n    return acc, running_loss / total\n\n\n@torch.no_grad()\ndef evaluate_lm(model: nn.Module, loader: torch.utils.data.DataLoader, device: torch.device, pad_id: int) -> float:\n    \"\"\"Return *perplexity* for language-model‐style tasks.\"\"\"\n    model.eval()\n    criterion = nn.CrossEntropyLoss(ignore_index=pad_id, reduction=\"sum\")\n    total_tokens = 0\n    total_loss = 0.0\n    for batch in loader:\n        inputs = batch[\"inputs\"].to(device)\n        targets = batch[\"targets\"].to(device)\n        logits = model(inputs)\n        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n        total_loss += loss.item()\n        total_tokens += (targets != pad_id).sum().item()\n    ppl = np.exp(total_loss / total_tokens)\n    return float(ppl)\n\n\n# ────────────────────────────────────────────────────────────────────────────────\n# Main experiment entry-point\n# ────────────────────────────────────────────────────────────────────────────────\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -> None:  # pragma: no cover – entry-point\n    \"\"\"Single experiment run controlled via *Hydra*.\n\n    The top-level Hydra config only carries two keys – ``run`` and\n    ``results_dir``.  Everything else is provided by the dedicated run-level\n    YAML file that we load *manually* from ``config/run``.\n    \"\"\"\n    # ---------------------------------------------------------------------\n    # Locate and parse the run-specific configuration file\n    # ---------------------------------------------------------------------\n    run_id: str = cfg.run\n    base_dir = Path(__file__).resolve().parent.parent\n    run_cfg_file = base_dir / \"config\" / \"run\" / f\"{run_id}.yaml\"\n    if not run_cfg_file.exists():\n        raise FileNotFoundError(f\"Run-configuration file not found: {run_cfg_file}\")\n\n    run_cfg: DictConfig = OmegaConf.load(run_cfg_file)\n    # Merge command-line overrides (e.g. results_dir)\n    run_cfg.results_dir = to_absolute_path(cfg.results_dir)\n\n    # Ensure deterministic behaviour *before* touching anything else\n    seed = run_cfg.get(\"seed\", 42)\n    set_deterministic(seed)\n\n    # ------------------------------------------------------------------\n    # I/O setup\n    # ------------------------------------------------------------------\n    out_dir = Path(run_cfg.results_dir) / run_id\n    out_dir.mkdir(parents=True, exist_ok=True)\n    # Persist a copy of the exact configuration that produced this run\n    with open(out_dir / \"config.yaml\", \"w\") as fp:\n        OmegaConf.save(config=run_cfg, f=fp.name)\n\n    # ------------------------------------------------------------------\n    # Device configuration\n    # ------------------------------------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # Build the complete data pipeline (pre-processing + DLs)\n    # ------------------------------------------------------------------\n    from src.preprocess import get_dataloaders  # local import avoids circularity\n\n    train_loader, val_loader, test_loader, tokenizer = get_dataloaders(run_cfg)\n\n    # ------------------------------------------------------------------\n    # Model instantiation\n    # ------------------------------------------------------------------\n    from src.model import create_model\n\n    num_classes = getattr(train_loader.dataset, \"num_classes\", None)\n    vocab_size = getattr(tokenizer, \"vocab_size\", None)\n\n    model = create_model(run_cfg, num_classes=num_classes, vocab_size=vocab_size)\n    model.to(device)\n\n    # ------------------------------------------------------------------\n    # Optimiser & LR scheduler\n    # ------------------------------------------------------------------\n    training_cfg = run_cfg.training\n    if training_cfg.optimizer.lower() == \"sgd\":\n        optimizer = optim.SGD(\n            model.parameters(),\n            lr=training_cfg.learning_rate,\n            momentum=training_cfg.get(\"momentum\", 0.0),\n            weight_decay=training_cfg.weight_decay,\n        )\n    elif training_cfg.optimizer.lower() in {\"adam\", \"adamw\"}:\n        optim_class = optim.AdamW if training_cfg.optimizer.lower() == \"adamw\" else optim.Adam\n        optimizer = optim_class(\n            model.parameters(),\n            lr=training_cfg.learning_rate,\n            weight_decay=training_cfg.weight_decay,\n        )\n    else:\n        raise ValueError(f\"Unsupported optimizer {training_cfg.optimizer}\")\n\n    # LR scheduler (optional)\n    scheduler = None\n    if \"lr_scheduler\" in training_cfg and training_cfg.lr_scheduler is not None:\n        sched_cfg = training_cfg.lr_scheduler\n        if sched_cfg.type == \"cosine\":\n            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=sched_cfg.T_max)\n        elif sched_cfg.type == \"linear\":\n            total_steps = len(train_loader) * training_cfg.epochs\n            warmup_steps = sched_cfg.get(\"warmup_steps\", 0)\n            def lr_lambda(current_step: int):\n                if current_step < warmup_steps:\n                    return float(current_step) / float(max(1, warmup_steps))\n                return max(0.0, float(total_steps - current_step) / float(max(1, total_steps - warmup_steps)))\n            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n        else:\n            raise ValueError(f\"Unsupported scheduler type: {sched_cfg.type}\")\n\n    # Criterion is chosen dynamically\n    if num_classes is not None:\n        criterion = nn.CrossEntropyLoss()\n    else:\n        pad_id = tokenizer.pad_id  # type: ignore[attr-defined]\n        criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n\n    # ------------------------------------------------------------------\n    # Weights & Biases initialisation – must happen *after* we have the full cfg\n    # ------------------------------------------------------------------\n    wandb_cfg: Dict[str, Any] = run_cfg.get(\"wandb\", {})\n    wandb_entity = wandb_cfg.get(\"entity\", \"gengaru617\")\n    wandb_project = wandb_cfg.get(\"project\", \"251014-test\")\n    wandb_run = wandb.init(\n        project=wandb_project,\n        entity=wandb_entity,\n        name=run_id,\n        config=OmegaConf.to_container(run_cfg, resolve=True),\n        reinit=False,\n    )\n\n    # Persist WandB metadata in the prescribed location\n    experiment_iteration = os.getenv(\"EXPERIMENT_ITERATION\", \"1\")\n    meta_dir = Path(\".research\") / f\"iteration{experiment_iteration}\"\n    meta_dir.mkdir(parents=True, exist_ok=True)\n    with open(meta_dir / \"wandb_metadata.json\", \"w\") as fp:\n        json.dump(\n            {\n                \"wandb_entity\": wandb_entity,\n                \"wandb_project\": wandb_project,\n                \"wandb_run_id\": wandb_run.id,\n            },\n            fp,\n            indent=2,\n        )\n\n    print(f\"WandB URL: {wandb_run.url}\")\n\n    # ------------------------------------------------------------------\n    #                               Training loop\n    # ------------------------------------------------------------------\n    best_val_score = -float(\"inf\") if num_classes is not None else float(\"inf\")\n    history = []  # keeps per-epoch metrics for output\n\n    for epoch in range(1, training_cfg.epochs + 1):\n        model.train()\n        epoch_loss = 0.0\n        epoch_correct = 0\n        epoch_total = 0\n        start_time = time.time()\n\n        for step, batch in enumerate(train_loader, 1):\n            optimizer.zero_grad()\n\n            if num_classes is not None:  # image / classification branch\n                inputs = batch[\"inputs\"].to(device)\n                labels = batch[\"labels\"].to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                epoch_loss += loss.item() * inputs.size(0)\n                preds = outputs.argmax(dim=1)\n                epoch_correct += (preds == labels).sum().item()\n                epoch_total += labels.size(0)\n\n            else:  # language-model branch\n                inputs = batch[\"inputs\"].to(device)\n                targets = batch[\"targets\"].to(device)\n                logits = model(inputs)\n                loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n                loss.backward()\n                optimizer.step()\n\n                valid_tokens = (targets != pad_id).sum().item()\n                epoch_loss += loss.item()\n                epoch_total += valid_tokens\n\n            if scheduler is not None:\n                scheduler.step()\n\n        epoch_time = time.time() - start_time\n        if num_classes is not None:\n            train_acc = epoch_correct / epoch_total * 100.0\n            train_metric = train_acc\n        else:\n            train_ppl = np.exp(epoch_loss / epoch_total)\n            train_metric = train_ppl\n\n        # --------------------------------------------------------------\n        # Validation\n        # --------------------------------------------------------------\n        if num_classes is not None:\n            val_acc, val_loss = evaluate_classification(model, val_loader, device)\n            current_val_metric = val_acc\n        else:\n            val_ppl = evaluate_lm(model, val_loader, device, pad_id)\n            val_loss = None\n            current_val_metric = -val_ppl  # lower is better → negate\n\n        # Save best model\n        if current_val_metric > best_val_score:\n            best_val_score = current_val_metric\n            torch.save(model.state_dict(), out_dir / \"best_model.pt\")\n\n        # WandB logging -------------------------------------------------\n        log_dict = {\n            \"epoch\": epoch,\n            \"train_metric\": train_metric,\n            \"val_metric\": -current_val_metric if num_classes is None else current_val_metric,\n            \"train_loss\": epoch_loss / epoch_total if num_classes is None else epoch_loss / len(train_loader.dataset),\n            \"epoch_time\": epoch_time,\n        }\n        wandb.log(log_dict, step=epoch)\n        history.append(log_dict)\n\n        print(json.dumps({\"run_id\": run_id, \"epoch\": epoch, **log_dict}))\n        sys.stdout.flush()\n\n    # ------------------------------------------------------------------\n    #                               Test set\n    # ------------------------------------------------------------------\n    if num_classes is not None:\n        test_acc, _ = evaluate_classification(model, test_loader, device)\n        test_metric = test_acc\n    else:\n        test_ppl = evaluate_lm(model, test_loader, device, pad_id)\n        test_metric = test_ppl\n    wandb.log({\"test_metric\": test_metric})\n\n    # Save final artefacts ------------------------------------------------\n    artefact = wandb.Artifact(name=f\"{run_id}-model\", type=\"model\")\n    artefact.add_file(out_dir / \"best_model.pt\")\n    wandb.run.log_artifact(artefact)\n\n    # ------------------------------------------------------------------\n    # Persist *all* numerical results in a structured JSON file\n    # ------------------------------------------------------------------\n    results: Dict[str, Any] = {\n        \"run_id\": run_id,\n        \"best_val_metric\": -best_val_score if num_classes is None else best_val_score,\n        \"test_metric\": test_metric,\n        \"epochs\": training_cfg.epochs,\n        \"seed\": seed,\n        \"device\": str(device),\n        \"history\": history,\n    }\n\n    with open(out_dir / \"results.json\", \"w\") as fp:\n        json.dump(results, fp, indent=2)\n\n    # Final console output – machine-readable --------------------------------------------------\n    print(json.dumps(results))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "\"\"\"src/evaluate.py\nAggregates the ``results.json`` files produced by individual experiment runs,\nprints a machine-readable summary **and** uploads a comparison figure to WandB.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport hydra\nimport matplotlib.pyplot as plt\nimport wandb\nfrom omegaconf import DictConfig\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -> None:  # pragma: no cover – entry-point\n    results_dir = Path(cfg.results_dir)\n    if not results_dir.exists():\n        raise FileNotFoundError(results_dir)\n\n    summaries: List[Dict[str, Any]] = []\n    for res_file in results_dir.rglob(\"results.json\"):\n        with open(res_file) as fp:\n            summaries.append(json.load(fp))\n\n    if not summaries:\n        print(\"{}\")\n        return\n\n    # Build bar-chart of test metrics\n    run_ids = [s[\"run_id\"] for s in summaries]\n    metrics = [s[\"test_metric\"] for s in summaries]\n\n    fig, ax = plt.subplots(figsize=(max(6, len(run_ids)), 4))\n    bars = ax.bar(run_ids, metrics, color=\"skyblue\")\n    ax.set_xticklabels(run_ids, rotation=45, ha=\"right\")\n    ax.set_ylabel(\"Test metric (accuracy ↑ / perplexity ↓)\")\n    ax.set_title(\"Experimental comparison across all runs\")\n\n    for bar, metric in zip(bars, metrics):\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width() / 2, height, f\"{metric:.2f}\", ha=\"center\", va=\"bottom\")\n\n    fig.tight_layout()\n    fig_path = results_dir / \"comparison.png\"\n    fig.savefig(fig_path)\n\n    # Upload to WandB as artefact if in a WandB run context\n    if wandb.run is not None:\n        artefact = wandb.Artifact(name=\"comparison-figure\", type=\"figure\")\n        artefact.add_file(str(fig_path))\n        wandb.run.log_artifact(artefact)\n\n    # Console output -----------------------------------------------------\n    output = {\n        \"num_runs\": len(summaries),\n        \"runs\": {s[\"run_id\"]: s[\"test_metric\"] for s in summaries},\n    }\n    print(json.dumps(output))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"src/preprocess.py\nUniversal data-pipeline factory.  Returns domain-specific *DataLoaders* plus an\noptional *tokenizer* for language tasks.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport random\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision.datasets import CIFAR10\n\n# ────────────────────────────────────────────────────────────────────────────────\n# Tokeniser for character-level language modelling\n# ────────────────────────────────────────────────────────────────────────────────\n\nclass CharTokenizer:\n    def __init__(self) -> None:\n        # Printable ASCII plus a handful of specials\n        ascii_chars = [chr(i) for i in range(32, 127)]\n        self.pad_token = \"<pad>\"\n        self.bos_token = \"<bos>\"\n        self.eos_token = \"<eos>\"\n        self.idx2char = [self.pad_token, self.bos_token, self.eos_token] + ascii_chars\n        self.char2idx = {ch: idx for idx, ch in enumerate(self.idx2char)}\n        self.pad_id = self.char2idx[self.pad_token]\n        self.bos_id = self.char2idx[self.bos_token]\n        self.eos_id = self.char2idx[self.eos_token]\n\n    def encode(self, text: str, add_special_tokens: bool = True) -> list[int]:\n        ids = [self.char2idx.get(ch, self.eos_id) for ch in text]\n        if add_special_tokens:\n            ids = [self.bos_id] + ids + [self.eos_id]\n        return ids\n\n    def decode(self, ids: list[int], skip_special_tokens: bool = True) -> str:\n        chars = []\n        for idx in ids:\n            ch = self.idx2char[idx]\n            if skip_special_tokens and ch in {self.pad_token, self.bos_token, self.eos_token}:\n                continue\n            chars.append(ch)\n        return \"\".join(chars)\n\n    @property\n    def vocab_size(self) -> int:\n        return len(self.idx2char)\n\n\n# ────────────────────────────────────────────────────────────────────────────────\n# Language-modelling dataset (Alpaca-cleaned)\n# ────────────────────────────────────────────────────────────────────────────────\n\nclass AlpacaCharDataset(Dataset):\n    def __init__(self, file_path: str | Path, tokenizer: CharTokenizer, max_seq_len: int = 512):\n        self.samples = []\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n        # ------------------------------------------------------------------\n        # Expected file format: one *JSON* object per line with fields\n        #   \"instruction\", \"input\", \"output\"      (same as the original dataset)\n        # ------------------------------------------------------------------\n        with open(file_path) as fp:\n            for line in fp:\n                obj = json.loads(line)\n                prompt = obj.get(\"instruction\", \"\") + \"\\n\" + obj.get(\"input\", \"\")\n                target = obj.get(\"output\", \"\")\n                # We train a *causal* LM: feed prompt+bos, predict full target+eos\n                src_ids = tokenizer.encode(prompt)\n                tgt_ids = tokenizer.encode(target)\n                ids = (src_ids + tgt_ids)[: max_seq_len]\n                self.samples.append(ids)\n\n    def __len__(self) -> int:\n        return len(self.samples)\n\n    def __getitem__(self, idx: int):\n        ids = self.samples[idx]\n        inp = ids[:-1]\n        tgt = ids[1:]\n        return {\n            \"input_ids\": torch.tensor(inp, dtype=torch.long),\n            \"target_ids\": torch.tensor(tgt, dtype=torch.long),\n        }\n\n    @property\n    def num_classes(self):  # not a classification task\n        return None\n\n\ndef alpaca_collate(batch):\n    pad_id = CharTokenizer().pad_id  # instantiate temporary to get pad id\n    max_len = max(item[\"input_ids\"].size(0) for item in batch)\n    inp_tensor = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    tgt_tensor = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n    for i, item in enumerate(batch):\n        n = item[\"input_ids\"].size(0)\n        inp_tensor[i, :n] = item[\"input_ids\"]\n        tgt_tensor[i, :n] = item[\"target_ids\"]\n    return {\"inputs\": inp_tensor, \"targets\": tgt_tensor}\n\n\n# ────────────────────────────────────────────────────────────────────────────────\n# Computer-vision (CIFAR-10) helpers\n# ────────────────────────────────────────────────────────────────────────────────\n\ndef build_cifar_transforms(cfg_dataset) -> Tuple[T.Compose, T.Compose]:\n    mean = cfg_dataset.normalization.mean\n    std = cfg_dataset.normalization.std\n    img_size = cfg_dataset.image_size\n\n    train_tf = [T.RandomCrop(img_size, padding=4)] if cfg_dataset.augmentations.random_crop else []\n    if cfg_dataset.augmentations.random_flip == \"horizontal\":\n        train_tf.append(T.RandomHorizontalFlip())\n    train_tf += [T.ToTensor(), T.Normalize(mean, std)]\n\n    val_tf = [T.ToTensor(), T.Normalize(mean, std)]\n    return T.Compose(train_tf), T.Compose(val_tf)\n\n\ndef get_cifar10_dataloaders(cfg_run) -> Tuple[DataLoader, DataLoader, DataLoader]:\n    dataset_cfg = cfg_run.dataset\n    training_cfg = cfg_run.training\n\n    train_tf, test_tf = build_cifar_transforms(dataset_cfg)\n    data_root = Path(\"data\")\n    train_full = CIFAR10(root=data_root, train=True, download=True, transform=train_tf)\n    test_set = CIFAR10(root=data_root, train=False, download=True, transform=test_tf)\n\n    # Train / val split as specified\n    train_size = dataset_cfg.splits.train\n    val_size = dataset_cfg.splits.val\n    assert train_size + val_size == len(train_full), \"Train/val split does not match dataset length\".\n    train_set, val_set = random_split(train_full, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n\n    dl_kwargs = dict(batch_size=training_cfg.batch_size, num_workers=2, pin_memory=True)\n    train_loader = DataLoader(train_set, shuffle=True, **dl_kwargs, collate_fn=lambda b: {\"inputs\": torch.stack([x[0] for x in b]), \"labels\": torch.tensor([x[1] for x in b])})\n    val_loader = DataLoader(val_set, shuffle=False, **dl_kwargs, collate_fn=lambda b: {\"inputs\": torch.stack([x[0] for x in b]), \"labels\": torch.tensor([x[1] for x in b])})\n    test_loader = DataLoader(test_set, shuffle=False, **dl_kwargs, collate_fn=lambda b: {\"inputs\": torch.stack([x[0] for x in b]), \"labels\": torch.tensor([x[1] for x in b])})\n\n    return train_loader, val_loader, test_loader\n\n\n# ────────────────────────────────────────────────────────────────────────────────\n# Public factory\n# ────────────────────────────────────────────────────────────────────────────────\n\ndef get_dataloaders(cfg_run):\n    if cfg_run.dataset.name == \"CIFAR-10\":\n        train, val, test = get_cifar10_dataloaders(cfg_run)\n        return train, val, test, None  # no tokenizer\n    elif cfg_run.dataset.name == \"alpaca-cleaned\":\n        tokenizer = CharTokenizer()\n        file_path = cfg_run.dataset.get(\"file_path\", \"data/alpaca-cleaned.json\")\n        ds_full = AlpacaCharDataset(file_path, tokenizer, cfg_run.dataset.max_seq_length)\n        n_total = len(ds_full)\n        train_size = int(0.9 * n_total)\n        val_size = int(0.05 * n_total)\n        test_size = n_total - train_size - val_size\n        train_set, val_set, test_set = random_split(ds_full, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n        dl_kwargs = dict(batch_size=cfg_run.training.batch_size, shuffle=True, num_workers=2, pin_memory=True, collate_fn=alpaca_collate)\n        return (\n            DataLoader(train_set, **dl_kwargs),\n            DataLoader(val_set, **{**dl_kwargs, \"shuffle\": False}),\n            DataLoader(test_set, **{**dl_kwargs, \"shuffle\": False}),\n            tokenizer,\n        )\n    else:\n        raise ValueError(f\"Unsupported dataset: {cfg_run.dataset.name}\")\n",
    "model_py": "\"\"\"src/model.py\nAll *model* building blocks live in this file.  The public factory function\n``create_model`` inspects the run configuration and returns a ready-to-train\n``torch.nn.Module`` instance.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as tvm\nfrom transformers import DistilBertConfig, DistilBertModel\n\n# ────────────────────────────────────────────────────────────────────────────────\n# Character-level language-model (baseline for *alpaca-cleaned*)\n# ────────────────────────────────────────────────────────────────────────────────\n\nclass CharRNNLM(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int = 256, hidden_dim: int = 512, num_layers: int = 2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:  # x: [B, T]\n        emb = self.embed(x)\n        out, _ = self.rnn(emb)\n        logits = self.fc(out)\n        return logits  # [B, T, V]\n\n\n# ────────────────────────────────────────────────────────────────────────────────\n# MobileNetV2 (image classification)\n# ────────────────────────────────────────────────────────────────────────────────\n\nclass MobileNetV2Classifier(nn.Module):\n    def __init__(self, num_classes: int, width_mult: float = 1.0, dropout: float = 0.2, pretrained: bool = False):\n        super().__init__()\n        mobilenet = tvm.mobilenet_v2(pretrained=pretrained, width_mult=width_mult)\n        in_features = mobilenet.classifier[1].in_features\n        features = list(mobilenet.classifier.children())[:-1]  # drop original head\n        self.features = nn.Sequential(mobilenet.features, nn.Dropout(dropout))\n        self.classifier = nn.Linear(in_features, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        return self.classifier(x)\n\n\n# ────────────────────────────────────────────────────────────────────────────────\n# Image → patch → DistilBERT classifier (*vision-language bridging*)\n# ────────────────────────────────────────────────────────────────────────────────\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size: int = 32, patch_size: int = 4, in_chans: int = 3, embed_dim: int = 768):\n        super().__init__()\n        assert img_size % patch_size == 0, \"Img size must be divisible by patch size\"\n        self.num_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:  # x: [B, C, H, W]\n        x = self.proj(x)  # [B, E, H', W']\n        x = x.flatten(2).transpose(1, 2)  # [B, N, E]\n        return x\n\n\nclass DistilBertPatchClassifier(nn.Module):\n    def __init__(\n        self,\n        num_classes: int,\n        patch_size: int = 4,\n        img_size: int = 32,\n        sequence_length: int = 64,\n        pretrained: bool = True,\n    ):\n        super().__init__()\n        embed_dim = 768\n        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n        bert_cfg = DistilBertConfig()\n        self.bert = DistilBertModel(bert_cfg) if not pretrained else DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.cls_head = nn.Linear(embed_dim, num_classes)\n        # Positional embedding so that seq_len matches\n        self.pos_embed = nn.Parameter(torch.zeros(1, sequence_length, embed_dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        tokens = self.patch_embed(x)\n        # Resize / interpolate positional embedding if needed\n        if tokens.size(1) != self.pos_embed.size(1):\n            pos = nn.functional.interpolate(self.pos_embed.permute(0, 2, 1), size=tokens.size(1), mode=\"nearest\")\n            pos = pos.permute(0, 2, 1)\n        else:\n            pos = self.pos_embed\n        out = self.bert(inputs_embeds=tokens + pos).last_hidden_state[:, 0]  # CLS token = first token embedding\n        return self.cls_head(out)\n\n\n# ────────────────────────────────────────────────────────────────────────────────\n# Public model factory helper\n# ────────────────────────────────────────────────────────────────────────────────\n\ndef create_model(cfg_run, *, num_classes: Optional[int], vocab_size: Optional[int]):\n    model_cfg = cfg_run.model\n\n    # ------------------------------------------------------------------\n    # Vision branch (MobileNetV2)\n    # ------------------------------------------------------------------\n    if model_cfg.name.lower().startswith(\"mobilenetv2\"):\n        if cfg_run.dataset.name == \"CIFAR-10\":\n            assert num_classes is not None\n            return MobileNetV2Classifier(\n                num_classes=num_classes,\n                width_mult=model_cfg.width_mult,\n                dropout=model_cfg.dropout,\n                pretrained=model_cfg.pretrained,\n            )\n        else:  # text domain → fall back to simple CharRNNLM\n            assert vocab_size is not None, \"Text task requires vocab size\"\n            adapter_cfg = model_cfg.input_adapter\n            return CharRNNLM(vocab_size, embed_dim=adapter_cfg.embedding_dim)\n\n    # ------------------------------------------------------------------\n    # DistilBERT variants\n    # ------------------------------------------------------------------\n    if model_cfg.name.lower().startswith(\"distilbert\"):\n        if cfg_run.dataset.name == \"alpaca-cleaned\":\n            # Standard DistilBERT LM head fine-tuning for text\n            assert vocab_size is not None\n            bert_cfg = DistilBertConfig(vocab_size=vocab_size)\n            model = torch.hub.load(\"huggingface/pytorch-transformers\", \"modelForMaskedLM\", \"distilbert-base-uncased\")\n            # Resize embeddings in case vocab differs\n            model.resize_token_embeddings(vocab_size)\n            return model\n        else:  # CIFAR-10 via patch embedding\n            assert num_classes is not None\n            return DistilBertPatchClassifier(\n                num_classes=num_classes,\n                patch_size=model_cfg.modality_adapter.patch_size,\n                sequence_length=model_cfg.modality_adapter.sequence_length,\n                pretrained=model_cfg.pretrained,\n            )\n\n    raise ValueError(f\"Unsupported model configuration: {model_cfg}\")\n",
    "main_py": "\"\"\"src/main.py\nOrchestrator that launches *train.py* as a dedicated subprocess, pipes all\nstdout/stderr into per-run log files **and** the console, and finally invokes\n``evaluate.py`` for aggregated analysis.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import TextIO\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\ndef tee(stream: TextIO, *targets: TextIO):  # helper to replicate `tee(1)`\n    for line in iter(stream.readline, \"\"):\n        for tgt in targets:\n            tgt.write(line)\n            tgt.flush()\n    stream.close()\n\n\n@hydra.main(version_base=None, config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -> None:  # pragma: no cover – entry-point\n    run_id = cfg.run\n    results_dir = Path(cfg.results_dir).absolute()\n\n    run_out_dir = results_dir / run_id\n    run_out_dir.mkdir(parents=True, exist_ok=True)\n\n    stdout_log = open(run_out_dir / \"stdout.log\", \"w\")\n    stderr_log = open(run_out_dir / \"stderr.log\", \"w\")\n\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={run_id}\",\n        f\"results_dir={results_dir}\",\n    ]\n    print(\"Executing:\", \" \".join(cmd))\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n\n    # Launch tee threads for stdout/stderr\n    threads = [\n        threading.Thread(target=tee, args=(proc.stdout, sys.stdout, stdout_log)),\n        threading.Thread(target=tee, args=(proc.stderr, sys.stderr, stderr_log)),\n    ]\n    for t in threads:\n        t.start()\n\n    proc.wait()\n    for t in threads:\n        t.join()\n    stdout_log.close()\n    stderr_log.close()\n\n    if proc.returncode != 0:\n        raise subprocess.CalledProcessError(proc.returncode, cmd)\n\n    # Optionally run evaluator ---------------------------------------------------\n    eval_cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.evaluate\",\n        f\"results_dir={results_dir}\",\n    ]\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=42\", \"wheel\"]\n\n[project]\nname = \"research-experiment\"\nversion = \"0.1.0\"\ndescription = \"Hydra-managed research experiments with WandB tracking\"\nauthors = [\n    {name = \"AI Generated\", email = \"ai@example.com\"}\n]\nrequires-python = \">=3.8\"\ndependencies = [\n    \"torch>=1.13\",\n    \"torchvision>=0.14\",\n    \"transformers>=4.35\",\n    \"omegaconf>=2.3\",\n    \"hydra-core>=1.3\",\n    \"wandb>=0.15\",\n    \"matplotlib>=3.7\",\n]\n\n",
    "config_yaml": "# config/config.yaml\n# Global Hydra configuration.  Only two command-line overrides are expected:\n#    1. `run`          – identifier of the experiment to run (must match YAML in config/run)\n#    2. `results_dir`  – output directory path provided by CI or user\n\nrun: \"\"\nresults_dir: \"results\"\n"
}
