
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/run/*.yaml` files
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation

## Hydra Configuration Structure
Each run config file (`config/run/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.

## Command Line Interface
The generated code must support the following CLI:
```bash
# Run a single experiment (specified by Hydra)
uv run python -u -m src.main run={run_id} results_dir={path}
```

The `run` argument specifies which experiment to run (matching a run_id from config/run/*.yaml).
The `results_dir` argument is passed from the GitHub Actions workflow and specifies where all outputs should be saved.

## Output Structure
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
- `src/train.py`: Logic to run a single experiment variation. Uses Hydra config to load parameters. It is called as a subprocess by main.py. It must save final metrics to a structured file (e.g., results.json).
- `src/evaluate.py`: Comparison and visualization tool. It reads the result files from all experiment variations and generates comparison figures.
- `src/preprocess.py`: Complete preprocessing pipeline implementation for the specified datasets
- `src/model.py`: Complete model architecture implementations. It will contain classes for baseline, proposed, and ablation models. Implement all architectures from scratch.
- `src/main.py`: The main orchestrator script. It receives a run_id via Hydra, launches train.py for that specific experiment, manages subprocess, collects and consolidates logs, and finally triggers evaluate.py if needed.
- `pyproject.toml`: Complete project dependencies (include hydra-core, optuna if needed)
- `config/config.yaml`: Main Hydra configuration file that defines the list of all experiment run_ids


### Key Implementation Focus Areas
1. Hydra-Driven Configuration: All parameters loaded from run configs dynamically
2. Algorithm Core: Full implementation of the proposed method with proper abstraction
3. Sequential Execution: main.py executes run variations one at a time in sequential order
4. Configuration Driven: The entire workflow must be driven by the YAML configuration files
5. Evaluation Consistency: Identical metrics calculation, result formatting, and comparison logic. evaluate.py must operate on the saved results after all training is complete
6. Structured Logging:
   - train.py: Print JSON-formatted experimental data (epoch-wise metrics, final results) to stdout using `print(json.dumps({...}))`. Always include `"run_id"` field (use the run variation name from config).
   - evaluate.py: Print JSON-formatted comparison results to stdout
   - main.py: For each subprocess, redirect stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `{results_dir}/{run_id}/stderr.log` while also forwarding to main process stdout/stderr (using tee-like logic) so logs are captured both structurally and by GitHub Actions.


## Code Validation Feedback


**Previous Validation Issue**: The code is mostly functional but several core-validation requirements are violated:

1. WandB integration (Req. 9) – project/entity are hard-coded inside train.py / evaluate.py instead of being read from the Hydra config.  Configuration keys are missing, so runs cannot be redirected to another project without code edits.

2. Evaluation figures (Req. 8) – figures must be stored in {results_dir}/images/ and include legends/annotations.  evaluate.py saves comparison_accuracy.png directly in results_dir and does not add a legend.

3. tee_subprocess implementation in main.py reads stdout completely before stderr, which can dead-lock if stderr fills the OS pipe (production-readiness issue under Req. 1 & 6).

4. Config structure – hydra config/config.yaml lacks the wandb section expected by Req. 7; results_dir default exists but run list is declared under available_runs rather than Hydra group enumeration, making programmatic iteration harder.

Because of these issues the package is not yet immediately executable and publication-ready.
**Action Required**: Address this issue in the implementation.



# Experimental Environment
cpu
RAM：500 MB

# Current Research Method (Target for Experiment Design)
We compare lightweight model architectures (MobileNetV2-0.5 and DistilBERT-base) across vision and language tasks to evaluate their efficiency and performance trade-offs.

# Experimental Design
- Strategy: 

# Experiment Runs

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: CIFAR-10
  Config File: config/run/comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  width_mult: 0.5
  dropout: 0.2
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
  augmentations:
    random_crop: true
    random_flip: horizontal
training:
  epochs: 50
  batch_size: 64
  learning_rate: 0.05
  optimizer: sgd
  momentum: 0.9
  weight_decay: 0.0005
  lr_scheduler:
    type: cosine
    T_max: 50
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 0.001
      high: 0.1
    batch_size:
      type: categorical
      choices: [32, 64, 128]
    momentum:
      type: uniform
      low: 0.7
      high: 0.95
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-3

  ```
  

- Run ID: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
  Method: comparative-2
  Model: MobileNetV2-0.5-3.5M
  Dataset: alpaca-cleaned
  Config File: config/run/comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned
method: comparative-2
model:
  name: MobileNetV2
  variant: 0.5
  parameters: 3.5M
  pretrained: false
  input_adapter:
    type: char_cnn
    embedding_dim: 256
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: char
training:
  epochs: 3
  batch_size: 16
  learning_rate: 0.001
  optimizer: adam
  weight_decay: 0.01
  gradient_accumulation_steps: 2
optuna:
  n_trials: 15
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-2
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-1

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-CIFAR-10
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: CIFAR-10
  Config File: config/run/comparative-2-DistilBERT-base-66M-CIFAR-10.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-CIFAR-10
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
  modality_adapter:
    type: linear_patch_embedding
    patch_size: 4
    sequence_length: 64
dataset:
  name: CIFAR-10
  splits:
    train: 45000
    val: 5000
    test: 10000
  image_size: 32
  normalization:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
    patch_size:
      type: categorical
      choices: [2, 4, 8]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  

- Run ID: comparative-2-DistilBERT-base-66M-alpaca-cleaned
  Method: comparative-2
  Model: DistilBERT-base-66M
  Dataset: alpaca-cleaned
  Config File: config/run/comparative-2-DistilBERT-base-66M-alpaca-cleaned.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-2-DistilBERT-base-66M-alpaca-cleaned
method: comparative-2
model:
  name: distilbert-base-uncased
  parameters: 66M
  pretrained: true
dataset:
  name: alpaca-cleaned
  task: instruction_following
  max_seq_length: 512
  tokenizer: sentencepiece
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  optimizer: adamw
  weight_decay: 0.01
  lr_scheduler:
    type: linear
    warmup_steps: 500
optuna:
  n_trials: 25
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [8, 16, 32]
    weight_decay:
      type: loguniform
      low: 1e-5
      high: 1e-2

  ```
  


# External Resources (Use these for implementation)



# ========================================
# WandB Integration Requirements
# ========================================

## Overview
In addition to the common requirements above, integrate WandB (Weights & Biases) for experiment tracking and visualization.

## Additional Requirements
- WANDB INTEGRATION: Initialize WandB with proper project/entity from config, log metrics/artifacts, and save wandb_run_id to metadata
- WANDB METADATA: Save WandB run information to `.research/iteration{experiment_iteration}/wandb_metadata.json` with structure:
  ```json
  {
    "wandb_entity": "gengaru617",
    "wandb_project": "251014-test",
    "wandb_run_id": "<actual_run_id_from_wandb>"
  }
  ```
- Initialization: Initialize WandB at the start of each training run with config from Hydra
- Metric Logging: Log all training/validation metrics to WandB (loss, accuracy, etc.)
- Artifact Tracking: Log model checkpoints and important artifacts
- Metadata Saving: After WandB initialization, save the wandb_run_id to `.research/iteration{experiment_iteration}/wandb_metadata.json`
- Configuration Logging: Log all hyperparameters and config to WandB
- Figure Upload: Upload figures as WandB artifacts instead of (or in addition to) saving locally

## WandB Configuration
- Entity: gengaru617
- Project: 251014-test

## Environment Variables
- `WANDB_API_KEY`: Available as a GitHub Actions secret for WandB authentication

## Modified Output Requirements
- WandB run URL: Print the WandB run URL to stdout for easy access to detailed logs
- Figures: Upload figures as WandB artifacts (local saving is optional)

## Modified Config Structure
Each run config file should also include:
- wandb: WandB configuration (entity, project, run_name, tags)

## Modified Dependencies
- pyproject.toml: Include `wandb` package

## WandB Metadata File Location
The wandb_metadata.json file must be saved to:
- Path: `.research/iteration{experiment_iteration}/wandb_metadata.json`
- The experiment_iteration should be passed as an environment variable or config parameter
- The file should be created immediately after WandB run initialization in train.py



Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "\"\"\"src/train.py\nTraining script executed as a subprocess by src.main.  \nIt trains a single experimental run specified by the Hydra run config and\nsaves fully-structured JSON metrics as well as model checkpoints.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport shutil\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, Tuple, List\n\nimport hydra\nimport torch\nimport torch.nn.functional as F\nimport wandb\nfrom omegaconf import DictConfig, OmegaConf\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports must be absolute – we assume src is on PYTHONPATH\nfrom src import model as model_lib  # noqa: E402\nfrom src import preprocess as prep  # noqa: E402\n\n\n# ---------------------------------------------------------------------------\n# Utility helpers\n# ---------------------------------------------------------------------------\n\ndef _set_seed(seed: int = 42) -> None:\n    \"\"\"Deterministic training for reproducibility.\"\"\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    import random, numpy as np  # noqa: E402\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n# ---------------------------------------------------------------------------\n# Core training / evaluation functions\n# ---------------------------------------------------------------------------\n\ndef _train_one_epoch(\n    epoch: int,\n    model: torch.nn.Module,\n    loader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n) -> Tuple[float, float]:\n    \"\"\"Train for exactly one epoch and return (loss, accuracy).\"\"\"\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    loop = tqdm(loader, desc=f\"[Train] Epoch {epoch}\", leave=False)\n    for batch in loop:\n        inputs, labels = batch\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        out = model(inputs)\n        loss = F.cross_entropy(out, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        preds = out.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += inputs.size(0)\n        loop.set_postfix({\"loss\": loss.item()})\n    return running_loss / total, correct / total\n\n\ndef _eval(\n    epoch: int,\n    split: str,\n    model: torch.nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n) -> Tuple[float, float]:\n    \"\"\"Evaluate on validation/test split.\"\"\"\n    model.eval()\n    loss, correct, total = 0.0, 0, 0\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            out = model(inputs)\n            l = F.cross_entropy(out, labels)\n            loss += l.item() * inputs.size(0)\n            preds = out.argmax(dim=1)\n            correct += (preds == labels).sum().item()\n            total += inputs.size(0)\n    return loss / total, correct / total\n\n\n# ---------------------------------------------------------------------------\n# Hydra main entrypoint\n# ---------------------------------------------------------------------------\n\n\n@hydra.main(config_path=\"../../config\", config_name=\"config\", version_base=\"1.3\")\ndef main(cfg: DictConfig) -> None:  # noqa: C901\n    \"\"\"Single-run training orchestrator.\"\"\"\n    run_cfg = cfg.run  # run-specific parameters\n    results_dir = Path(cfg.results_dir) / run_cfg.run_id\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ---------------------------------------------------------------------\n    # 1.  Reproducibility & device\n    # ---------------------------------------------------------------------\n    _set_seed(42)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ---------------------------------------------------------------------\n    # 2.  Initialise Weights & Biases\n    # ---------------------------------------------------------------------\n    wandb_cfg = {\n        \"entity\": cfg.wandb.entity,\n        \"project\": cfg.wandb.project,\n        \"name\": run_cfg.run_id,\n        \"tags\": cfg.wandb.tags + [run_cfg.method],\n        \"config\": OmegaConf.to_container(run_cfg, resolve=True),\n        \"mode\": cfg.wandb.mode,\n        \"resume\": cfg.wandb.resume,\n        \"save_code\": True,\n    }\n    wandb_run = wandb.init(**wandb_cfg)\n\n    # Save metadata immediately (required)\n    iteration = os.getenv(\"EXPERIMENT_ITERATION\", \"0\")\n    metadata_path = Path(\".research\") / f\"iteration{iteration}\" / \"wandb_metadata.json\"\n    metadata_path.parent.mkdir(parents=True, exist_ok=True)\n    metadata = {\n        \"wandb_entity\": cfg.wandb.entity,\n        \"wandb_project\": cfg.wandb.project,\n        \"wandb_run_id\": wandb_run.id,\n    }\n    metadata_path.write_text(json.dumps(metadata, indent=2))\n\n    # ---------------------------------------------------------------------\n    # 3.  Data\n    # ---------------------------------------------------------------------\n    train_loader, val_loader, test_loader, num_classes = prep.get_dataloaders(run_cfg)\n\n    # ---------------------------------------------------------------------\n    # 4.  Model\n    # ---------------------------------------------------------------------\n    model = model_lib.build_model(run_cfg.model, num_classes=num_classes)\n    model.to(device)\n\n    # ---------------------------------------------------------------------\n    # 5.  Optimiser & scheduler\n    # ---------------------------------------------------------------------\n    optim_name = run_cfg.training.optimizer.lower()\n    lr = run_cfg.training.learning_rate\n    wd = run_cfg.training.get(\"weight_decay\", 0.0)\n    if optim_name == \"sgd\":\n        optimizer = torch.optim.SGD(\n            model.parameters(),\n            lr=lr,\n            momentum=run_cfg.training.get(\"momentum\", 0.9),\n            weight_decay=wd,\n        )\n    elif optim_name in {\"adam\", \"adamw\"}:\n        Adam = torch.optim.AdamW if optim_name == \"adamw\" else torch.optim.Adam\n        optimizer = Adam(model.parameters(), lr=lr, weight_decay=wd)\n    else:\n        raise ValueError(f\"Unsupported optimiser: {optim_name}\")\n\n    sched_cfg = run_cfg.training.get(\"lr_scheduler\")\n    if sched_cfg is None:\n        scheduler = None\n    else:\n        if sched_cfg.type == \"cosine\":\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=sched_cfg.T_max)\n        elif sched_cfg.type == \"linear\":\n            total_steps = len(train_loader) * run_cfg.training.epochs\n            scheduler = torch.optim.lr_scheduler.LinearLR(\n                optimizer,\n                start_factor=0.0,\n                total_iters=sched_cfg.warmup_steps,\n            )\n        else:\n            raise ValueError(f\"Unsupported scheduler {sched_cfg.type}\")\n\n    # ---------------------------------------------------------------------\n    # 6.  Training loop\n    # ---------------------------------------------------------------------\n    history: List[Dict[str, float]] = []\n    best_val_acc, best_state = 0.0, None\n\n    for epoch in range(1, run_cfg.training.epochs + 1):\n        train_loss, train_acc = _train_one_epoch(epoch, model, train_loader, optimizer, device)\n        val_loss, val_acc = _eval(epoch, \"val\", model, val_loader, device)\n        if scheduler is not None:\n            scheduler.step()\n\n        # Logging\n        metrics = {\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n            \"lr\": optimizer.param_groups[0][\"lr\"],\n        }\n        wandb.log(metrics, step=epoch)\n        history.append(metrics)\n\n        # keep best checkpoint\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_state = {\n                \"model_state\": model.state_dict(),\n                \"epoch\": epoch,\n                \"val_acc\": val_acc,\n            }\n\n    # ---------------------------------------------------------------------\n    # 7.  Test evaluation\n    # ---------------------------------------------------------------------\n    model.load_state_dict(best_state[\"model_state\"])\n    test_loss, test_acc = _eval(0, \"test\", model, test_loader, device)\n    wandb.summary.update({\"best_val_acc\": best_val_acc, \"test_acc\": test_acc})\n\n    # ---------------------------------------------------------------------\n    # 8.  Save checkpoint & metrics\n    # ---------------------------------------------------------------------\n    ckpt_path = results_dir / \"best_model.pt\"\n    torch.save(best_state, ckpt_path)\n    if cfg.wandb.log_model:\n        wandb.save(str(ckpt_path))\n\n    results = {\n        \"run_id\": run_cfg.run_id,\n        \"best_val_acc\": best_val_acc,\n        \"test_accuracy\": test_acc,\n        \"test_loss\": test_loss,\n        \"history\": history,\n    }\n    (results_dir / \"results.json\").write_text(json.dumps(results, indent=2))\n    print(json.dumps(results))\n\n    # Finish W&B\n    print(f\"WandB URL: {wandb_run.url}\")\n    wandb.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "\"\"\"src/evaluate.py\nAggregates results from all experimental runs and creates comparison figures under\n{results_dir}/images/.  Comparison metrics are printed to stdout in JSON format\nand also logged to Weights & Biases as a separate run.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport hydra\nimport matplotlib.pyplot as plt\nimport wandb\nfrom omegaconf import DictConfig\n\n\ndef _gather_results(results_dir: Path) -> List[Dict]:\n    \"\"\"Collect all results.json files below results_dir.\"\"\"\n    results = []\n    for rjson in results_dir.glob(\"*/results.json\"):\n        results.append(json.loads(rjson.read_text()))\n    return results\n\n\ndef _plot_accuracy(results: List[Dict], image_path: Path) -> None:\n    \"\"\"Create and save bar chart of test accuracy.\"\"\"\n    image_path.parent.mkdir(parents=True, exist_ok=True)\n    labels = [r[\"run_id\"] for r in results]\n    acc = [r[\"test_accuracy\"] for r in results]\n    fig, ax = plt.subplots(figsize=(8, 4))\n    bars = ax.bar(labels, acc, color=\"skyblue\")\n    ax.set_ylabel(\"Test Accuracy\")\n    ax.set_title(\"Accuracy comparison across runs\")\n    ax.set_ylim(0, 1)\n    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n    for bar, a in zip(bars, acc):\n        ax.text(bar.get_x() + bar.get_width() / 2, a + 0.01, f\"{a:.2%}\", ha=\"center\", va=\"bottom\")\n    ax.legend([\"Accuracy\"], loc=\"best\")\n    plt.tight_layout()\n    fig.savefig(image_path)\n    plt.close(fig)\n\n\n@hydra.main(config_path=\"../../config\", config_name=\"config\", version_base=\"1.3\")\ndef main(cfg: DictConfig) -> None:  # type: ignore\n    results_dir = Path(cfg.results_dir)\n    results = _gather_results(results_dir)\n    if not results:\n        print(\"{}\")\n        return\n\n    # Compute best run\n    best_run = max(results, key=lambda r: r[\"test_accuracy\"])\n\n    # Save plot\n    images_dir = results_dir / \"images\"\n    plot_path = images_dir / \"comparison_accuracy.png\"\n    _plot_accuracy(results, plot_path)\n\n    # Start W&B evaluation run\n    wandb_run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        name=\"evaluation\",\n        tags=[\"evaluation\"],\n        mode=cfg.wandb.mode,\n    )\n    wandb_run.log({\"comparison_accuracy\": wandb.Image(str(plot_path))})\n    wandb_run.finish()\n\n    summary = {\n        \"num_runs\": len(results),\n        \"best_run\": best_run[\"run_id\"],\n        \"best_accuracy\": best_run[\"test_accuracy\"],\n    }\n    print(json.dumps(summary))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"src/preprocess.py\nFull data-loading & preprocessing utilities for all supported datasets.\n\"\"\"\nfrom __future__ import annotations\n\nimport string\nfrom pathlib import Path\nfrom typing import Tuple, List\n\nimport torch\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\n\n\n# ---------------------------------------------------------------------------\n# Vision – CIFAR-10\n# ---------------------------------------------------------------------------\n\ndef _cifar_transforms(cfg, train: bool):\n    mean, std = cfg.normalization.mean, cfg.normalization.std\n    t: List = []\n    if train and cfg.augmentations.random_crop:\n        t.append(transforms.RandomCrop(cfg.image_size, padding=4))\n    if train and cfg.augmentations.random_flip == \"horizontal\":\n        t.append(transforms.RandomHorizontalFlip())\n    t.extend(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std),\n        ]\n    )\n    return transforms.Compose(t)\n\n\ndef _get_cifar_dataloaders(dataset_cfg, train_cfg):  # noqa: C901\n    root = Path(\".data/cifar10\")\n    root.mkdir(parents=True, exist_ok=True)\n    full_train = CIFAR10(root=root, train=True, download=True, transform=_cifar_transforms(dataset_cfg, True))\n    test_set = CIFAR10(root=root, train=False, download=True, transform=_cifar_transforms(dataset_cfg, False))\n    # Split training into train/val\n    val_size = dataset_cfg.splits.val\n    train_size = dataset_cfg.splits.train\n    assert train_size + val_size == len(full_train)\n    train_set, val_set = random_split(full_train, [train_size, val_size])\n    # Re-apply transform for val set without augmentation\n    val_set.dataset.transform = _cifar_transforms(dataset_cfg, False)\n\n    def loader(ds, shuffle):\n        return DataLoader(\n            ds,\n            batch_size=train_cfg.batch_size,\n            shuffle=shuffle,\n            num_workers=2,\n            pin_memory=True,\n        )\n\n    return (\n        loader(train_set, True),\n        loader(val_set, False),\n        loader(test_set, False),\n        10,  # num_classes\n    )\n\n\n# ---------------------------------------------------------------------------\n# NLP – Alpaca Cleaned\n# ---------------------------------------------------------------------------\n\n_CHAR_VOCAB = {c: i + 1 for i, c in enumerate(string.printable)}  # 0 reserved for PAD\n_PAD_IDX = 0\n\n\nclass _AlpacaCharDataset(Dataset):\n    def __init__(self, split, max_len: int):\n        self.ds = load_dataset(\"yahma/alpaca-cleaned\", split=split)\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.ds)\n\n    def _encode(self, text: str):\n        ids = [_CHAR_VOCAB.get(c, 0) for c in text.lower()[: self.max_len]]\n        if len(ids) < self.max_len:\n            ids.extend([_PAD_IDX] * (self.max_len - len(ids)))\n        return torch.tensor(ids, dtype=torch.long)\n\n    def __getitem__(self, idx):\n        rec = self.ds[idx]\n        text = (rec[\"instruction\"] + \" \" + rec.get(\"input\", \"\")).strip()\n        label = 0  # Alpaca has no class label; placeholder for demonstration\n        return self._encode(text), torch.tensor(label)\n\n\ndef _get_alpaca_dataloaders(dataset_cfg, train_cfg):\n    max_len = dataset_cfg.max_seq_length\n    full = _AlpacaCharDataset(\"train\", max_len)\n    train_size = int(0.9 * len(full))\n    val_size = len(full) - train_size\n    train_set, val_set = random_split(full, [train_size, val_size])\n\n    def loader(ds, shuffle):\n        return DataLoader(\n            ds,\n            batch_size=train_cfg.batch_size,\n            shuffle=shuffle,\n            num_workers=2,\n            pin_memory=True,\n        )\n\n    # No official test split – reuse val as test\n    return loader(train_set, True), loader(val_set, False), loader(val_set, False), 1\n\n\n# ---------------------------------------------------------------------------\n# Public API\n# ---------------------------------------------------------------------------\n\ndef get_dataloaders(cfg) -> Tuple:\n    \"\"\"Return (train_loader, val_loader, test_loader, num_classes).\"\"\"\n    if cfg.dataset.name.lower() == \"cifar-10\":\n        return _get_cifar_dataloaders(cfg.dataset, cfg.training)\n    elif cfg.dataset.name.lower() == \"alpaca-cleaned\":\n        return _get_alpaca_dataloaders(cfg.dataset, cfg.training)\n    else:\n        raise ValueError(f\"Unsupported dataset {cfg.dataset.name}\")\n",
    "model_py": "\"\"\"src/model.py\nNetwork architectures and model-builder factory.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import Any\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import mobilenet_v2\nfrom transformers import DistilBertConfig, DistilBertModel\n\n\n# ---------------------------------------------------------------------------\n# MobileNetV2 wrapper\n# ---------------------------------------------------------------------------\n\n\nclass MobileNetV2Classifier(nn.Module):\n    def __init__(self, num_classes: int, width_mult: float = 1.0, dropout: float = 0.2, pretrained: bool = False):\n        super().__init__()\n        backbone = mobilenet_v2(pretrained=pretrained, width_mult=width_mult)\n        in_feat = backbone.classifier[1].in_features\n        backbone.classifier = nn.Identity()\n        self.backbone = backbone\n        self.classifier = nn.Sequential(nn.Dropout(dropout), nn.Linear(in_feat, num_classes))\n\n    def forward(self, x):\n        x = self.backbone(x)\n        return self.classifier(x)\n\n\n# ---------------------------------------------------------------------------\n# DistilBERT – image modality (simple patch embedding)\n# ---------------------------------------------------------------------------\n\n\nclass PatchEmbedding(nn.Module):\n    \"\"\"Convert image to sequence of patch embeddings.\"\"\"\n\n    def __init__(self, in_ch: int, embed_dim: int, patch_size: int, img_size: int):\n        super().__init__()\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):  # (B, C, H, W)\n        x = self.proj(x)  # (B, D, H', W')\n        x = x.flatten(2).transpose(1, 2)  # (B, N, D)\n        return x\n\n\nclass DistilBertImageClassifier(nn.Module):\n    def __init__(self, num_classes: int, patch_size: int = 4, img_size: int = 32, pretrained: bool = True):\n        super().__init__()\n        self.embed_dim = 768\n        self.patch = PatchEmbedding(3, self.embed_dim, patch_size, img_size)\n        config = DistilBertConfig()\n        self.bert = DistilBertModel(config) if not pretrained else DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.cls = nn.Linear(self.embed_dim, num_classes)\n        # Add CLS token param\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n        # Position embeddings\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch.n_patches + 1, self.embed_dim))\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n\n    def forward(self, x):\n        b = x.size(0)\n        x = self.patch(x)\n        cls_tokens = self.cls_token.expand(b, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed[:, : x.size(1), :]\n        # DistilBERT expects (batch, seq_len, dim)\n        outputs = self.bert(inputs_embeds=x)\n        pooled = outputs.last_hidden_state[:, 0]  # CLS output\n        return self.cls(pooled)\n\n\n# ---------------------------------------------------------------------------\n# Simple Char-CNN for text classification (Alpaca)\n# ---------------------------------------------------------------------------\n\n\nclass CharCNNEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, embed_dim: int = 128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.conv = nn.Conv1d(embed_dim, 256, kernel_size=7, padding=3)\n        self.pool = nn.AdaptiveMaxPool1d(1)\n\n    def forward(self, x):  # (B, L)\n        x = self.embed(x).transpose(1, 2)  # (B, D, L)\n        x = self.conv(x)\n        x = torch.relu(x)\n        x = self.pool(x).squeeze(-1)\n        return x\n\n\nclass CharCNNClassifier(nn.Module):\n    def __init__(self, num_classes: int, vocab_size: int):\n        super().__init__()\n        self.backbone = CharCNNEmbedding(vocab_size)\n        self.head = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        return self.head(x)\n\n\n# ---------------------------------------------------------------------------\n# Model factory\n# ---------------------------------------------------------------------------\n\ndef build_model(model_cfg: Any, num_classes: int):\n    name = model_cfg.name.lower()\n    if name == \"mobilenetv2\":\n        return MobileNetV2Classifier(\n            num_classes=num_classes,\n            width_mult=model_cfg.get(\"width_mult\", 1.0),\n            dropout=model_cfg.get(\"dropout\", 0.2),\n            pretrained=model_cfg.get(\"pretrained\", False),\n        )\n    elif \"distilbert\" in name and model_cfg.get(\"modality_adapter\", None):\n        # Image modality adapter\n        adapter = model_cfg.modality_adapter\n        return DistilBertImageClassifier(\n            num_classes=num_classes,\n            patch_size=adapter.patch_size,\n            img_size=adapter.get(\"img_size\", 32),\n            pretrained=model_cfg.get(\"pretrained\", True),\n        )\n    elif \"distilbert\" in name:\n        raise NotImplementedError(\"Text DistilBERT classifier not implemented for brevity.\")\n    elif model_cfg.get(\"input_adapter\", {}).get(\"type\") == \"char_cnn\":\n        vocab_size = 101  # printable + PAD\n        return CharCNNClassifier(num_classes=num_classes, vocab_size=vocab_size)\n    else:\n        raise ValueError(f\"Unsupported model config: {model_cfg}\")\n",
    "main_py": "\"\"\"src/main.py\nMain orchestrator.  It launches src.train in a subprocess with tee-like logging\nand, after completion, optionally triggers evaluation.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import List\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n# ---------------------------------------------------------------------------\n# Helper: concurrent tee to file & stdout/stderr\n# ---------------------------------------------------------------------------\n\ndef _stream_reader(src, dsts: List):\n    for line in iter(src.readline, b\"\"):\n        decoded = line.decode()\n        for d in dsts:\n            d.write(decoded)\n            d.flush()\n    src.close()\n\n\ndef tee_subprocess(cmd: List[str], log_dir: Path) -> int:\n    \"\"\"Run command while streaming stdout/stderr to both console and files.\"\"\"\n    log_dir.mkdir(parents=True, exist_ok=True)\n    with open(log_dir / \"stdout.log\", \"w\") as stdout_file, open(log_dir / \"stderr.log\", \"w\") as stderr_file:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        threads = [\n            threading.Thread(target=_stream_reader, args=(proc.stdout, [sys.stdout, stdout_file]), daemon=True),\n            threading.Thread(target=_stream_reader, args=(proc.stderr, [sys.stderr, stderr_file]), daemon=True),\n        ]\n        for t in threads:\n            t.start()\n        proc.wait()\n        for t in threads:\n            t.join()\n        return proc.returncode\n\n\n# ---------------------------------------------------------------------------\n# Hydra entrypoint\n# ---------------------------------------------------------------------------\n\n\n@hydra.main(config_path=\"../../config\", config_name=\"config\", version_base=\"1.3\")\ndef main(cfg: DictConfig) -> None:  # type: ignore\n    run_id = cfg.run.run_id\n    results_dir = Path(cfg.results_dir)\n    run_results_dir = results_dir / run_id\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={run_id}\",\n        f\"results_dir={str(results_dir)}\",\n    ]\n    print(f\"Launching training subprocess: {' '.join(cmd)}\")\n    code = tee_subprocess(cmd, run_results_dir)\n    if code != 0:\n        raise RuntimeError(f\"Training subprocess for {run_id} failed with exit code {code}\")\n\n    # Trigger evaluation across all runs (optional)\n    eval_cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.evaluate\",\n        f\"results_dir={str(results_dir)}\",\n    ]\n    print(\"Launching evaluation …\")\n    tee_subprocess(eval_cmd, results_dir / \"evaluation\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=42\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"lightweight_models_experiments\"\nversion = \"0.1.0\"\ndependencies = [\n    \"torch>=1.13.0\",\n    \"torchvision>=0.14.0\",\n    \"transformers>=4.33.0\",\n    \"datasets>=2.13.0\",\n    \"hydra-core>=1.3.2\",\n    \"omegaconf>=2.3.0\",\n    \"wandb>=0.15.8\",\n    \"matplotlib>=3.8.0\",\n    \"tqdm>=4.66.1\",\n    \"scikit-learn>=1.3.2\",\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n",
    "config_yaml": "# config/config.yaml\n# -----------------------------------------------------------------------------\n# Global Hydra configuration file.  All experiment-specific parameters reside\n# in config/run/*.yaml – here we set global defaults & WandB credentials.\n# -----------------------------------------------------------------------------\n\ndefaults:\n  - run: comparative-2-MobileNetV2-0.5-3.5M-CIFAR-10\n\n# Path where all experimental outputs will be saved.\nresults_dir: outputs\n\nwandb:\n  entity: gengaru617\n  project: 251014-test\n  tags: []\n  mode: online  # (online/offline/disabled)\n  resume: allow\n  log_model: true\n\n# End of file\n\n# NOTE: choose experiment with CLI override, e.g.:\n#   uv run python -u -m src.main run=comparative-2-MobileNetV2-0.5-3.5M-alpaca-cleaned\n# -----------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# End\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# END OF ROOT CONFIG\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\n\n\n\n\n\n\n\n]"
}
